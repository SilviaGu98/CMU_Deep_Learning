{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSLkT0qL3jgl"
      },
      "source": [
        "# HW4P1: Language Modelling\n",
        "\n",
        "Welcome to the final part 1 hw of this course. This is the only part 1 in which you have PyTorch training (Yay). You will be working on training language models and evaluating them on the task of prediction and generation.<br>\n",
        "Note: A major change which we have made this semester is that we have made the model which you will be coding in this HW very similar to the Speller module from HW4P2. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "95e48c7693e34a389da49dcb6e448e0c",
        "deepnote_cell_type": "markdown",
        "id": "EB2bOV3bzYLR"
      },
      "source": [
        "# Get modules and datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r4_-qG9rSULt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchsummaryX\n",
            "  Downloading torchsummaryX-1.3.0-py3-none-any.whl (3.6 kB)\n",
            "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torchsummaryX) (1.13.1)\n",
            "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torchsummaryX) (1.22.4)\n",
            "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torchsummaryX) (1.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas->torchsummaryX) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas->torchsummaryX) (2022.7.1)\n",
            "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torch->torchsummaryX) (4.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->torchsummaryX) (1.14.0)\n",
            "Installing collected packages: torchsummaryX\n",
            "Successfully installed torchsummaryX-1.3.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.8 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install torchsummaryX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnrUvEIC5i5j"
      },
      "outputs": [],
      "source": [
        "# TODO: Import drive if you are using Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "65e59cd2e6514d9594258167a6a0f6db",
        "deepnote_cell_type": "markdown",
        "id": "INh9p3v3zbF_"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cell_id": "03bf3bd639a048f098d5febc42e2baff",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 4,
        "execution_start": 1679856365820,
        "id": "QZNwme4320LW",
        "source_hash": "b7876178"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/Users/guyunxin/Desktop/CMU/23Spring/11685/hw4p1\") # TODO: Add path to handout/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/guyunxin/Desktop/CMU/23Spring/11685/hw4p1'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sys.path[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "cell_id": "b48a9e95f26c4d2e89d95b1b311cedd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:09.992480Z",
          "iopub.status.busy": "2022-08-10T14:02:09.987693Z",
          "iopub.status.idle": "2022-08-10T14:02:12.872562Z",
          "shell.execute_reply": "2022-08-10T14:02:12.870819Z",
          "shell.execute_reply.started": "2022-08-10T14:02:09.991351Z"
        },
        "execution_millis": 2669,
        "execution_start": 1679856365830,
        "id": "oxiZ42B4SwQ-",
        "outputId": "9888f04a-bc26-4830-d89d-5f0ab94fa875",
        "source_hash": "ec149d26",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cpu\n",
            "/Users/guyunxin/Desktop/CMU/23Spring/11685/hw4p1/handout\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "\n",
        "import os\n",
        "\n",
        "import time \n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import torchsummaryX\n",
        "\n",
        "# Importing necessary modules from hw4\n",
        "from hw4.tests_hw4 import test_prediction, test_generation\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device: \", DEVICE)\n",
        "\n",
        "%cd /Users/guyunxin/Desktop/CMU/23Spring/11685/hw4p1/handout "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a4ff875589ee46da8f749a7e5088a3ef",
        "deepnote_cell_type": "markdown",
        "id": "u-R794-0zc9V"
      },
      "source": [
        "# Load datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HU4e_6l0Whda",
        "outputId": "bd0d665c-4ee9-4856-aeba-1d74df960d89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab length:  33280\n",
            "['!' '\"' '#' ... 'ï½ž' '<sos>' '<eos>']\n"
          ]
        }
      ],
      "source": [
        "# Loading the vocabulary. Try printing and see\n",
        "VOCAB       = np.load('dataset/vocab.npy') \n",
        "\n",
        "# We have also included <sos> and <eos> in the vocabulary for you\n",
        "# However in real life, you include it explicitly if not provided\n",
        "SOS_TOKEN   = np.where(VOCAB == '<sos>')[0][0]\n",
        "EOS_TOKEN   = np.where(VOCAB == '<eos>')[0][0]\n",
        "NUM_WORDS   = len(VOCAB) - 2 # Actual number of words in vocabulary\n",
        "\n",
        "print(\"Vocab length: \", len(VOCAB))\n",
        "print(VOCAB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<eol>'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "VOCAB[1417]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "33278"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SOS_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "33279"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "EOS_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"'Italia\""
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "VOCAB[19]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "33278"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SOS_TOKEN "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "LA7SapmyXHr7"
      },
      "outputs": [],
      "source": [
        "# Loding the training dataset. Refer to write up section 2 to understand the structure\n",
        "dataset     = np.load('dataset/wiki.train.npy', allow_pickle=True)\n",
        "\n",
        "# The dataset does not have <sos> and <eos> because they are just regular articles. \n",
        "# TODO: Add <sos> and <eos> to every article in the dataset.\n",
        "# articles = np.zeros((dataset.shape[0],))\n",
        "articles = []\n",
        "for i in range(dataset.shape[0]):\n",
        "    article = dataset[i]\n",
        "    article = np.insert(article, 0, SOS_TOKEN)\n",
        "    article = np.append(article, EOS_TOKEN)\n",
        "    # articles = np.vstack((articles, article))\n",
        "    articles.append(article)\n",
        "# Before doing do, try printing the dataset to see if they are words or integers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-150-6b293fa8402e>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  dataset = np.array(articles)\n"
          ]
        }
      ],
      "source": [
        "dataset = np.array(articles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "cell_id": "09f3a2efaeef49ef9f4c0b2b9a614cca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:12.888156Z",
          "iopub.status.busy": "2022-08-10T14:02:12.884281Z",
          "iopub.status.idle": "2022-08-10T14:02:12.960590Z",
          "shell.execute_reply": "2022-08-10T14:02:12.958805Z",
          "shell.execute_reply.started": "2022-08-10T14:02:12.888058Z"
        },
        "execution_millis": 46,
        "execution_start": 1679856368507,
        "id": "x5znxQhLSwRC",
        "outputId": "994601b9-86c5-4406-b37f-be48b6d3244a",
        "source_hash": "42e4c03c",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation shapes    :  (128, 21) (128,)\n",
            "Test shapes          :  (128, 21)\n"
          ]
        }
      ],
      "source": [
        "# Loading the fixtures for validation and test - prediction\n",
        "fixtures_pred       = np.load('fixtures/prediction.npz')        # validation\n",
        "fixtures_pred_test  = np.load('fixtures/prediction_test.npz')   # test\n",
        "\n",
        "print(\"Validation shapes    : \", fixtures_pred['inp'].shape, fixtures_pred['out'].shape)\n",
        "print(\"Test shapes          : \", fixtures_pred_test['inp'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pes7mCr5WdAw",
        "outputId": "57199f71-170e-439f-802f-0da5f474c617"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Gen Shapes    : (32, 21)\n",
            "Test Gen Shapes          : (128, 31)\n"
          ]
        }
      ],
      "source": [
        "# Loading the fixtures for validation and test - generation\n",
        "fixtures_gen        = np.load('fixtures/generation.npy')        # validation\n",
        "fixtures_gen_test   = np.load('fixtures/generation_test.npy')   # test\n",
        "\n",
        "print(\"Validation Gen Shapes    :\", fixtures_gen.shape)\n",
        "print(\"Test Gen Shapes          :\", fixtures_gen_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "jO_Qt7O6rL8L"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:  [33278 26096 26972 25821 14658 29325 32935 21820 25639 16134 31353 29092\n",
            "    79  6916    76 21415 14658 24911  1424 29456 29325]\n",
            "Output:  72\n",
            "generation  [33278 32885 31353 22124 32747 20202 28955    76 16786 25881 31453 32846\n",
            " 32022 14686 31543 15794    79  9916 25821 31365 32846]\n"
          ]
        }
      ],
      "source": [
        "# Example Prediction Dev Input and Output\n",
        "# Optional TODO: You can try printing a few samples from the validation set which has both inputs and outputs\n",
        "print(\"Input: \", fixtures_pred['inp'][0])\n",
        "print(\"Output: \", fixtures_pred['out'][0])\n",
        "print('generation ', fixtures_gen[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2007"
            ]
          },
          "execution_count": 269,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[578].shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "aec0165a3f1245dfa52a0cb80dba2578",
        "deepnote_cell_type": "markdown",
        "id": "dHjYhXAOzkrP"
      },
      "source": [
        "# Custom DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {
        "cell_id": "b2e63a7f6dec4a3f98588725a72a8ff2",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:13.079390Z",
          "iopub.status.busy": "2022-08-10T14:02:13.078847Z",
          "iopub.status.idle": "2022-08-10T14:02:13.196189Z",
          "shell.execute_reply": "2022-08-10T14:02:13.192167Z",
          "shell.execute_reply.started": "2022-08-10T14:02:13.079324Z"
        },
        "execution_millis": 48,
        "execution_start": 1679856368575,
        "id": "OZNrJ8XvSwRF",
        "source_hash": "a81eaa14",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class DataLoaderForLanguageModeling(torch.utils.data.DataLoader): # Inherit from torch.utils.data.DataLoader\n",
        "    \"\"\"\n",
        "        TODO: Define data loader logic here\n",
        "    \"\"\"\n",
        "    # TODO: You can probably add more parameters as well. Eg. sequence length\n",
        "    def __init__(self, dataset, batch_size, shuffle= False, drop_last= False, sequence_length= 20): \n",
        "        \n",
        "        # If you remember, these are the standard things which you give while defining a dataloader.\n",
        "        # Now you are just customizing your dataloader\n",
        "        self.dataset    = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle    = shuffle\n",
        "        self.drop_last  = drop_last\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        # What output do you get when you print len(loader)? You get the number of batches\n",
        "        # Your dataset has (579, ) articles and each article has a specified amount of words.\n",
        "        # You concatenate the dataset and then batch parts of it according to the sequence length\n",
        "        # TODO: return the number of batches\n",
        "        # If you are using variable sequence_length, the length might not be fixed \n",
        "\n",
        "        concat_length = np.sum([self.dataset[i].shape[0] for i in range(self.dataset.shape[0])])\n",
        "        # print(concat_length)\n",
        "        num_of_batches = concat_length//(self.batch_size*self.sequence_length)\n",
        "        \n",
        "\n",
        "        return num_of_batches\n",
        "\n",
        "    def __iter__(self):\n",
        "        # TODOs: \n",
        "        # 1. Shuffle data if shuffle is True\n",
        "        # 2. Concatenate articles and drop extra words\n",
        "        # 3. Divide the concatenated dataset into inputs and targets. How do they vary? \n",
        "        # 4. Reshape the inputs and targets into batches (think about the final shape)\n",
        "        # 5. Loop though the batches and yield the input and target according to the sequence length\n",
        "        \n",
        "        if self.shuffle:\n",
        "            # TODO\n",
        "\n",
        "            np.random.shuffle(self.dataset)\n",
        "        \n",
        "        num_batches = self.__len__()\n",
        "        \n",
        "        # print(num_batches)\n",
        "\n",
        "        batches = []\n",
        "        # concat dataset and create batch\n",
        "        concat_dataset = np.concatenate(self.dataset)\n",
        "        for i in range(num_batches):\n",
        "            batch = concat_dataset[i*self.batch_size*self.sequence_length:(i+1)*self.batch_size*self.sequence_length+1]\n",
        "            # add extra one sequence to the end \n",
        "            # print(batch.shape)\n",
        "            batches.append(batch)\n",
        "\n",
        "        batch_idx = 0\n",
        "\n",
        "        if self.drop_last:\n",
        "            # TODO\n",
        "            batches = batches[:-1]\n",
        "\n",
        "        while batch_idx < num_batches:\n",
        "            # print(batches[batch_idx].shape)\n",
        "            inputs = batches[batch_idx][:-1].reshape(self.batch_size, self.sequence_length)\n",
        "            targets = batches[batch_idx][1:].reshape(self.batch_size, self.sequence_length)\n",
        "            batch_idx += 1  \n",
        "            yield torch.tensor(inputs).to(DEVICE), torch.tensor(targets).to(DEVICE)\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {
        "cell_id": "773573c8374048d4bcb5a67b905ee2e0",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 3,
        "execution_start": 1679856368714,
        "id": "fBZSzmy10M9M",
        "source_hash": "27952b8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 20]) torch.Size([32, 20])\n",
            "torch.Size([32, 20]) torch.Size([32, 20])\n",
            "x:  ['<sos>', '=', 'Curtis', 'Woodhouse', '=', '<eol>', 'Curtis', 'Woodhouse', '(', 'born', '17', 'April', '1980', ')', 'is', 'an', 'English', 'former', 'professional', 'footballer']\n",
            "y:  ['=', 'Curtis', 'Woodhouse', '=', '<eol>', 'Curtis', 'Woodhouse', '(', 'born', '17', 'April', '1980', ')', 'is', 'an', 'English', 'former', 'professional', 'footballer', 'turned']\n"
          ]
        }
      ],
      "source": [
        "# Some sanity checks\n",
        "\n",
        "dl = DataLoaderForLanguageModeling(\n",
        "    dataset     = dataset, \n",
        "    batch_size  = 32, \n",
        "    shuffle     = True, \n",
        "    drop_last   = True\n",
        "    # Input Extra parameters here if needed\n",
        ")\n",
        "\n",
        "inputs, targets = next(iter(dl))\n",
        "print(inputs.shape, targets.shape)\n",
        "\n",
        "for x, y in dl:\n",
        "    print(x.shape, y.shape)\n",
        "    # shape should be (batch_size, sequence_length)\n",
        "    print(\"x: \", [VOCAB[i] for i in x[0, :]])\n",
        "    print(\"y: \", [VOCAB[i] for i in y[0, :]])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "0e75c3c3318d481aa99230d81eb68c13",
        "deepnote_cell_type": "markdown",
        "id": "WcWU0YlnzmVM"
      },
      "source": [
        "# LanguageModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {
        "id": "cebwoorWttWe"
      },
      "outputs": [],
      "source": [
        "# Here comes the main portion of this HW.\n",
        "# You can do this with a regular LSTM similar to HW3P2. \n",
        "# However, using LSTMCells will make this Language model very similar to the decoder in HW4P2 and we recommend you use that for writing resuable code.\n",
        "\n",
        "class LanguageModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, hid_dim): # TODO: Add more parameters if you want\n",
        "        super().__init__()\n",
        "\n",
        "        # For all the layers which you will define, please read the documentation thoroughly before implementation\n",
        "\n",
        "        self.token_embedding    = torch.nn.Embedding(num_embeddings= vocab_size,\n",
        "                                                     embedding_dim = embed_dim) # TODO: Define a PyTorch embedding layer \n",
        "\n",
        "        self.lstm_cells         = torch.nn.Sequential(\n",
        "            torch.nn.LSTMCell(input_size = embed_dim, hidden_size = hid_dim), # TODO: Enter the parameters for the LSTMCells\n",
        "            torch.nn.LSTMCell(input_size = hid_dim, hidden_size = hid_dim)\n",
        "            # You can add multiple LSTMCells too if you want\n",
        "        )\n",
        "        self.hidden_dim = hid_dim\n",
        "\n",
        "        self.token_probability  = torch.nn.Linear(hid_dim, vocab_size) # TODO: Define the parameters\n",
        "\n",
        "        # Optional TODO: Weight Tying. You just need to make the embedding layer weights equal to the Linear layer weight. \n",
        "                \n",
        "        # So the basic pipline is:\n",
        "        # word -> embedding -> lstm -> projection (linear) to get  probability distribution\n",
        "        # And this is happening across all time steps\n",
        "\n",
        "    def rnn_step(self, embedding, hidden_states_list):\n",
        "        # initialize hidden and cell states\n",
        "        hx = torch.zeros((embedding.shape[0],self.hidden_dim)) # (batch, hidden_size)\n",
        "        cx = torch.zeros((embedding.shape[0],self.hidden_dim))\n",
        "        hidden_states_list[0]=(hx,cx)\n",
        "        for i in range(len(self.lstm_cells)):\n",
        "            # TODO: Forward pass through each LSTMCell\n",
        "            hx, cx = self.lstm_cells[i](embedding, hidden_states_list[-1])\n",
        "            hidden_states_list[i]=(hx,cx)\n",
        "        embedding = hx\n",
        "        return embedding, hidden_states_list\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Refer to Section 1.3.1 to understand this function\n",
        "        if not torch.is_tensor(x):\n",
        "            x = torch.tensor(x).long().to(DEVICE)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            # TODO: Pass the input sequence through the model \n",
        "            # and return the probability distribution of the last timestep\n",
        "            prob,_ = self.forward(x)\n",
        "\n",
        "            return prob\n",
        "\n",
        "    def generate(self, x, timesteps): \n",
        "        # Refer to section 1.3.2 to understand this function\n",
        "        # Important Note: We do not draw <eos> from the distribution unlike the writeup\n",
        "        if not torch.is_tensor(x):\n",
        "            x = torch.tensor(x).long().to(DEVICE)\n",
        "\n",
        "        # TODO: Pass the input sequence through the model \n",
        "        # Obtain the probability distribution and hidden_states_list of the last timestep\n",
        "        \n",
        "        token_prob_dist, hidden_states_list     = self.forward(x)\n",
        "        next_token                              = token_prob_dist.argmax(dim = 1) \n",
        "        # TODO: Draw the next predicted token from the probability distribution ()  \n",
        "\n",
        "        generated_sequence  = [] \n",
        "        with torch.inference_mode():\n",
        "            for t in range(timesteps): # Loop through the timesteps\n",
        "                #   TODO: Pass the next_token and hidden_states_list through the model\n",
        "                token_prob_dist, hidden_states_list = self.forward(next_token, hidden_states_list)\n",
        "                #   TODO: You will get 2 outputs. What is the shape of the probability distribution?\n",
        "                #   TODO: Get the most probable token for the next timestep\n",
        "                next_token = token_prob_dist.argmax(dim = 1)\n",
        "\n",
        "                generated_sequence.append(next_token)\n",
        "            \n",
        "            generated_sequence = torch.stack(generated_sequence, dim= 1) # keep last timesteps generated words\n",
        "\n",
        "        return generated_sequence\n",
        "\n",
        "    # We are also having a hidden_states_list parameter because you need that in generation\n",
        "    def forward(self, x, hidden_states_list= None): # train model\n",
        "        # x (Batch, Seq_len)\n",
        "        # Note: you dont have to return the sum of log probabilities according to Pseudocode 1 in the writeup\n",
        "        # However, feel free to calculate and print it if you are curious\n",
        "\n",
        "        batch_size, timesteps   = x.shape \n",
        "\n",
        "        token_prob_distribution = [] # list which will contain probability distributions for all timesteps\n",
        "        # Initializing the hidden states\n",
        "        hidden_states_list      = [None]*len(self.lstm_cells) if hidden_states_list == None else hidden_states_list       \n",
        "\n",
        "        token_embeddings        = self.token_embedding(x)\n",
        "        # TODO\n",
        "        # When you get the embeddings of the input x, remember that you get it for all time steps.\n",
        "        # Embedding is just a linear transformation so you can precompute it for all time steps.\n",
        "\n",
        "        for t in range(timesteps): # LSTMCell is for just 1 timestep. Hence you need to loop through the total timesteps\n",
        "\n",
        "            token_embedding_t  = token_embeddings[:, t, :]\n",
        "            # TODO  \n",
        "\n",
        "            rnn_out, hidden_states_list = self.rnn_step(token_embedding_t, hidden_states_list)\n",
        "            # TODO (What should you do with the hidden_states_list?)\n",
        "            \n",
        "            token_prob_dist_t   = self.token_probability(rnn_out)\n",
        "             # TODO\n",
        "\n",
        "            token_prob_distribution.append(token_prob_dist_t) \n",
        "\n",
        "        token_prob_distribution = torch.stack(token_prob_distribution, dim = 1)\n",
        "        # TODO: Stack along the timesteps dimension\n",
        "\n",
        "        return token_prob_distribution, hidden_states_list "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8ed5a6ef54f9446fab752b79c70a0216",
        "deepnote_cell_type": "markdown",
        "id": "TlWF_bpLznup"
      },
      "source": [
        "# Trainer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {
        "cell_id": "8ea986fc372643389d1ab4c445659e9d",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": true,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:13.440820Z",
          "iopub.status.busy": "2022-08-10T14:02:13.440281Z",
          "iopub.status.idle": "2022-08-10T14:02:13.644455Z",
          "shell.execute_reply": "2022-08-10T14:02:13.642614Z",
          "shell.execute_reply.started": "2022-08-10T14:02:13.440752Z"
        },
        "id": "kIvZOIfjSwRK",
        "source_hash": "451a140f",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Unlike all the P2s, we are using a Trainer class for this HW.\n",
        "# Many researchers also use classes like this for training. You may have encountered them in your project as well.\n",
        "# You dont have to complete everything in this class, you only need to complete the train function.\n",
        "# However, its good to go through the code and see what it does. \n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, loader, optimizer, criterion, scheduler, max_epochs= 1, run_id= 'exp'):\n",
        "        \"\"\"\n",
        "            Use this class to train your model\n",
        "        \"\"\"\n",
        "        # feel free to add any other parameters here\n",
        "        self.model      = model\n",
        "        self.loader     = loader\n",
        "        self.optimizer  = optimizer\n",
        "        self.criterion  = criterion\n",
        "        self.scheduler  = scheduler\n",
        "\n",
        "        self.train_losses           = []\n",
        "        self.val_losses             = []\n",
        "        self.predictions            = []\n",
        "        self.predictions_test       = []\n",
        "        self.generated_logits       = []\n",
        "        self.generated              = []\n",
        "        self.generated_logits_test  = []\n",
        "        self.generated_test         = []\n",
        "        self.epochs                 = 0\n",
        "        self.max_epochs             = max_epochs\n",
        "        self.run_id                 = run_id\n",
        "\n",
        "\n",
        "    def calculate_loss(self, out, target):\n",
        "        # output: (B, T, Vocab_size) - probability distributions\n",
        "        # target: (B, T)\n",
        "        # Read the documentation of CrossEntropyLoss and try to understand how it takes inputs\n",
        "\n",
        "        # Tip: If your target is of shape (B, T) it means that you have B batches with T words. \n",
        "        # Tip: What is the total number of words in this batch? \n",
        "        # Tip: Crossentropy calculates the loss between a label and its probability distribution.\n",
        "\n",
        "        out     = out.reshape(-1, out.shape[-1]) # flatten batch and sequence, keep num of classes - vocab size\n",
        "        # TODO\n",
        "        targets = target.reshape(-1)\n",
        "        # TODO\n",
        "        loss    = self.criterion(out, targets)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        self.model.train() # set to training mode\n",
        "        self.model.to(DEVICE)\n",
        "        epoch_loss  = 0\n",
        "        num_batches = 0\n",
        "        \n",
        "        for batch_num, (inputs, targets) in enumerate(tqdm(self.loader)):\n",
        "\n",
        "            # TODO: Complete the loop. You should be able to complete this without any helper comments after 3 HWs\n",
        "            # Tip: Mixed precision training\n",
        "            # For loss calculation, use the calculate_loss function. You need to complete it before using.\n",
        "            self.optimizer.zero_grad()\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            out,_ = self.model(inputs)\n",
        "            loss = self.calculate_loss(out, targets)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            self.scheduler.step(loss)\n",
        "\n",
        "            num_batches += 1\n",
        "\n",
        "\n",
        "            loss = loss.item()\n",
        "            epoch_loss += loss\n",
        "        \n",
        "        epoch_loss = epoch_loss / (batch_num + 1)\n",
        "        self.epochs += 1\n",
        "        print('[TRAIN] \\tEpoch [%d/%d] \\tLoss: %.4f \\tLr: %.6f'\n",
        "                      % (self.epochs, self.max_epochs, epoch_loss, self.optimizer.param_groups[0]['lr']))\n",
        "        self.train_losses.append(epoch_loss)\n",
        "\n",
        "\n",
        "    \n",
        "    def test(self): # Don't change this function\n",
        "        \n",
        "        self.model.eval() # set to eval mode\n",
        "        predictions     = self.model.predict(fixtures_pred['inp']).detach().cpu().numpy() # get predictions\n",
        "        self.predictions.append(predictions)\n",
        "\n",
        "        generated_logits        = self.model.generate(fixtures_gen, 10).detach().cpu().numpy() # generated predictions for 10 words\n",
        "        generated_logits_test   = self.model.generate(fixtures_gen_test, 10).detach().cpu().numpy()\n",
        "\n",
        "        nll             = test_prediction(predictions, fixtures_pred['out'])\n",
        "        generated       = test_generation(fixtures_gen, generated_logits, VOCAB)\n",
        "        generated_test  = test_generation(fixtures_gen_test, generated_logits_test, VOCAB)\n",
        "        self.val_losses.append(nll)\n",
        "        \n",
        "        self.generated.append(generated)\n",
        "        self.generated_test.append(generated_test)\n",
        "        self.generated_logits.append(generated_logits)\n",
        "        self.generated_logits_test.append(generated_logits_test)\n",
        "        \n",
        "        # generate predictions for test data\n",
        "        predictions_test = self.model.predict(fixtures_pred_test['inp']).detach().cpu().numpy() # get predictions\n",
        "        self.predictions_test.append(predictions_test)\n",
        "            \n",
        "        print('[VAL] \\tEpoch [%d/%d] \\tLoss: %.4f'\n",
        "                      % (self.epochs, self.max_epochs, nll))\n",
        "        return nll\n",
        "\n",
        "    \n",
        "    def save(self): # Don't change this function\n",
        "\n",
        "        model_path = os.path.join('hw4/experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
        "        torch.save({'state_dict': self.model.state_dict()}, model_path)\n",
        "        np.save(os.path.join('hw4/experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
        "        np.save(os.path.join('hw4/experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
        "        np.save(os.path.join('hw4/experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
        "        np.save(os.path.join('hw4/experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
        "        \n",
        "        with open(os.path.join('hw4/experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated[-1])\n",
        "\n",
        "        with open(os.path.join('hw4/experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated_test[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "db5de3ac0c6e48ca9cff0cd79bef7ae8",
        "deepnote_cell_type": "markdown",
        "id": "E6NKG0j8zsv-"
      },
      "source": [
        "# Experiment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {
        "cell_id": "7fc44ee4771a42f996d0a00d35529fb6",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": true,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:13.852171Z",
          "iopub.status.busy": "2022-08-10T14:02:13.850633Z",
          "iopub.status.idle": "2022-08-10T14:02:13.927227Z",
          "shell.execute_reply": "2022-08-10T14:02:13.924500Z",
          "shell.execute_reply.started": "2022-08-10T14:02:13.852093Z"
        },
        "id": "TiUrjbEjSwRQ",
        "source_hash": "f7524436",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TODO: define other hyperparameters here\n",
        "\n",
        "configs = dict(\n",
        "    batch_size  = 256,\n",
        "    num_epochs  = 10, # 10 or 20 epochs should be enough given the model is good\n",
        "\n",
        "    init_lr     = 0.001 # TODO\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "cell_id": "4aaccf1c32fa480a9a15e8bb8bc4d9e4",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": true,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:14.110787Z",
          "iopub.status.busy": "2022-08-10T14:02:14.109778Z",
          "iopub.status.idle": "2022-08-10T14:02:14.929087Z",
          "shell.execute_reply": "2022-08-10T14:02:14.925078Z",
          "shell.execute_reply.started": "2022-08-10T14:02:14.110707Z"
        },
        "id": "DbHH6zXTSwRa",
        "source_hash": "2acff566",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LanguageModel(\n",
            "  (token_embedding): Embedding(33280, 128)\n",
            "  (lstm_cells): Sequential(\n",
            "    (0): LSTMCell(128, 128)\n",
            "    (1): LSTMCell(128, 128)\n",
            "  )\n",
            "  (token_probability): Linear(in_features=128, out_features=33280, bias=True)\n",
            ")\n",
            "===========================================================================\n",
            "                          Kernel Shape   Output Shape    Params  Mult-Adds\n",
            "Layer                                                                     \n",
            "0_token_embedding         [128, 33280]  [32, 20, 128]  4.25984M    4259840\n",
            "1_lstm_cells.LSTMCell_0              -      [32, 128]  132.096k     131072\n",
            "2_lstm_cells.LSTMCell_1              -      [32, 128]  132.096k     131072\n",
            "3_token_probability       [128, 33280]    [32, 33280]  4.29312M    4259840\n",
            "4_lstm_cells.LSTMCell_0              -      [32, 128]         -     131072\n",
            "5_lstm_cells.LSTMCell_1              -      [32, 128]         -     131072\n",
            "6_token_probability       [128, 33280]    [32, 33280]         -    4259840\n",
            "7_lstm_cells.LSTMCell_0              -      [32, 128]         -     131072\n",
            "8_lstm_cells.LSTMCell_1              -      [32, 128]         -     131072\n",
            "9_token_probability       [128, 33280]    [32, 33280]         -    4259840\n",
            "10_lstm_cells.LSTMCell_0             -      [32, 128]         -     131072\n",
            "11_lstm_cells.LSTMCell_1             -      [32, 128]         -     131072\n",
            "12_token_probability      [128, 33280]    [32, 33280]         -    4259840\n",
            "13_lstm_cells.LSTMCell_0             -      [32, 128]         -     131072\n",
            "14_lstm_cells.LSTMCell_1             -      [32, 128]         -     131072\n",
            "15_token_probability      [128, 33280]    [32, 33280]         -    4259840\n",
            "16_lstm_cells.LSTMCell_0             -      [32, 128]         -     131072\n",
            "17_lstm_cells.LSTMCell_1             -      [32, 128]         -     131072\n",
            "18_token_probability      [128, 33280]    [32, 33280]         -    4259840\n",
            "19_lstm_cells.LSTMCell_0             -      [32, 128]         -     131072\n",
            "20_lstm_cells.LSTMCell_1             -      [32, 128]         -     131072\n",
            "21_token_probability      [128, 33280]    [32, 33280]         -    4259840\n",
            "22_lstm_cells.LSTMCell_0             -      [32, 128]         -     131072\n",
            "23_lstm_cells.LSTMCell_1             -      [32, 128]         -     131072\n",
            "24_token_probability      [128, 33280]    [32, 33280]         -    4259840\n",
            "25_lstm_cells.LSTMCell_0             -      [32, 128]         -     131072\n",
            "26_lstm_cells.LSTMCell_1             -      [32, 128]         -     131072\n",
            "27_token_probability      [128, 33280]    [32, 33280]         -    4259840\n",
            "28_lstm_cells.LSTMCell_0             -      [32, 128]         -     131072\n",
            "29_lstm_cells.LSTMCell_1             -      [32, 128]         -     131072\n",
            "30_token_probability      [128, 33280]    [32, 33280]         -    4259840\n",
            "31_lstm_cells.LSTMCell_0             -      [32, 128]         -     131072\n",
            "32_lstm_cells.LSTMCell_1             -      [32, 128]         -     131072\n",
            "33_token_probability      [128, 33280]    [32, 33280]         -    4259840\n",
            "34_lstm_cells.LSTMCell_0             -      [32, 128]         -     131072\n",
            "35_lstm_cells.LSTMCell_1             -      [32, 128]         -     131072\n",
            "36_token_probability      [128, 33280]    [32, 33280]         -    4259840\n",
            "37_lstm_cells.LSTMCell_0             -      [32, 128]         -     131072\n",
            "38_lstm_cells.LSTMCell_1             -      [32, 128]         -     131072\n",
            "39_token_probability      [128, 33280]    [32, 33280]         -    4259840\n",
            "40_lstm_cells.LSTMCell_0             -      [32, 128]         -     131072\n",
            "41_lstm_cells.LSTMCell_1             -      [32, 128]         -     131072\n",
            "42_token_probability      [128, 33280]    [32, 33280]         -    4259840\n",
            "43_lstm_cells.LSTMCell_0             -      [32, 128]         -     131072\n",
            "44_lstm_cells.LSTMCell_1             -      [32, 128]         -     131072\n",
            "45_token_probability      [128, 33280]    [32, 33280]         -    4259840\n",
            "46_lstm_cells.LSTMCell_0             -      [32, 128]         -     131072\n",
            "47_lstm_cells.LSTMCell_1             -      [32, 128]         -     131072\n",
            "48_token_probability      [128, 33280]    [32, 33280]         -    4259840\n",
            "49_lstm_cells.LSTMCell_0             -      [32, 128]         -     131072\n",
            "50_lstm_cells.LSTMCell_1             -      [32, 128]         -     131072\n",
            "51_token_probability      [128, 33280]    [32, 33280]         -    4259840\n",
            "52_lstm_cells.LSTMCell_0             -      [32, 128]         -     131072\n",
            "53_lstm_cells.LSTMCell_1             -      [32, 128]         -     131072\n",
            "54_token_probability      [128, 33280]    [32, 33280]         -    4259840\n",
            "55_lstm_cells.LSTMCell_0             -      [32, 128]         -     131072\n",
            "56_lstm_cells.LSTMCell_1             -      [32, 128]         -     131072\n",
            "57_token_probability      [128, 33280]    [32, 33280]         -    4259840\n",
            "58_lstm_cells.LSTMCell_0             -      [32, 128]         -     131072\n",
            "59_lstm_cells.LSTMCell_1             -      [32, 128]         -     131072\n",
            "60_token_probability      [128, 33280]    [32, 33280]         -    4259840\n",
            "---------------------------------------------------------------------------\n",
            "                         Totals\n",
            "Total params          8.817152M\n",
            "Trainable params      8.817152M\n",
            "Non-trainable params        0.0\n",
            "Mult-Adds             94.69952M\n",
            "===========================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  df_sum = df.sum()\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_token_embedding</th>\n",
              "      <td>[128, 33280]</td>\n",
              "      <td>[32, 20, 128]</td>\n",
              "      <td>4259840.0</td>\n",
              "      <td>4259840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_lstm_cells.LSTMCell_0</th>\n",
              "      <td>-</td>\n",
              "      <td>[32, 128]</td>\n",
              "      <td>132096.0</td>\n",
              "      <td>131072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_lstm_cells.LSTMCell_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[32, 128]</td>\n",
              "      <td>132096.0</td>\n",
              "      <td>131072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_token_probability</th>\n",
              "      <td>[128, 33280]</td>\n",
              "      <td>[32, 33280]</td>\n",
              "      <td>4293120.0</td>\n",
              "      <td>4259840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_lstm_cells.LSTMCell_0</th>\n",
              "      <td>-</td>\n",
              "      <td>[32, 128]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>131072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56_lstm_cells.LSTMCell_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[32, 128]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>131072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57_token_probability</th>\n",
              "      <td>[128, 33280]</td>\n",
              "      <td>[32, 33280]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4259840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58_lstm_cells.LSTMCell_0</th>\n",
              "      <td>-</td>\n",
              "      <td>[32, 128]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>131072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59_lstm_cells.LSTMCell_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[32, 128]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>131072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60_token_probability</th>\n",
              "      <td>[128, 33280]</td>\n",
              "      <td>[32, 33280]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4259840</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>61 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                          Kernel Shape   Output Shape     Params  Mult-Adds\n",
              "Layer                                                                      \n",
              "0_token_embedding         [128, 33280]  [32, 20, 128]  4259840.0    4259840\n",
              "1_lstm_cells.LSTMCell_0              -      [32, 128]   132096.0     131072\n",
              "2_lstm_cells.LSTMCell_1              -      [32, 128]   132096.0     131072\n",
              "3_token_probability       [128, 33280]    [32, 33280]  4293120.0    4259840\n",
              "4_lstm_cells.LSTMCell_0              -      [32, 128]        NaN     131072\n",
              "...                                ...            ...        ...        ...\n",
              "56_lstm_cells.LSTMCell_1             -      [32, 128]        NaN     131072\n",
              "57_token_probability      [128, 33280]    [32, 33280]        NaN    4259840\n",
              "58_lstm_cells.LSTMCell_0             -      [32, 128]        NaN     131072\n",
              "59_lstm_cells.LSTMCell_1             -      [32, 128]        NaN     131072\n",
              "60_token_probability      [128, 33280]    [32, 33280]        NaN    4259840\n",
              "\n",
              "[61 rows x 4 columns]"
            ]
          },
          "execution_count": 275,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model       = LanguageModel(vocab_size = len(VOCAB), embed_dim = 128, hid_dim = 128)\n",
        "# TODO: Define the model\n",
        "\n",
        "loader      = DataLoaderForLanguageModeling(dataset, batch_size = 32) # TODO: Define the dataloader\n",
        "\n",
        "criterion   = torch.nn.CrossEntropyLoss() \n",
        "\n",
        "optimizer   = torch.optim.Adam(model.parameters(), lr=configs['init_lr'])\n",
        "# TODO: Define the optimizer. Adam/AdamW usually works good for this HW\n",
        "\n",
        "# Optional TODO: Use a scheduler if you want\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode= 'min', patience =2)\n",
        "\n",
        "print(model)\n",
        "torchsummaryX.summary(model, x = inputs.to(DEVICE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {
        "cell_id": "aaff53cf948e44b7b9bd49cbcad0ac58",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": true,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:13.931258Z",
          "iopub.status.busy": "2022-08-10T14:02:13.930204Z",
          "iopub.status.idle": "2022-08-10T14:02:14.107883Z",
          "shell.execute_reply": "2022-08-10T14:02:14.105987Z",
          "shell.execute_reply.started": "2022-08-10T14:02:13.931185Z"
        },
        "id": "2HCVG5YISwRW",
        "source_hash": "c9f4594a",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving models, predictions, and generated words to ./hw4/experiments/1682215502\n"
          ]
        }
      ],
      "source": [
        "# Dont change this cell\n",
        "\n",
        "run_id = str(int(time.time()))\n",
        "if not os.path.exists('./hw4/experiments'):\n",
        "    os.mkdir('./hw4/experiments')\n",
        "os.mkdir('./hw4/experiments/%s' % run_id)\n",
        "print(\"Saving models, predictions, and generated words to ./hw4/experiments/%s\" % run_id)\n",
        "\n",
        "# The object of the Trainer class takes in everything\n",
        "trainer = Trainer(\n",
        "    model       = model, \n",
        "    loader      = loader, \n",
        "    \n",
        "\n",
        "    optimizer   = optimizer,\n",
        "    criterion   = criterion, \n",
        "    scheduler   = scheduler,\n",
        "    max_epochs  = configs['num_epochs'], \n",
        "    run_id      = run_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {
        "id": "V0dy4CpXJgN2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3c85cd7d4b04eacbb12b84a3c73d953",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3245 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-277-d915c446962e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mbest_nll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mnll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnll\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_nll\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-273-a207d179c0a6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Run the experiments loop. \n",
        "# Each epoch wont take more than 2-3min. If its taking more time, it might be due to (but not limited to) the following:\n",
        "#   * You might be overlapping batches \n",
        "#       Eg. Input: \"I had biryani for lunch today\" and sequence length = 3,\n",
        "#           --> \"I had biryani\", \"for lunch today\" are ideal examples for inputs\n",
        "#           --> \"I had biryani\", \"had biryani for\", \"biryani for lunch\", ... is just redundant info :')\n",
        "#   * Your length calculation in the dataloader might be wrong\n",
        "# If you haven't had biryani, try it :D \n",
        "\n",
        "# %%time\n",
        "best_nll = 1e30 \n",
        "for epoch in range(configs['num_epochs']):\n",
        "    trainer.train()\n",
        "    nll = trainer.test()\n",
        "    if nll < best_nll:\n",
        "        best_nll = nll\n",
        "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch+1)+\" with NLL: \"+ str(best_nll))\n",
        "        trainer.save()    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_gYqXq9Jgo1"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses[0:10], label='Validation losses')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('NLL')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBqjqy-EyU27"
      },
      "source": [
        "# Create handin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWRyPvWmgLQs"
      },
      "outputs": [],
      "source": [
        "# TODO: Generate the handin to submit to autolab\n",
        "!make runid=<ENTER RUN ID> epoch=<ENTER EPOCH NUMBER>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "EB2bOV3bzYLR",
        "INh9p3v3zbF_",
        "u-R794-0zc9V"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "989a3c3836794109ac641230122845a3",
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
