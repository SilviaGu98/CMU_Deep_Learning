{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR4qfYrVoO4v"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA9qZoIDcx-h"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "J2U59HbRLEoE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SS7a7xeEoaV9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "152b508a-5304-4706-e689-f3ca817db73b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ctcdecode' already exists and is not an empty directory.\n",
            "/content/ctcdecode\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ctcdecode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb --quiet\n",
        "!pip install python-Levenshtein -q\n",
        "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "!pip install wget -q\n",
        "%cd ctcdecode\n",
        "!pip install . -q\n",
        "%cd ..\n",
        "\n",
        "!pip install torchsummaryX -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWVONJxCobPc"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78ZTCIXoof2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f9d2109-43c7-4cd8-fa5f-50ef1f99682b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torchaudio.transforms as tat\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# imports for decoding and distance calculation\n",
        "import ctcdecode\n",
        "import Levenshtein\n",
        "from ctcdecode import CTCBeamDecoder\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg3-yJ8tok34"
      },
      "source": [
        "# Kaggle Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdUelfGhom1m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb13d8ba-2724-42f4-c447-b9eca5a33553"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kaggle==1.5.8\n",
            "  Using cached kaggle-1.5.8-py3-none-any.whl\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.8\n",
            "    Uninstalling kaggle-1.5.8:\n",
            "      Successfully uninstalled kaggle-1.5.8\n",
            "Successfully installed kaggle-1.5.8\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"yunxingu\",\"key\":\"b0d791b8d8e9237e047196d9e5f3c070\"}') # TODO: Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSjBwfXeoq4B"
      },
      "outputs": [],
      "source": [
        "# !kaggle competitions download -c 11-785-s23-hw3p2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ruxWP60LCQA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1ae171c8-7214-4fa6-d3a1-96a8bb2ace3e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThis will take a couple minutes, but you should see at least the following:\\n11-785-f22-hw3p2.zip  ctcdecode  hw3p2\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "'''\n",
        "This will take a couple minutes, but you should see at least the following:\n",
        "11-785-f22-hw3p2.zip  ctcdecode  hw3p2\n",
        "'''\n",
        "# !unzip -q 11-785-s23-hw3p2.zip\n",
        "# !ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio"
      ],
      "metadata": {
        "id": "9YOxKqhe31tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9v5ewZDMpYA"
      },
      "source": [
        "# Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Cp-716IMZRd"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ORNHnSFroP0"
      },
      "source": [
        "# Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0v7wHRWrqH6"
      },
      "outputs": [],
      "source": [
        "# ARPABET PHONEME MAPPING\n",
        "# DO NOT CHANGE\n",
        "# This overwrites the phonetics.py file.\n",
        "\n",
        "CMUdict_ARPAbet = {\n",
        "    \"\" : \" \",\n",
        "    \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\", \n",
        "    \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\", \n",
        "    \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\", \n",
        "    \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\", \n",
        "    \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\", \n",
        "    \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\", \n",
        "    \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\", \n",
        "    \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n",
        "    \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"\n",
        "}\n",
        "\n",
        "CMUdict = list(CMUdict_ARPAbet.keys())\n",
        "ARPAbet = list(CMUdict_ARPAbet.values())\n",
        "\n",
        "\n",
        "PHONEMES = CMUdict[:-2]\n",
        "LABELS = ARPAbet[:-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eN2kcxwXLLBb"
      },
      "outputs": [],
      "source": [
        "# You might want to play around with the mapping as a sanity check here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agmNBKf4JrLV"
      },
      "source": [
        "### Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afd0_vlbJmr_"
      },
      "outputs": [],
      "source": [
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    # For this homework, we give you full flexibility to design your data set class.\n",
        "    # Hint: The data from HW1 is very similar to this HW\n",
        "\n",
        "    #TODO\n",
        "    def __init__(self, root, PHONEMES, BATCH_SIZE): \n",
        "        '''\n",
        "        Initializes the dataset.\n",
        "\n",
        "        INPUTS: What inputs do you need here?\n",
        "        '''\n",
        "\n",
        "        # Load the directory and all files in them\n",
        "\n",
        "        self.mfcc_dir = root+'/mfcc/' \n",
        "        self.transcript_dir = root+'/transcript/' \n",
        "\n",
        "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
        "        self.transcript_files = sorted(os.listdir(self.transcript_dir))\n",
        "\n",
        "        self.PHONEMES = PHONEMES\n",
        "\n",
        "        #TODO\n",
        "        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n",
        "        self.length = len(self.mfcc_files)\n",
        "        \n",
        "        #TODO\n",
        "        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n",
        "        # HINT: TENSORS CANNOT STORE NON-NUMERICAL VALUES OR STRINGS\n",
        "\n",
        "        #TODO\n",
        "        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n",
        "        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "\n",
        "        # Iterate through mfccs and transcripts\n",
        "        for i in range(len(self.mfcc_files)):\n",
        "        #   Load a single mfcc\n",
        "            mfcc        = np.load(self.mfcc_dir + self.mfcc_files[i])\n",
        "        #   Do Cepstral Normalization of mfcc (explained in writeup)\n",
        "\n",
        "            normalized_mfcc = (mfcc - np.mean(mfcc, axis = 0))/np.std(mfcc, axis = 0)\n",
        "        #   Load the corresponding transcript\n",
        "            transcript  = np.load(self.transcript_dir + self.transcript_files[i]) \n",
        "            transcript = transcript[1:-1] # Remove [SOS] and [EOS] from the transcript \n",
        "            transcript = [self.PHONEMES.index(p) for p in transcript]\n",
        "\n",
        "            \n",
        "\n",
        "        #   Append each mfcc to self.mfcc, transcript to self.transcript\n",
        "            self.mfccs.append(mfcc)\n",
        "            self.transcripts.append(transcript)   \n",
        "        '''\n",
        "        You may decide to do this in __getitem__ if you wish.\n",
        "        However, doing this here will make the __init__ function take the load of\n",
        "        loading the data, and shift it away from training.\n",
        "        '''\n",
        "       \n",
        "\n",
        "    def __len__(self):\n",
        "        \n",
        "        '''\n",
        "        TODO: What do we return here?\n",
        "        '''\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        '''\n",
        "        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
        "\n",
        "        If you didn't do the loading and processing of the data in __init__,\n",
        "        do that here.\n",
        "\n",
        "        Once done, return a tuple of features and labels.\n",
        "        '''\n",
        "\n",
        "        mfcc = self.mfccs[ind] # TODO\n",
        "        transcript = self.transcripts[ind] # TODO\n",
        "        return torch.FloatTensor(mfcc), torch.tensor(transcript)\n",
        "\n",
        "\n",
        "    def collate_fn(self,batch):\n",
        "        '''\n",
        "        TODO:\n",
        "        1.  Extract the features and labels from 'batch'\n",
        "        2.  We will additionally need to pad both features and labels,\n",
        "            look at pytorch's docs for pad_sequence\n",
        "        3.  This is a good place to perform transforms, if you so wish. \n",
        "            Performing them on batches will speed the process up a bit.\n",
        "        4.  Return batch of features, labels, lenghts of features, \n",
        "            and lengths of labels.\n",
        "          '''\n",
        "        batch_mfcc, batch_transcript, = [], []\n",
        "        lengths_mfcc, lengths_transcript = [], []\n",
        "        mfcc_maxlen, trans_maxlen = 0,0\n",
        "        for (_mfcc,_transcript) in batch:\n",
        "          batch_mfcc.append(_mfcc)\n",
        "          lengths_mfcc.append(len(_mfcc))\n",
        "          batch_transcript.append(_transcript)\n",
        "          lengths_transcript.append(len(_transcript))\n",
        "        \n",
        "       \n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first = True)\n",
        "        \n",
        "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first = True)\n",
        "\n",
        "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
        "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
        "        #                  -> Would we apply transformation on the validation set as well?\n",
        "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
        "        \n",
        "        audio_transforms = nn.Sequential(\n",
        "            PermuteBlock(), \n",
        "            torchaudio.transforms.FrequencyMasking(freq_mask_param=5),\n",
        "            torchaudio.transforms.TimeMasking(time_mask_param=100),\n",
        "            PermuteBlock()\n",
        "        )\n",
        "        \n",
        "        batch_mfcc_pad = audio_transforms(batch_mfcc_pad)\n",
        "        \n",
        "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
        "        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)\n",
        "\n",
        "       "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZmZioQdW__R_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0DFpDxX__oF"
      },
      "outputs": [],
      "source": [
        "class AudioDatasetValid(torch.utils.data.Dataset):\n",
        "\n",
        "    # For this homework, we give you full flexibility to design your data set class.\n",
        "    # Hint: The data from HW1 is very similar to this HW\n",
        "\n",
        "    #TODO\n",
        "    def __init__(self, root, PHONEMES, BATCH_SIZE): \n",
        "        '''\n",
        "        Initializes the dataset.\n",
        "\n",
        "        INPUTS: What inputs do you need here?\n",
        "        '''\n",
        "\n",
        "        # Load the directory and all files in them\n",
        "\n",
        "        self.mfcc_dir = root+'/mfcc/' \n",
        "        self.transcript_dir = root+'/transcript/' \n",
        "\n",
        "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
        "        self.transcript_files = sorted(os.listdir(self.transcript_dir))\n",
        "\n",
        "        self.PHONEMES = PHONEMES\n",
        "\n",
        "        #TODO\n",
        "        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n",
        "        self.length = len(self.mfcc_files)\n",
        "        \n",
        "        #TODO\n",
        "        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n",
        "        # HINT: TENSORS CANNOT STORE NON-NUMERICAL VALUES OR STRINGS\n",
        "\n",
        "        #TODO\n",
        "        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n",
        "        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "\n",
        "        # Iterate through mfccs and transcripts\n",
        "        for i in range(len(self.mfcc_files)):\n",
        "        #   Load a single mfcc\n",
        "            mfcc        = np.load(self.mfcc_dir + self.mfcc_files[i])\n",
        "        #   Do Cepstral Normalization of mfcc (explained in writeup)\n",
        "\n",
        "            normalized_mfcc = (mfcc - np.mean(mfcc, axis = 0))/np.std(mfcc, axis = 0)\n",
        "        #   Load the corresponding transcript\n",
        "            transcript  = np.load(self.transcript_dir + self.transcript_files[i]) \n",
        "            transcript = transcript[1:-1] # Remove [SOS] and [EOS] from the transcript \n",
        "            transcript = [self.PHONEMES.index(p) for p in transcript]\n",
        "\n",
        "            \n",
        "\n",
        "        #   Append each mfcc to self.mfcc, transcript to self.transcript\n",
        "            self.mfccs.append(mfcc)\n",
        "            self.transcripts.append(transcript)   \n",
        "        '''\n",
        "        You may decide to do this in __getitem__ if you wish.\n",
        "        However, doing this here will make the __init__ function take the load of\n",
        "        loading the data, and shift it away from training.\n",
        "        '''\n",
        "       \n",
        "\n",
        "    def __len__(self):\n",
        "        \n",
        "        '''\n",
        "        TODO: What do we return here?\n",
        "        '''\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        '''\n",
        "        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
        "\n",
        "        If you didn't do the loading and processing of the data in __init__,\n",
        "        do that here.\n",
        "\n",
        "        Once done, return a tuple of features and labels.\n",
        "        '''\n",
        "\n",
        "        mfcc = self.mfccs[ind] # TODO\n",
        "        transcript = self.transcripts[ind] # TODO\n",
        "        return torch.FloatTensor(mfcc), torch.tensor(transcript)\n",
        "\n",
        "\n",
        "    def collate_fn(self,batch):\n",
        "\n",
        "        batch_mfcc, batch_transcript, = [], []\n",
        "        lengths_mfcc, lengths_transcript = [], []\n",
        "        mfcc_maxlen, trans_maxlen = 0,0\n",
        "        for (_mfcc,_transcript) in batch:\n",
        "          batch_mfcc.append(_mfcc)\n",
        "          # mfcc_maxlen = max(mfcc_maxlen, len(_mfcc))\n",
        "          lengths_mfcc.append(len(_mfcc))\n",
        "          batch_transcript.append(_transcript)\n",
        "          # trans_maxlen = max(trans_maxlen, len(_transcript))\n",
        "          lengths_transcript.append(len(_transcript))\n",
        "        \n",
        "       \n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first = True)\n",
        "        \n",
        "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first = True)\n",
        "\n",
        "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
        "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
        "        #                  -> Would we apply transformation on the validation set as well?\n",
        "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
        "        \n",
        "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
        "        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)\n",
        "\n",
        "       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqDrxeHfJw4g"
      },
      "source": [
        "### Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrLS1wfVJppA"
      },
      "outputs": [],
      "source": [
        "# Test Dataloader\n",
        "#TODO\n",
        "class AudioDatasetTest(torch.utils.data.Dataset):\n",
        "  def __init__(self, root, BATCH_SIZE):\n",
        "        self.mfcc_dir = root+'/mfcc/' \n",
        "        \n",
        "\n",
        "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
        "        \n",
        "        #TODO\n",
        "        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n",
        "        self.length = len(self.mfcc_files)\n",
        "        self.mfccs = []\n",
        "\n",
        "        # Iterate through mfccs and transcripts\n",
        "        for i in range(len(self.mfcc_files)):\n",
        "        #   Load a single mfcc\n",
        "            mfcc = np.load(self.mfcc_dir + self.mfcc_files[i])\n",
        "            normalized_mfcc = (mfcc - np.mean(mfcc, axis = 0))/np.std(mfcc, axis = 0)\n",
        "            self.mfccs.append(mfcc)\n",
        "       \n",
        "\n",
        "  def __len__(self):\n",
        "      \n",
        "      '''\n",
        "      TODO: What do we return here?\n",
        "      '''\n",
        "      return self.length\n",
        "\n",
        "  def __getitem__(self, ind):\n",
        "      \n",
        "      mfcc = self.mfccs[ind] # TODO\n",
        "      \n",
        "      return torch.FloatTensor(mfcc)\n",
        "  \n",
        "  def collate_fn(self,batch):\n",
        "\n",
        "        batch_mfcc = []\n",
        "        lengths_mfcc = []\n",
        "        \n",
        "        for mfcc in batch:\n",
        "          batch_mfcc.append(mfcc)\n",
        "          \n",
        "          lengths_mfcc.append(len(mfcc))\n",
        "        \n",
        "    \n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first = True)\n",
        "        \n",
        "\n",
        "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
        "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
        "        #                  -> Would we apply transformation on the validation set as well?\n",
        "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
        "        \n",
        "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
        "        return batch_mfcc_pad, torch.tensor(lengths_mfcc)\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt-veYcdL6Fe"
      },
      "source": [
        "### Data - Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4icymeX1ImUN"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128 # Increase if your device can handle it\n",
        "\n",
        "# transforms = [] # set of tranformations\n",
        "# You may pass this as a parameter to the dataset class above\n",
        "# This will help modularize your implementation\n",
        "\n",
        "# root = '/content/11-785-s23-hw3p2/train_100'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "YxuYbIBp5S3X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "044bfa16-1c66-4ddc-b1e6-0a2c66c5a762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11-785-s23-hw3p2\t\t    ctcdecode\t\t\tmodel\n",
            "11-785-s23-hw3p2.zip\t\t    data\t\t\tsample_data\n",
            "best_model.pt\t\t\t    empatheticdialogues.tar.gz\tsubmission.csv\n",
            "body_data-20230330T051611Z-001.zip  epoch_model.pt\t\twandb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmuPk9J6L8dz"
      },
      "source": [
        "### Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CzQmmDutK8h-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_kG0gU2x4hH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f094a2-f954-4762-cf5d-5d904654d46f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# get me RAMMM!!!! \n",
        "import gc \n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd4BEX_yMUzz"
      },
      "outputs": [],
      "source": [
        "# Utils for network\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "class PermuteBlock(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.transpose(1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mzoYfTKu14s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddeef2ad-8cd4-4c47-a4a4-de61da5a2e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size:  128\n",
            "Train dataset samples = 104013, batches = 813\n",
            "Val dataset samples = 2703, batches = 22\n",
            "Test dataset samples = 2620, batches = 2620\n"
          ]
        }
      ],
      "source": [
        "# Create objects for the dataset class\n",
        "train_data = AudioDataset('/content/11-785-s23-hw3p2/train-clean-360', PHONEMES, BATCH_SIZE) #TODO\n",
        "val_data =  AudioDatasetValid('/content/11-785-s23-hw3p2/dev-clean', PHONEMES, BATCH_SIZE) # TODO : You can either use the same class with some modifications or make a new one :)\n",
        "test_data = AudioDatasetTest('/content/11-785-s23-hw3p2/test-clean', 1) #TODO\n",
        "\n",
        "# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_data, \n",
        "    num_workers = 4,\n",
        "    batch_size  = BATCH_SIZE, \n",
        "    pin_memory  = True,\n",
        "    shuffle     = True,\n",
        "    collate_fn = train_data.collate_fn\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = val_data, \n",
        "    num_workers = 4,\n",
        "    batch_size  = BATCH_SIZE, \n",
        "    pin_memory  = True,\n",
        "    shuffle     = False,\n",
        "    collate_fn = val_data.collate_fn\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = test_data, \n",
        "    num_workers = 4,\n",
        "    batch_size  = 1, \n",
        "    pin_memory  = True,\n",
        "    shuffle     = False,\n",
        "    collate_fn = test_data.collate_fn\n",
        ")\n",
        "print(\"Batch size: \", BATCH_SIZE)\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXMtwyviKaxK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14e8acae-46a2-4efa-f044-6036c8fa1ca1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 1701, 27]) torch.Size([128, 220]) torch.Size([128]) torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "for data in train_loader:\n",
        "    x, y, lx, ly = data\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    break "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSexxhdfMUzx"
      },
      "source": [
        "# NETWORK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLad4pChcuvX"
      },
      "source": [
        "## Basic\n",
        "\n",
        "This is a basic block for understanding, you can skip this and move to pBLSTM one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQhvHr71GJfq"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        # Adding some sort of embedding layer or feature extractor might help performance.\n",
        "        # permutation?\n",
        "        self.embedding = nn.Conv1d(in_channels= 27, out_channels = 64, kernel_size = 3, stride = 1, padding = 1)\n",
        "       \n",
        "        \n",
        "        # TODO : look up the documentation. You might need to pass some additional parameters.\n",
        "        #pack\n",
        "        self.lstm = nn.LSTM(input_size = 64, hidden_size = 256, num_layers = 1, \n",
        "                            bidirectional = True, batch_first = True) \n",
        "       \n",
        "        self.classification = nn.Sequential(\n",
        "            #TODO: Linear layer with in_features from the lstm module above and out_features = OUT_SIZE\n",
        "            nn.Linear(512, 41)\n",
        "        )\n",
        "\n",
        "        \n",
        "        self.logSoftmax = nn.LogSoftmax(dim = -1)#TODO: Apply a log softmax here. Which dimension would apply it on ?\n",
        "\n",
        "    def forward(self, x, lx):\n",
        "        #TODO\n",
        "        # The forward function takes 2 parameter inputs here. Why?\n",
        "        x = torch.permute(x, (0, 2, 1))\n",
        "        x = self.embedding(x)\n",
        "        x = torch.permute(x, (0, 2, 1))\n",
        "\n",
        "        clamped_lx = lx.clamp(max=x.shape[1]) # to match the embedding\n",
        "\n",
        "        x = pack_padded_sequence(x,clamped_lx, batch_first = True, enforce_sorted= False)\n",
        "        \n",
        "        x , hidden= self.lstm(x)\n",
        "        output, input_sizes = pad_packed_sequence(x, batch_first = True)\n",
        "        output = self.classification(output)\n",
        "        out = self.logSoftmax(output)\n",
        "\n",
        "        # Refer to the handout for hints\n",
        "        return out,lx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB6eh3gnMUzy"
      },
      "source": [
        "## Pyramid Bi-LSTM (pBLSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmdyXI6KMUzz"
      },
      "outputs": [],
      "source": [
        "class pBLSTM(torch.nn.Module):\n",
        "\n",
        "    '''\n",
        "    Pyramidal BiLSTM\n",
        "    Read the write up/paper and understand the concepts and then write your implementation here.\n",
        "\n",
        "    At each step,\n",
        "    1. Pad your input if it is packed (Unpack it)\n",
        "    2. Reduce the input length dimension by concatenating feature dimension\n",
        "        (Tip: Write down the shapes and understand)\n",
        "        (i) How should  you deal with odd/even length input? \n",
        "        (ii) How should you deal with input length array (x_lens) after truncating the input?\n",
        "    3. Pack your input\n",
        "    4. Pass it into LSTM layer\n",
        "\n",
        "    To make our implementation modular, we pass 1 layer at a time.\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(pBLSTM, self).__init__()\n",
        "\n",
        "        self.blstm = nn.LSTM(input_size = 2*input_size, hidden_size = hidden_size, num_layers = 1, \n",
        "                             bidirectional = True, dropout = 0.2, batch_first = True) \n",
        "        # TODO: Initialize a single layer bidirectional LSTM with the given input_size and hidden_size\n",
        "\n",
        "    def forward(self, x_packed): # x_packed is a PackedSequence\n",
        "\n",
        "        # TODO: Pad Packed Sequence\n",
        "        \n",
        "        x , lens_unpacked = pad_packed_sequence(x_packed, batch_first = True)\n",
        "        \n",
        "        # Call self.trunc_reshape() which downsamples the time steps of x and increases the feature dimensions as mentioned above\n",
        "        \n",
        "        # self.trunc_reshape will return 2 outputs. What are they? Think about what quantites are changing.\n",
        "        x, x_lens = self.trunc_reshape(x, lens_unpacked)\n",
        "        # TODO: Pack Padded Sequence. What output(s) would you get?\n",
        "        x = pack_padded_sequence(x, x_lens, batch_first = True, enforce_sorted= False)\n",
        "        # TODO: Pass the sequence through bLSTM\n",
        "        x , hidden= self.blstm(x)\n",
        "\n",
        "        # What do you return?\n",
        "\n",
        "        return x\n",
        "\n",
        "    def trunc_reshape(self, x, x_lens): \n",
        "        # TODO: If you have odd number of timesteps, how can you handle it? (Hint: You can exclude them)\n",
        "        # TODO: Reshape x. When reshaping x, you have to reduce number of timesteps by a downsampling factor while increasing number of features by the same factor\n",
        "        # TODO: Reduce lengths by the same downsampling factor\n",
        "        \n",
        "        if x.shape[1]%2 != 0:\n",
        "            x= x[:,:-1,:]\n",
        "\n",
        "        x = x.reshape(x.shape[0],x.shape[1]//2, x.shape[2]*2)\n",
        "        x_lens  = x_lens//2\n",
        "        # x_lens = torch.clamp(x_lens,max=x.shape[1])\n",
        "        # x_lens = torch.div(x_lens, 2)\n",
        "        return x, x_lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3ZQ75OcMUz0"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LockedDropout(nn.Module):\n",
        "    def __init__(self, drop_prob):\n",
        "        super(LockedDropout, self).__init__()\n",
        "        self.prob = drop_prob\n",
        "    def forward(self, x):\n",
        "        if not self.training or not self.prob: # turn it off during inference\n",
        "            return x\n",
        "        x, x_lens = pad_packed_sequence(x, batch_first = True)\n",
        "        m = x.new_empty(x.size(0), 1, x.size(2),requires_grad=False).bernoulli_(1 - self.prob)\n",
        "        mask = m / (1 - self.prob)\n",
        "        mask = mask.expand_as(x)\n",
        "        out = x * mask\n",
        "        out = pack_padded_sequence(out,x_lens, batch_first = True, enforce_sorted= False)\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "dN5HKackdtRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEzw5_xmMUz0"
      },
      "outputs": [],
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    '''\n",
        "    The Encoder takes utterances as inputs and returns latent feature representations\n",
        "    '''\n",
        "    def __init__(self, input_size, encoder_hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.expand_dim = [128, 256]\n",
        "                \n",
        "        self.embedding = torch.nn.Sequential(\n",
        "            PermuteBlock(), \n",
        "            nn.Conv1d(in_channels= input_size, out_channels = self.expand_dim[0], kernel_size = 3, stride = 1, padding = 1),\n",
        "            nn.BatchNorm1d(num_features =self.expand_dim[0]),\n",
        "            nn.GELU(),\n",
        "            nn.Conv1d(in_channels= self.expand_dim[0], out_channels = self.expand_dim[1], kernel_size = 3, stride = 1, padding = 1),\n",
        "            nn.BatchNorm1d(num_features =self.expand_dim[1]),\n",
        "            PermuteBlock())\n",
        "        #TODO: You can use CNNs as Embedding layer to extract features. Keep in mind the Input dimensions and expected dimension of Pytorch CNN.\n",
        "\n",
        "        self.pBLSTMs = torch.nn.Sequential( # How many pBLSTMs are required???\n",
        "            # TODO: Fill this up with pBLSTMs - What should the input_size be? \n",
        "            # Hint: You are downsampling timesteps by a factor of 2, upsampling features by a factor of 2 and the LSTM is bidirectional)\n",
        "            # Optional: Dropout/Locked Dropout after each pBLSTM (Not needed for early submission)\n",
        "            pBLSTM(input_size = self.expand_dim[1], hidden_size = encoder_hidden_size),\n",
        "            LockedDropout(0.4),\n",
        "            pBLSTM(input_size =  2*encoder_hidden_size, hidden_size = encoder_hidden_size),\n",
        "            LockedDropout(0.3)\n",
        "        )\n",
        "        # self.dropout = LockedDropout(0.2)\n",
        "        \n",
        "         \n",
        "    def forward(self, x, x_lens):\n",
        "        # Where are x and x_lens coming from? The dataloader\n",
        "        #TODO: Call the embedding layer\n",
        "        x = self.embedding(x)      \n",
        "        # TODO: Pack Padded Sequence\n",
        "        # TODO: Pass Sequence through the pyramidal Bi-LSTM layer\n",
        "        # TODO: Pad Packed Sequence\n",
        "\n",
        "        clamped_lx = x_lens.clamp(max=x.shape[1]) # to match the embedding\n",
        "\n",
        "        x = pack_padded_sequence(x,clamped_lx, batch_first = True, enforce_sorted= False)\n",
        "        \n",
        "        x = self.pBLSTMs(x)\n",
        "        \n",
        "        encoder_outputs, encoder_lens = pad_packed_sequence(x, batch_first = True)\n",
        "\n",
        "\n",
        "        # Remember the number of output(s) each function returns\n",
        "\n",
        "        return encoder_outputs, encoder_lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg82HXa3MUz1"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQIRxdNTMUz1"
      },
      "outputs": [],
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, output_size= 41):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "            PermuteBlock(), torch.nn.BatchNorm1d(embed_size), PermuteBlock(),\n",
        "            #TODO define your MLP arch. Refer HW1P2\n",
        "            #Use Permute Block before and after BatchNorm1d() to match the size\n",
        "            nn.Linear(embed_size, 2048),\n",
        "            nn.GELU(),\n",
        "            PermuteBlock(), torch.nn.BatchNorm1d(2048), PermuteBlock(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.GELU(),\n",
        "            PermuteBlock(), torch.nn.BatchNorm1d(1024), PermuteBlock(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(1024, output_size)\n",
        "        )\n",
        "        \n",
        "        self.softmax = torch.nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, encoder_out):\n",
        "        #TODO call your MLP\n",
        "        #TODO Think what should be the final output of the decoder for the classification \n",
        "        out = self.mlp(encoder_out)\n",
        "        out = self.softmax(out)\n",
        "\n",
        "        return out "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmHf6pFiMUz1"
      },
      "outputs": [],
      "source": [
        "class ASRModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, embed_size= 192, output_size= len(PHONEMES)):\n",
        "        super().__init__()\n",
        "\n",
        "        # self.augmentations  = torch.nn.Sequential(\n",
        "        #     #TODO Add Time Masking/ Frequency Masking\n",
        "        #     #Hint: See how to use PermuteBlock() function defined above\n",
        "        #     PermuteBlock(), \n",
        "        #     torchaudio.transforms.FrequencyMasking(freq_mask_param=10),\n",
        "        #     torchaudio.transforms.TimeMasking(time_mask_param=100),\n",
        "        #     PermuteBlock(),\n",
        "        # ) # did augmentation in the collate_fn\n",
        "        self.encoder        = Encoder(input_size, embed_size) # TODO: Initialize Encoder\n",
        "        self.decoder        = Decoder(embed_size*2, output_size) # TODO: Initialize Decoder \n",
        "\n",
        "        \n",
        "    \n",
        "    def forward(self, x, lengths_x):\n",
        "        \n",
        "        # if self.training:\n",
        "        #     x = self.augmentations(x)\n",
        "\n",
        "        encoder_out, encoder_lens   = self.encoder(x, lengths_x)\n",
        "        decoder_out                 = self.decoder(encoder_out)\n",
        "\n",
        "        return decoder_out, encoder_lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUThsowyQdN7"
      },
      "source": [
        "## INIT\n",
        "(If trying out the basic Network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGoiXd70tb5z"
      },
      "outputs": [],
      "source": [
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# model = Network().to(device)\n",
        "# summary(model, x.to(device), lx) # x and lx come from the sanity check above :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV7DMPDoMUz2"
      },
      "source": [
        "## INIT ASR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchsummaryX"
      ],
      "metadata": {
        "id": "mCGxhbIkMNLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaaDsnnLMUz2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dc3a2c97-bcb3-468c-c55d-9c3e0f4b6c93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ASRModel(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Sequential(\n",
            "      (0): PermuteBlock()\n",
            "      (1): Conv1d(27, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): GELU(approximate='none')\n",
            "      (4): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (6): PermuteBlock()\n",
            "    )\n",
            "    (pBLSTMs): Sequential(\n",
            "      (0): pBLSTM(\n",
            "        (blstm): LSTM(512, 512, batch_first=True, dropout=0.2, bidirectional=True)\n",
            "      )\n",
            "      (1): LockedDropout()\n",
            "      (2): pBLSTM(\n",
            "        (blstm): LSTM(2048, 512, batch_first=True, dropout=0.2, bidirectional=True)\n",
            "      )\n",
            "      (3): LockedDropout()\n",
            "    )\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (mlp): Sequential(\n",
            "      (0): PermuteBlock()\n",
            "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): PermuteBlock()\n",
            "      (3): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "      (4): GELU(approximate='none')\n",
            "      (5): PermuteBlock()\n",
            "      (6): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (7): PermuteBlock()\n",
            "      (8): Dropout(p=0.2, inplace=False)\n",
            "      (9): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "      (10): GELU(approximate='none')\n",
            "      (11): PermuteBlock()\n",
            "      (12): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (13): PermuteBlock()\n",
            "      (14): Dropout(p=0.2, inplace=False)\n",
            "      (15): Linear(in_features=1024, out_features=41, bias=True)\n",
            "    )\n",
            "    (softmax): LogSoftmax(dim=2)\n",
            "  )\n",
            ")\n",
            "============================================================================================\n",
            "                                     Kernel Shape      Output Shape  \\\n",
            "Layer                                                                 \n",
            "0_encoder.embedding.PermuteBlock_0              -   [128, 27, 1701]   \n",
            "1_encoder.embedding.Conv1d_1         [27, 128, 3]  [128, 128, 1701]   \n",
            "2_encoder.embedding.BatchNorm1d_2           [128]  [128, 128, 1701]   \n",
            "3_encoder.embedding.GELU_3                      -  [128, 128, 1701]   \n",
            "4_encoder.embedding.Conv1d_4        [128, 256, 3]  [128, 256, 1701]   \n",
            "5_encoder.embedding.BatchNorm1d_5           [256]  [128, 256, 1701]   \n",
            "6_encoder.embedding.PermuteBlock_6              -  [128, 1701, 256]   \n",
            "7_encoder.pBLSTMs.0.LSTM_blstm                  -     [80113, 1024]   \n",
            "8_encoder.pBLSTMs.LockedDropout_1               -     [80113, 1024]   \n",
            "9_encoder.pBLSTMs.2.LSTM_blstm                  -     [40026, 1024]   \n",
            "10_encoder.pBLSTMs.LockedDropout_3              -     [40026, 1024]   \n",
            "11_decoder.mlp.PermuteBlock_0                   -  [128, 1024, 425]   \n",
            "12_decoder.mlp.BatchNorm1d_1               [1024]  [128, 1024, 425]   \n",
            "13_decoder.mlp.PermuteBlock_2                   -  [128, 425, 1024]   \n",
            "14_decoder.mlp.Linear_3              [1024, 2048]  [128, 425, 2048]   \n",
            "15_decoder.mlp.GELU_4                           -  [128, 425, 2048]   \n",
            "16_decoder.mlp.PermuteBlock_5                   -  [128, 2048, 425]   \n",
            "17_decoder.mlp.BatchNorm1d_6               [2048]  [128, 2048, 425]   \n",
            "18_decoder.mlp.PermuteBlock_7                   -  [128, 425, 2048]   \n",
            "19_decoder.mlp.Dropout_8                        -  [128, 425, 2048]   \n",
            "20_decoder.mlp.Linear_9              [2048, 1024]  [128, 425, 1024]   \n",
            "21_decoder.mlp.GELU_10                          -  [128, 425, 1024]   \n",
            "22_decoder.mlp.PermuteBlock_11                  -  [128, 1024, 425]   \n",
            "23_decoder.mlp.BatchNorm1d_12              [1024]  [128, 1024, 425]   \n",
            "24_decoder.mlp.PermuteBlock_13                  -  [128, 425, 1024]   \n",
            "25_decoder.mlp.Dropout_14                       -  [128, 425, 1024]   \n",
            "26_decoder.mlp.Linear_15               [1024, 41]    [128, 425, 41]   \n",
            "27_decoder.LogSoftmax_softmax                   -    [128, 425, 41]   \n",
            "\n",
            "                                        Params    Mult-Adds  \n",
            "Layer                                                        \n",
            "0_encoder.embedding.PermuteBlock_0           -            -  \n",
            "1_encoder.embedding.Conv1d_1           10.496k   17.635968M  \n",
            "2_encoder.embedding.BatchNorm1d_2        256.0        128.0  \n",
            "3_encoder.embedding.GELU_3                   -            -  \n",
            "4_encoder.embedding.Conv1d_4            98.56k  167.215104M  \n",
            "5_encoder.embedding.BatchNorm1d_5        512.0        256.0  \n",
            "6_encoder.embedding.PermuteBlock_6           -            -  \n",
            "7_encoder.pBLSTMs.0.LSTM_blstm       4.202496M    4.194304M  \n",
            "8_encoder.pBLSTMs.LockedDropout_1            -            -  \n",
            "9_encoder.pBLSTMs.2.LSTM_blstm      10.493952M    10.48576M  \n",
            "10_encoder.pBLSTMs.LockedDropout_3           -            -  \n",
            "11_decoder.mlp.PermuteBlock_0                -            -  \n",
            "12_decoder.mlp.BatchNorm1d_1            2.048k       1.024k  \n",
            "13_decoder.mlp.PermuteBlock_2                -            -  \n",
            "14_decoder.mlp.Linear_3                2.0992M    2.097152M  \n",
            "15_decoder.mlp.GELU_4                        -            -  \n",
            "16_decoder.mlp.PermuteBlock_5                -            -  \n",
            "17_decoder.mlp.BatchNorm1d_6            4.096k       2.048k  \n",
            "18_decoder.mlp.PermuteBlock_7                -            -  \n",
            "19_decoder.mlp.Dropout_8                     -            -  \n",
            "20_decoder.mlp.Linear_9              2.098176M    2.097152M  \n",
            "21_decoder.mlp.GELU_10                       -            -  \n",
            "22_decoder.mlp.PermuteBlock_11               -            -  \n",
            "23_decoder.mlp.BatchNorm1d_12           2.048k       1.024k  \n",
            "24_decoder.mlp.PermuteBlock_13               -            -  \n",
            "25_decoder.mlp.Dropout_14                    -            -  \n",
            "26_decoder.mlp.Linear_15               42.025k      41.984k  \n",
            "27_decoder.LogSoftmax_softmax                -            -  \n",
            "--------------------------------------------------------------------------------------------\n",
            "                           Totals\n",
            "Total params           19.053865M\n",
            "Trainable params       19.053865M\n",
            "Non-trainable params          0.0\n",
            "Mult-Adds             203.771904M\n",
            "============================================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     Kernel Shape      Output Shape  \\\n",
              "Layer                                                                 \n",
              "0_encoder.embedding.PermuteBlock_0              -   [128, 27, 1701]   \n",
              "1_encoder.embedding.Conv1d_1         [27, 128, 3]  [128, 128, 1701]   \n",
              "2_encoder.embedding.BatchNorm1d_2           [128]  [128, 128, 1701]   \n",
              "3_encoder.embedding.GELU_3                      -  [128, 128, 1701]   \n",
              "4_encoder.embedding.Conv1d_4        [128, 256, 3]  [128, 256, 1701]   \n",
              "5_encoder.embedding.BatchNorm1d_5           [256]  [128, 256, 1701]   \n",
              "6_encoder.embedding.PermuteBlock_6              -  [128, 1701, 256]   \n",
              "7_encoder.pBLSTMs.0.LSTM_blstm                  -     [80113, 1024]   \n",
              "8_encoder.pBLSTMs.LockedDropout_1               -     [80113, 1024]   \n",
              "9_encoder.pBLSTMs.2.LSTM_blstm                  -     [40026, 1024]   \n",
              "10_encoder.pBLSTMs.LockedDropout_3              -     [40026, 1024]   \n",
              "11_decoder.mlp.PermuteBlock_0                   -  [128, 1024, 425]   \n",
              "12_decoder.mlp.BatchNorm1d_1               [1024]  [128, 1024, 425]   \n",
              "13_decoder.mlp.PermuteBlock_2                   -  [128, 425, 1024]   \n",
              "14_decoder.mlp.Linear_3              [1024, 2048]  [128, 425, 2048]   \n",
              "15_decoder.mlp.GELU_4                           -  [128, 425, 2048]   \n",
              "16_decoder.mlp.PermuteBlock_5                   -  [128, 2048, 425]   \n",
              "17_decoder.mlp.BatchNorm1d_6               [2048]  [128, 2048, 425]   \n",
              "18_decoder.mlp.PermuteBlock_7                   -  [128, 425, 2048]   \n",
              "19_decoder.mlp.Dropout_8                        -  [128, 425, 2048]   \n",
              "20_decoder.mlp.Linear_9              [2048, 1024]  [128, 425, 1024]   \n",
              "21_decoder.mlp.GELU_10                          -  [128, 425, 1024]   \n",
              "22_decoder.mlp.PermuteBlock_11                  -  [128, 1024, 425]   \n",
              "23_decoder.mlp.BatchNorm1d_12              [1024]  [128, 1024, 425]   \n",
              "24_decoder.mlp.PermuteBlock_13                  -  [128, 425, 1024]   \n",
              "25_decoder.mlp.Dropout_14                       -  [128, 425, 1024]   \n",
              "26_decoder.mlp.Linear_15               [1024, 41]    [128, 425, 41]   \n",
              "27_decoder.LogSoftmax_softmax                   -    [128, 425, 41]   \n",
              "\n",
              "                                        Params    Mult-Adds  \n",
              "Layer                                                        \n",
              "0_encoder.embedding.PermuteBlock_0         NaN          NaN  \n",
              "1_encoder.embedding.Conv1d_1           10496.0   17635968.0  \n",
              "2_encoder.embedding.BatchNorm1d_2        256.0        128.0  \n",
              "3_encoder.embedding.GELU_3                 NaN          NaN  \n",
              "4_encoder.embedding.Conv1d_4           98560.0  167215104.0  \n",
              "5_encoder.embedding.BatchNorm1d_5        512.0        256.0  \n",
              "6_encoder.embedding.PermuteBlock_6         NaN          NaN  \n",
              "7_encoder.pBLSTMs.0.LSTM_blstm       4202496.0    4194304.0  \n",
              "8_encoder.pBLSTMs.LockedDropout_1          NaN          NaN  \n",
              "9_encoder.pBLSTMs.2.LSTM_blstm      10493952.0   10485760.0  \n",
              "10_encoder.pBLSTMs.LockedDropout_3         NaN          NaN  \n",
              "11_decoder.mlp.PermuteBlock_0              NaN          NaN  \n",
              "12_decoder.mlp.BatchNorm1d_1            2048.0       1024.0  \n",
              "13_decoder.mlp.PermuteBlock_2              NaN          NaN  \n",
              "14_decoder.mlp.Linear_3              2099200.0    2097152.0  \n",
              "15_decoder.mlp.GELU_4                      NaN          NaN  \n",
              "16_decoder.mlp.PermuteBlock_5              NaN          NaN  \n",
              "17_decoder.mlp.BatchNorm1d_6            4096.0       2048.0  \n",
              "18_decoder.mlp.PermuteBlock_7              NaN          NaN  \n",
              "19_decoder.mlp.Dropout_8                   NaN          NaN  \n",
              "20_decoder.mlp.Linear_9              2098176.0    2097152.0  \n",
              "21_decoder.mlp.GELU_10                     NaN          NaN  \n",
              "22_decoder.mlp.PermuteBlock_11             NaN          NaN  \n",
              "23_decoder.mlp.BatchNorm1d_12           2048.0       1024.0  \n",
              "24_decoder.mlp.PermuteBlock_13             NaN          NaN  \n",
              "25_decoder.mlp.Dropout_14                  NaN          NaN  \n",
              "26_decoder.mlp.Linear_15               42025.0      41984.0  \n",
              "27_decoder.LogSoftmax_softmax              NaN          NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4b3bbd2f-e4c8-434f-b937-aefe86cf74ed\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_encoder.embedding.PermuteBlock_0</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 27, 1701]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_encoder.embedding.Conv1d_1</th>\n",
              "      <td>[27, 128, 3]</td>\n",
              "      <td>[128, 128, 1701]</td>\n",
              "      <td>10496.0</td>\n",
              "      <td>17635968.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_encoder.embedding.BatchNorm1d_2</th>\n",
              "      <td>[128]</td>\n",
              "      <td>[128, 128, 1701]</td>\n",
              "      <td>256.0</td>\n",
              "      <td>128.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_encoder.embedding.GELU_3</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 128, 1701]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_encoder.embedding.Conv1d_4</th>\n",
              "      <td>[128, 256, 3]</td>\n",
              "      <td>[128, 256, 1701]</td>\n",
              "      <td>98560.0</td>\n",
              "      <td>167215104.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_encoder.embedding.BatchNorm1d_5</th>\n",
              "      <td>[256]</td>\n",
              "      <td>[128, 256, 1701]</td>\n",
              "      <td>512.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6_encoder.embedding.PermuteBlock_6</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 1701, 256]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7_encoder.pBLSTMs.0.LSTM_blstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[80113, 1024]</td>\n",
              "      <td>4202496.0</td>\n",
              "      <td>4194304.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8_encoder.pBLSTMs.LockedDropout_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[80113, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_encoder.pBLSTMs.2.LSTM_blstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[40026, 1024]</td>\n",
              "      <td>10493952.0</td>\n",
              "      <td>10485760.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10_encoder.pBLSTMs.LockedDropout_3</th>\n",
              "      <td>-</td>\n",
              "      <td>[40026, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11_decoder.mlp.PermuteBlock_0</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 1024, 425]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12_decoder.mlp.BatchNorm1d_1</th>\n",
              "      <td>[1024]</td>\n",
              "      <td>[128, 1024, 425]</td>\n",
              "      <td>2048.0</td>\n",
              "      <td>1024.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13_decoder.mlp.PermuteBlock_2</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 425, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14_decoder.mlp.Linear_3</th>\n",
              "      <td>[1024, 2048]</td>\n",
              "      <td>[128, 425, 2048]</td>\n",
              "      <td>2099200.0</td>\n",
              "      <td>2097152.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15_decoder.mlp.GELU_4</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 425, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16_decoder.mlp.PermuteBlock_5</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 2048, 425]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17_decoder.mlp.BatchNorm1d_6</th>\n",
              "      <td>[2048]</td>\n",
              "      <td>[128, 2048, 425]</td>\n",
              "      <td>4096.0</td>\n",
              "      <td>2048.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18_decoder.mlp.PermuteBlock_7</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 425, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19_decoder.mlp.Dropout_8</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 425, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_decoder.mlp.Linear_9</th>\n",
              "      <td>[2048, 1024]</td>\n",
              "      <td>[128, 425, 1024]</td>\n",
              "      <td>2098176.0</td>\n",
              "      <td>2097152.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21_decoder.mlp.GELU_10</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 425, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22_decoder.mlp.PermuteBlock_11</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 1024, 425]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23_decoder.mlp.BatchNorm1d_12</th>\n",
              "      <td>[1024]</td>\n",
              "      <td>[128, 1024, 425]</td>\n",
              "      <td>2048.0</td>\n",
              "      <td>1024.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24_decoder.mlp.PermuteBlock_13</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 425, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25_decoder.mlp.Dropout_14</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 425, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26_decoder.mlp.Linear_15</th>\n",
              "      <td>[1024, 41]</td>\n",
              "      <td>[128, 425, 41]</td>\n",
              "      <td>42025.0</td>\n",
              "      <td>41984.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27_decoder.LogSoftmax_softmax</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 425, 41]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4b3bbd2f-e4c8-434f-b937-aefe86cf74ed')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4b3bbd2f-e4c8-434f-b937-aefe86cf74ed button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4b3bbd2f-e4c8-434f-b937-aefe86cf74ed');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "model = ASRModel(\n",
        "    input_size  = 27,\n",
        "    embed_size  = 512, #TODO\n",
        "    output_size = len(PHONEMES)\n",
        ").to(device)\n",
        "print(model)\n",
        "torchsummaryX.summary(model, x.to(device), lx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN82c3KpLup8"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"beam_width\" : 5,\n",
        "    \"lr\" : 2e-3,\n",
        "    \"epochs\" : 50\n",
        "    } # Feel free to add more items here"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ctcdecode import CTCBeamDecoder"
      ],
      "metadata": {
        "id": "M1bI1sXCUXn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGoozH2nd6KB"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "\n",
        "\n",
        "criterion = torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=False) # Define CTC loss as the criterion. How would the losses be reduced?\n",
        "# CTC Loss: https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n",
        "# Refer to the handout for hints\n",
        "\n",
        "optimizer =  torch.optim.AdamW(model.parameters(), lr= config['lr']) # What goes in here?\n",
        "\n",
        "# Declare the decoder. Use the CTC Beam Decoder to decode phonemes\n",
        "# CTC Beam Decoder Doc: https://github.com/parlance/ctcdecode\n",
        "\n",
        "decoder = CTCBeamDecoder(LABELS, beam_width = config[\"beam_width\"], log_probs_input = True)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', patience = 3, threshold=1e-2)\n",
        "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 50, eta_min = 1e-6)\n",
        "# Mixed Precision, if you need it\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmc6_4eWL2Xp"
      },
      "source": [
        "## Decode Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHjnCDddL36E"
      },
      "outputs": [],
      "source": [
        "def decode_prediction(output, output_lens, decoder, PHONEME_MAP= LABELS):\n",
        "    \n",
        "    # TODO: look at docs for CTC.decoder and find out what is returned here. Check the shape of output and expected shape in decode.\n",
        "    beam_results, beam_scores, timesteps, out_seq_len = decoder.decode(output, seq_lens= output_lens) #lengths - list of lengths\n",
        "\n",
        "    pred_strings  = []\n",
        "    # print(beam_results.shape)\n",
        "    # print(beam_results)\n",
        "    for i in range(output_lens.shape[0]):\n",
        "        #TODO: Create the prediction from the output of decoder.decode. Don't forget to map it using PHONEMES_MAP.\n",
        "        pred_strings.append(''.join([PHONEME_MAP[n] for n in beam_results[i][0][:out_seq_len[i][0]]]))\n",
        "    # print(pred_strings)\n",
        "    \n",
        "    return pred_strings\n",
        "\n",
        "def calculate_levenshtein(output, label, output_lens, label_lens, decoder, PHONEME_MAP= LABELS): # y - sequence of integers\n",
        "    \n",
        "    dist            = 0\n",
        "    batch_size      = label.shape[0]\n",
        "\n",
        "    pred_strings    = decode_prediction(output, output_lens, decoder, PHONEME_MAP)\n",
        "    # print(batch_size)\n",
        "    for i in range(batch_size):\n",
        "        # TODO: Get predicted string and label string for each element in the batch\n",
        "        pred_string = pred_strings[i]#TODO\n",
        "        # print('pred',pred_string)\n",
        "        label_string = ''.join([PHONEME_MAP[n] for n in label[i][:label_lens[i]]]) #TODO\n",
        "        # print('label',label_string)\n",
        "        dist += Levenshtein.distance(pred_string, label_string)\n",
        "\n",
        "    dist /= batch_size # TODO: Uncomment this, but think about why we are doing this\n",
        "    \n",
        "    return dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnTLL-5gMBrY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dcaada3-0f0d-47d4-b642-2c9f351ce4ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 734, 41])\n",
            "197.7578125\n",
            "torch.Size([734, 128, 41]) torch.Size([128, 265])\n",
            "tensor(7.1806, device='cuda:0', grad_fn=<MeanBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# test code to check shapes\n",
        "\n",
        "model.eval()\n",
        "for i, data in enumerate(val_loader, 0):\n",
        "    x, y, lx, ly = data\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    h, lh = model(x, lx)\n",
        "    print(h.shape)\n",
        "    print(calculate_levenshtein(h, y, lx, ly, decoder, LABELS))\n",
        "\n",
        "    h = torch.permute(h, (1, 0, 2))\n",
        "    print(h.shape, y.shape)\n",
        "    loss = criterion(h, y, lh, ly)\n",
        "    print(loss)\n",
        "\n",
        "    # print(calculate_levenshtein(h, y, lx, ly, decoder, LABELS))\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd5aNaLVoR_g"
      },
      "source": [
        "## wandb\n",
        "\n",
        "You will need to fetch your api key from wandb.ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiDduMaDIARE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc424b3-8913-45c7-e2a1-6e6843af1bcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msilviagu\u001b[0m (\u001b[33m11785-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login(key=\"eac70e317e1b0b232f1e96dda29ff11405e8b4bc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4s52yBOvICPZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "634ac4bd-0ef9-4f5a-b550-93b6daee7f98"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "wandb version 0.14.1 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230406_052020-tx2wqt8c</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/11785-team/hw3p2-ablations/runs/tx2wqt8c' target=\"_blank\">silvia-Pblstm-cnnnorm-360</a></strong> to <a href='https://wandb.ai/11785-team/hw3p2-ablations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/11785-team/hw3p2-ablations' target=\"_blank\">https://wandb.ai/11785-team/hw3p2-ablations</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/11785-team/hw3p2-ablations/runs/tx2wqt8c' target=\"_blank\">https://wandb.ai/11785-team/hw3p2-ablations/runs/tx2wqt8c</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run = wandb.init(\n",
        "    name = \"silvia-Pblstm-cnnnorm-360\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"hw3p2-ablations\", ### Project should be created in your wandb account \n",
        "    config = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fLLj5KIMMOe"
      },
      "source": [
        "# Train Functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jwWle6_nS8y",
        "outputId": "e00266e7-5cbb-41bb-bd44-dddbdfffa8b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1441"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri87MAdhMUz5"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer):\n",
        "    \n",
        "    model.train()\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast():     \n",
        "            h, lh = model(x, lx)\n",
        "            h = torch.permute(h, (1, 0, 2))\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "\n",
        "        # Another couple things you need for FP16. \n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update() # This is something added just for FP16\n",
        "\n",
        "        del x, y, lx, ly, h, lh, loss \n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "    \n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "def validate_model(model, val_loader, decoder, phoneme_map= LABELS):\n",
        "\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    total_loss = 0\n",
        "    vdist = 0\n",
        "\n",
        "    for i, data in enumerate(val_loader):\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            h, lh = model(x, lx)\n",
        "            h = torch.permute(h, (1, 0, 2))\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "\n",
        "        total_loss += float(loss)\n",
        "        vdist += calculate_levenshtein(torch.permute(h, (1, 0, 2)), y, lh, ly, decoder, phoneme_map)\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), dist=\"{:.04f}\".format(float(vdist / (i + 1))))\n",
        "\n",
        "        batch_bar.update()\n",
        "    \n",
        "        del x, y, lx, ly, h, lh, loss\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "    batch_bar.close()\n",
        "    total_loss = total_loss/len(val_loader)\n",
        "    val_dist = vdist/len(val_loader)\n",
        "    return total_loss, val_dist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test code to check shapes\n",
        "\n",
        "model.eval()\n",
        "for i, data in enumerate(val_loader, 0):\n",
        "    x, y, lx, ly = data\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    h, lh = model(x, lx)\n",
        "    print(h.shape)\n",
        "    print(calculate_levenshtein(h, y, lx, ly, decoder, LABELS))\n",
        "    h = torch.permute(h, (1, 0, 2))\n",
        "    print(h.shape, y.shape)\n",
        "    loss = criterion(h, y, lh, ly)\n",
        "    print(loss)\n",
        "\n",
        "    \n",
        "\n",
        "    break"
      ],
      "metadata": {
        "id": "D7J9pyt45lYP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18bd099c-516c-4be8-c80d-c1d1d50e0536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 734, 41])\n",
            "197.7578125\n",
            "torch.Size([734, 128, 41]) torch.Size([128, 265])\n",
            "tensor(7.1806, device='cuda:0', grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGrEFE38MUz5"
      },
      "outputs": [],
      "source": [
        "# test code to check shapes - simple\n",
        "\n",
        "# model.eval()\n",
        "# for i, data in enumerate(val_loader, 0):\n",
        "#     x, y, lx, ly = data\n",
        "#     x, y = x.to(device), y.to(device)\n",
        "#     h, lh = model(x, lx)\n",
        "#     print(h.shape)\n",
        "#     print(calculate_levenshtein(h, y, lx, ly, decoder, LABELS))\n",
        "#     h = torch.permute(h, (1, 0, 2))\n",
        "#     print(h.shape, y.shape)\n",
        "#     loss = criterion(h, y, lh, ly)\n",
        "#     print(loss)\n",
        "\n",
        "    \n",
        "\n",
        "#     break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpYExu4vT4_g"
      },
      "source": [
        "### Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "husa5_EYMUz6"
      },
      "outputs": [],
      "source": [
        "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
        "    torch.save(\n",
        "        {'model_state_dict'         : model.state_dict(),\n",
        "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
        "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
        "         metric[0]                  : metric[1], \n",
        "         'epoch'                    : epoch}, \n",
        "         path\n",
        "    )\n",
        "\n",
        "def load_model(path, model, metric= 'valid_dist', optimizer= None, scheduler= None):\n",
        "\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if optimizer != None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if scheduler != None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        \n",
        "    epoch   = checkpoint['epoch']\n",
        "    metric  = checkpoint[metric]\n",
        "\n",
        "    return [model, optimizer, scheduler, epoch, metric]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir /content/model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iq2cwgvOnbuY",
        "outputId": "7fd95f44-a00d-41e0-9e4b-9ef399f4c8f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/model’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tExvyl1BIdMC"
      },
      "outputs": [],
      "source": [
        "# This is for checkpointing, if you're doing it over multiple sessions\n",
        "\n",
        "last_epoch_completed = 0\n",
        "start = last_epoch_completed\n",
        "end = config[\"epochs\"]\n",
        "best_lev_dist = float(\"inf\") # if you're restarting from some checkpoint, use what you saw there.\n",
        "epoch_model_path = '/content/model/drop-cnnnorm-360-epoch_model.pt' \n",
        "#TODO set the model path( Optional, you can just store best one. Make sure to make the changes below)\n",
        "best_model_path = '/content/model/drop-cnnnorm-360-best_model.pt'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer.param_groups[0]['lr'] = 2e-5"
      ],
      "metadata": {
        "id": "3CBkoDvwzO4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JR43E28rM9Ak",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8585df59-df43-4e82-a2d8-cf00d5ef6bb5"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.2356\t Learning Rate 0.0002000\n",
            "\tVal Dist 3.4808%\t Val Loss 0.1741\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved epoch model\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved best model\n",
            "\n",
            "Epoch: 2/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.2307\t Learning Rate 0.0002000\n",
            "\tVal Dist 3.4350%\t Val Loss 0.1722\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved epoch model\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved best model\n",
            "\n",
            "Epoch: 3/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.2291\t Learning Rate 0.0002000\n",
            "\tVal Dist 3.4350%\t Val Loss 0.1726\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved epoch model\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved best model\n",
            "\n",
            "Epoch: 4/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.2338\t Learning Rate 0.0002000\n",
            "\tVal Dist 3.3886%\t Val Loss 0.1709\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved epoch model\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved best model\n",
            "\n",
            "Epoch: 5/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.2294\t Learning Rate 0.0002000\n",
            "\tVal Dist 3.3647%\t Val Loss 0.1699\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved epoch model\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved best model\n",
            "\n",
            "Epoch: 6/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.2273\t Learning Rate 0.0002000\n",
            "\tVal Dist 3.3500%\t Val Loss 0.1707\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved epoch model\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved best model\n",
            "\n",
            "Epoch: 7/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss 0.2239\t Learning Rate 0.0002000\n",
            "\tVal Dist 3.3485%\t Val Loss 0.1706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved epoch model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved best model\n",
            "\n",
            "Epoch: 8/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss 0.2219\t Learning Rate 0.0002000\n",
            "\tVal Dist 3.3298%\t Val Loss 0.1704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved epoch model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved best model\n",
            "\n",
            "Epoch: 9/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss 0.2252\t Learning Rate 0.0002000\n",
            "\tVal Dist 3.3472%\t Val Loss 0.1700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved epoch model\n",
            "\n",
            "Epoch: 10/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:   3%|▎         | 23/813 [01:07<39:50,  3.03s/it, loss=0.2268, lr=0.000200]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-1678aee17802>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m              \u001b[0;34m=\u001b[0m  \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dist\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphoneme_map\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mLABELS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-b404a22a5f25>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Another couple things you need for FP16.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This is a replacement for loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This is a replacement for optimizer.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This is something added just for FP16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "#TODO: Please complete the training loop\n",
        "\n",
        "for epoch in range(0, config['epochs']):\n",
        "\n",
        "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n",
        "    \n",
        "    curr_lr = float(optimizer.param_groups[0]['lr']) #TODO\n",
        "    \n",
        "\n",
        "    train_loss              =  train_model(model, train_loader, criterion, optimizer) #TODO\n",
        "   \n",
        "    valid_loss, valid_dist  = validate_model(model, val_loader, decoder, phoneme_map= LABELS) #TODO\n",
        "    scheduler.step(valid_dist)\n",
        "\n",
        "    print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n",
        "    print(\"\\tVal Dist {:.04f}%\\t Val Loss {:.04f}\".format(valid_dist, valid_loss))\n",
        "\n",
        "\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss,  \n",
        "        'valid_dist': valid_dist, \n",
        "        'valid_loss': valid_loss, \n",
        "        'lr'        : curr_lr\n",
        "    })\n",
        "    \n",
        "    save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, epoch_model_path)\n",
        "    wandb.save(epoch_model_path)\n",
        "    print(\"Saved epoch model\")\n",
        "\n",
        "    if valid_dist <= best_lev_dist:\n",
        "        best_lev_dist = valid_dist\n",
        "        save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, best_model_path)\n",
        "        wandb.save(best_model_path)\n",
        "        print(\"Saved best model\")\n",
        "      # You may find it interesting to exlplore Wandb Artifcats to version your models\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  valid_loss, valid_dist  = validate_model(model, val_loader, decoder, phoneme_map= LABELS)"
      ],
      "metadata": {
        "id": "FVFtY6-ZMu7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2H4EEj-sD32"
      },
      "source": [
        "# Generate Predictions and Submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best = load_model(best_model_path,model)"
      ],
      "metadata": {
        "id": "_6-2xpv5osGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = best[0]"
      ],
      "metadata": {
        "id": "oSiFcYluqeee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = best_model"
      ],
      "metadata": {
        "id": "XKQUsCbyqxQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2moYJhTWsOG-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15271652-701c-4c88-aace-7b32a0df2c94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/2620 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 1/2620 [00:01<54:10,  1.24s/it]\u001b[A\n",
            "  0%|          | 3/2620 [00:01<15:47,  2.76it/s]\u001b[A\n",
            "  0%|          | 6/2620 [00:01<07:41,  5.67it/s]\u001b[A\n",
            "  0%|          | 8/2620 [00:01<06:01,  7.22it/s]\u001b[A\n",
            "  0%|          | 10/2620 [00:01<05:12,  8.35it/s]\u001b[A\n",
            "  0%|          | 12/2620 [00:02<04:42,  9.24it/s]\u001b[A\n",
            "  1%|          | 14/2620 [00:02<04:31,  9.58it/s]\u001b[A\n",
            "  1%|          | 17/2620 [00:02<03:25, 12.68it/s]\u001b[A\n",
            "  1%|          | 19/2620 [00:02<03:54, 11.10it/s]\u001b[A\n",
            "  1%|          | 21/2620 [00:02<04:34,  9.48it/s]\u001b[A\n",
            "  1%|          | 23/2620 [00:03<04:26,  9.75it/s]\u001b[A\n",
            "  1%|          | 25/2620 [00:03<04:41,  9.21it/s]\u001b[A\n",
            "  1%|          | 27/2620 [00:03<04:00, 10.78it/s]\u001b[A\n",
            "  1%|          | 29/2620 [00:03<03:31, 12.27it/s]\u001b[A\n",
            "  1%|          | 32/2620 [00:03<02:56, 14.68it/s]\u001b[A\n",
            "  1%|▏         | 35/2620 [00:03<02:33, 16.80it/s]\u001b[A\n",
            "  1%|▏         | 38/2620 [00:03<02:17, 18.79it/s]\u001b[A\n",
            "  2%|▏         | 41/2620 [00:04<02:24, 17.90it/s]\u001b[A\n",
            "  2%|▏         | 44/2620 [00:04<02:13, 19.24it/s]\u001b[A\n",
            "  2%|▏         | 47/2620 [00:04<02:34, 16.61it/s]\u001b[A\n",
            "  2%|▏         | 49/2620 [00:04<03:04, 13.96it/s]\u001b[A\n",
            "  2%|▏         | 51/2620 [00:05<04:01, 10.66it/s]\u001b[A\n",
            "  2%|▏         | 53/2620 [00:05<04:06, 10.41it/s]\u001b[A\n",
            "  2%|▏         | 55/2620 [00:05<03:41, 11.56it/s]\u001b[A\n",
            "  2%|▏         | 57/2620 [00:05<03:31, 12.11it/s]\u001b[A\n",
            "  2%|▏         | 60/2620 [00:05<03:15, 13.10it/s]\u001b[A\n",
            "  2%|▏         | 62/2620 [00:05<03:08, 13.60it/s]\u001b[A\n",
            "  2%|▏         | 64/2620 [00:05<02:53, 14.70it/s]\u001b[A\n",
            "  3%|▎         | 66/2620 [00:06<03:10, 13.44it/s]\u001b[A\n",
            "  3%|▎         | 68/2620 [00:06<03:58, 10.68it/s]\u001b[A\n",
            "  3%|▎         | 70/2620 [00:06<03:55, 10.83it/s]\u001b[A\n",
            "  3%|▎         | 72/2620 [00:06<03:46, 11.25it/s]\u001b[A\n",
            "  3%|▎         | 74/2620 [00:07<05:07,  8.27it/s]\u001b[A\n",
            "  3%|▎         | 76/2620 [00:07<04:49,  8.77it/s]\u001b[A\n",
            "  3%|▎         | 78/2620 [00:07<04:27,  9.52it/s]\u001b[A\n",
            "  3%|▎         | 80/2620 [00:07<04:21,  9.72it/s]\u001b[A\n",
            "  3%|▎         | 82/2620 [00:07<04:18,  9.80it/s]\u001b[A\n",
            "  3%|▎         | 84/2620 [00:08<04:29,  9.40it/s]\u001b[A\n",
            "  3%|▎         | 86/2620 [00:08<04:33,  9.25it/s]\u001b[A\n",
            "  3%|▎         | 88/2620 [00:08<05:04,  8.31it/s]\u001b[A\n",
            "  3%|▎         | 89/2620 [00:08<05:13,  8.07it/s]\u001b[A\n",
            "  3%|▎         | 91/2620 [00:09<05:12,  8.08it/s]\u001b[A\n",
            "  4%|▎         | 92/2620 [00:09<05:03,  8.32it/s]\u001b[A\n",
            "  4%|▎         | 93/2620 [00:09<05:32,  7.60it/s]\u001b[A\n",
            "  4%|▎         | 96/2620 [00:09<03:33, 11.84it/s]\u001b[A\n",
            "  4%|▎         | 98/2620 [00:09<03:29, 12.05it/s]\u001b[A\n",
            "  4%|▍         | 100/2620 [00:09<03:51, 10.90it/s]\u001b[A\n",
            "  4%|▍         | 102/2620 [00:09<03:58, 10.54it/s]\u001b[A\n",
            "  4%|▍         | 104/2620 [00:10<03:29, 12.01it/s]\u001b[A\n",
            "  4%|▍         | 106/2620 [00:10<03:13, 13.00it/s]\u001b[A\n",
            "  4%|▍         | 109/2620 [00:10<03:18, 12.62it/s]\u001b[A\n",
            "  4%|▍         | 111/2620 [00:10<03:10, 13.18it/s]\u001b[A\n",
            "  4%|▍         | 113/2620 [00:10<02:53, 14.42it/s]\u001b[A\n",
            "  4%|▍         | 116/2620 [00:10<02:22, 17.52it/s]\u001b[A\n",
            "  5%|▍         | 118/2620 [00:10<02:20, 17.76it/s]\u001b[A\n",
            "  5%|▍         | 120/2620 [00:11<02:35, 16.06it/s]\u001b[A\n",
            "  5%|▍         | 123/2620 [00:11<02:09, 19.22it/s]\u001b[A\n",
            "  5%|▍         | 126/2620 [00:11<02:25, 17.09it/s]\u001b[A\n",
            "  5%|▍         | 128/2620 [00:11<03:47, 10.97it/s]\u001b[A\n",
            "  5%|▍         | 130/2620 [00:12<04:27,  9.30it/s]\u001b[A\n",
            "  5%|▌         | 132/2620 [00:12<06:04,  6.83it/s]\u001b[A\n",
            "  5%|▌         | 133/2620 [00:12<05:46,  7.18it/s]\u001b[A\n",
            "  5%|▌         | 135/2620 [00:12<05:05,  8.12it/s]\u001b[A\n",
            "  5%|▌         | 137/2620 [00:12<04:16,  9.68it/s]\u001b[A\n",
            "  5%|▌         | 140/2620 [00:13<03:23, 12.20it/s]\u001b[A\n",
            "  5%|▌         | 142/2620 [00:13<03:02, 13.60it/s]\u001b[A\n",
            "  6%|▌         | 146/2620 [00:13<02:18, 17.80it/s]\u001b[A\n",
            "  6%|▌         | 148/2620 [00:13<02:16, 18.08it/s]\u001b[A\n",
            "  6%|▌         | 152/2620 [00:13<01:47, 22.88it/s]\u001b[A\n",
            "  6%|▌         | 155/2620 [00:13<02:02, 20.08it/s]\u001b[A\n",
            "  6%|▌         | 158/2620 [00:13<02:08, 19.18it/s]\u001b[A\n",
            "  6%|▌         | 161/2620 [00:14<02:55, 14.03it/s]\u001b[A\n",
            "  6%|▌         | 163/2620 [00:14<03:08, 13.06it/s]\u001b[A\n",
            "  6%|▋         | 166/2620 [00:14<02:56, 13.88it/s]\u001b[A\n",
            "  6%|▋         | 169/2620 [00:14<02:33, 15.93it/s]\u001b[A\n",
            "  7%|▋         | 171/2620 [00:14<02:45, 14.76it/s]\u001b[A\n",
            "  7%|▋         | 173/2620 [00:15<03:26, 11.83it/s]\u001b[A\n",
            "  7%|▋         | 175/2620 [00:15<03:29, 11.69it/s]\u001b[A\n",
            "  7%|▋         | 177/2620 [00:15<03:45, 10.84it/s]\u001b[A\n",
            "  7%|▋         | 179/2620 [00:15<03:45, 10.81it/s]\u001b[A\n",
            "  7%|▋         | 181/2620 [00:16<03:49, 10.64it/s]\u001b[A\n",
            "  7%|▋         | 183/2620 [00:16<04:38,  8.75it/s]\u001b[A\n",
            "  7%|▋         | 184/2620 [00:16<04:52,  8.32it/s]\u001b[A\n",
            "  7%|▋         | 187/2620 [00:16<03:27, 11.71it/s]\u001b[A\n",
            "  7%|▋         | 189/2620 [00:16<04:28,  9.05it/s]\u001b[A\n",
            "  7%|▋         | 191/2620 [00:17<05:01,  8.04it/s]\u001b[A\n",
            "  7%|▋         | 192/2620 [00:17<05:23,  7.51it/s]\u001b[A\n",
            "  7%|▋         | 194/2620 [00:17<05:14,  7.72it/s]\u001b[A\n",
            "  7%|▋         | 195/2620 [00:17<05:10,  7.81it/s]\u001b[A\n",
            "  8%|▊         | 197/2620 [00:17<04:27,  9.05it/s]\u001b[A\n",
            "  8%|▊         | 199/2620 [00:18<04:30,  8.97it/s]\u001b[A\n",
            "  8%|▊         | 200/2620 [00:18<04:37,  8.71it/s]\u001b[A\n",
            "  8%|▊         | 201/2620 [00:18<04:39,  8.67it/s]\u001b[A\n",
            "  8%|▊         | 204/2620 [00:18<03:56, 10.23it/s]\u001b[A\n",
            "  8%|▊         | 205/2620 [00:18<04:20,  9.25it/s]\u001b[A\n",
            "  8%|▊         | 206/2620 [00:18<04:19,  9.29it/s]\u001b[A\n",
            "  8%|▊         | 209/2620 [00:19<03:27, 11.62it/s]\u001b[A\n",
            "  8%|▊         | 211/2620 [00:19<04:06,  9.76it/s]\u001b[A\n",
            "  8%|▊         | 213/2620 [00:19<03:40, 10.92it/s]\u001b[A\n",
            "  8%|▊         | 215/2620 [00:19<03:25, 11.69it/s]\u001b[A\n",
            "  8%|▊         | 218/2620 [00:19<02:51, 13.99it/s]\u001b[A\n",
            "  8%|▊         | 220/2620 [00:19<02:46, 14.46it/s]\u001b[A\n",
            "  8%|▊         | 222/2620 [00:20<02:51, 14.01it/s]\u001b[A\n",
            "  9%|▊         | 224/2620 [00:20<02:44, 14.56it/s]\u001b[A\n",
            "  9%|▊         | 227/2620 [00:20<02:18, 17.22it/s]\u001b[A\n",
            "  9%|▉         | 230/2620 [00:20<02:19, 17.18it/s]\u001b[A\n",
            "  9%|▉         | 232/2620 [00:20<02:59, 13.30it/s]\u001b[A\n",
            "  9%|▉         | 234/2620 [00:20<02:59, 13.32it/s]\u001b[A\n",
            "  9%|▉         | 237/2620 [00:21<02:34, 15.42it/s]\u001b[A\n",
            "  9%|▉         | 239/2620 [00:21<02:43, 14.52it/s]\u001b[A\n",
            "  9%|▉         | 242/2620 [00:21<02:26, 16.22it/s]\u001b[A\n",
            "  9%|▉         | 244/2620 [00:21<02:49, 14.01it/s]\u001b[A\n",
            "  9%|▉         | 247/2620 [00:21<02:45, 14.30it/s]\u001b[A\n",
            " 10%|▉         | 250/2620 [00:21<02:29, 15.80it/s]\u001b[A\n",
            " 10%|▉         | 252/2620 [00:22<02:33, 15.38it/s]\u001b[A\n",
            " 10%|▉         | 254/2620 [00:22<02:26, 16.11it/s]\u001b[A\n",
            " 10%|▉         | 256/2620 [00:22<02:24, 16.32it/s]\u001b[A\n",
            " 10%|▉         | 258/2620 [00:22<02:34, 15.27it/s]\u001b[A\n",
            " 10%|▉         | 260/2620 [00:22<02:55, 13.48it/s]\u001b[A\n",
            " 10%|█         | 262/2620 [00:22<02:51, 13.71it/s]\u001b[A\n",
            " 10%|█         | 264/2620 [00:22<02:50, 13.83it/s]\u001b[A\n",
            " 10%|█         | 267/2620 [00:23<02:21, 16.64it/s]\u001b[A\n",
            " 10%|█         | 269/2620 [00:23<02:38, 14.84it/s]\u001b[A\n",
            " 10%|█         | 271/2620 [00:23<03:38, 10.76it/s]\u001b[A\n",
            " 10%|█         | 273/2620 [00:23<04:23,  8.89it/s]\u001b[A\n",
            " 10%|█         | 275/2620 [00:24<04:21,  8.96it/s]\u001b[A\n",
            " 11%|█         | 277/2620 [00:24<04:19,  9.03it/s]\u001b[A\n",
            " 11%|█         | 279/2620 [00:24<03:59,  9.78it/s]\u001b[A\n",
            " 11%|█         | 281/2620 [00:24<03:29, 11.17it/s]\u001b[A\n",
            " 11%|█         | 284/2620 [00:24<02:59, 13.00it/s]\u001b[A\n",
            " 11%|█         | 286/2620 [00:24<02:52, 13.52it/s]\u001b[A\n",
            " 11%|█         | 288/2620 [00:25<03:16, 11.89it/s]\u001b[A\n",
            " 11%|█         | 290/2620 [00:25<02:53, 13.43it/s]\u001b[A\n",
            " 11%|█         | 293/2620 [00:25<02:36, 14.83it/s]\u001b[A\n",
            " 11%|█▏        | 295/2620 [00:25<03:12, 12.06it/s]\u001b[A\n",
            " 11%|█▏        | 297/2620 [00:25<03:15, 11.86it/s]\u001b[A\n",
            " 11%|█▏        | 299/2620 [00:25<02:54, 13.34it/s]\u001b[A\n",
            " 11%|█▏        | 301/2620 [00:26<03:14, 11.90it/s]\u001b[A\n",
            " 12%|█▏        | 303/2620 [00:26<03:13, 11.95it/s]\u001b[A\n",
            " 12%|█▏        | 305/2620 [00:26<03:12, 12.03it/s]\u001b[A\n",
            " 12%|█▏        | 307/2620 [00:26<03:06, 12.40it/s]\u001b[A\n",
            " 12%|█▏        | 309/2620 [00:26<03:00, 12.79it/s]\u001b[A\n",
            " 12%|█▏        | 311/2620 [00:26<02:57, 13.04it/s]\u001b[A\n",
            " 12%|█▏        | 313/2620 [00:27<02:59, 12.86it/s]\u001b[A\n",
            " 12%|█▏        | 316/2620 [00:27<02:37, 14.62it/s]\u001b[A\n",
            " 12%|█▏        | 318/2620 [00:27<02:37, 14.65it/s]\u001b[A\n",
            " 12%|█▏        | 320/2620 [00:27<02:28, 15.52it/s]\u001b[A\n",
            " 12%|█▏        | 322/2620 [00:27<02:59, 12.81it/s]\u001b[A\n",
            " 12%|█▏        | 324/2620 [00:27<02:42, 14.14it/s]\u001b[A\n",
            " 12%|█▏        | 326/2620 [00:27<03:03, 12.47it/s]\u001b[A\n",
            " 13%|█▎        | 328/2620 [00:28<03:38, 10.48it/s]\u001b[A\n",
            " 13%|█▎        | 330/2620 [00:28<03:37, 10.51it/s]\u001b[A\n",
            " 13%|█▎        | 333/2620 [00:28<03:03, 12.45it/s]\u001b[A\n",
            " 13%|█▎        | 335/2620 [00:28<02:53, 13.17it/s]\u001b[A\n",
            " 13%|█▎        | 337/2620 [00:28<02:55, 13.02it/s]\u001b[A\n",
            " 13%|█▎        | 339/2620 [00:29<02:41, 14.15it/s]\u001b[A\n",
            " 13%|█▎        | 341/2620 [00:29<02:38, 14.37it/s]\u001b[A\n",
            " 13%|█▎        | 344/2620 [00:29<02:23, 15.87it/s]\u001b[A\n",
            " 13%|█▎        | 347/2620 [00:29<02:12, 17.18it/s]\u001b[A\n",
            " 13%|█▎        | 350/2620 [00:29<01:59, 19.03it/s]\u001b[A\n",
            " 13%|█▎        | 353/2620 [00:29<01:45, 21.49it/s]\u001b[A\n",
            " 14%|█▎        | 356/2620 [00:29<01:41, 22.41it/s]\u001b[A\n",
            " 14%|█▎        | 359/2620 [00:29<01:36, 23.47it/s]\u001b[A\n",
            " 14%|█▍        | 362/2620 [00:30<01:38, 22.88it/s]\u001b[A\n",
            " 14%|█▍        | 365/2620 [00:30<01:46, 21.16it/s]\u001b[A\n",
            " 14%|█▍        | 368/2620 [00:30<01:53, 19.82it/s]\u001b[A\n",
            " 14%|█▍        | 371/2620 [00:30<01:55, 19.43it/s]\u001b[A\n",
            " 14%|█▍        | 373/2620 [00:30<02:03, 18.23it/s]\u001b[A\n",
            " 14%|█▍        | 376/2620 [00:30<01:51, 20.15it/s]\u001b[A\n",
            " 14%|█▍        | 379/2620 [00:30<01:50, 20.34it/s]\u001b[A\n",
            " 15%|█▍        | 382/2620 [00:31<01:41, 22.00it/s]\u001b[A\n",
            " 15%|█▍        | 385/2620 [00:31<01:34, 23.70it/s]\u001b[A\n",
            " 15%|█▍        | 388/2620 [00:31<01:28, 25.26it/s]\u001b[A\n",
            " 15%|█▍        | 391/2620 [00:31<01:32, 23.99it/s]\u001b[A\n",
            " 15%|█▌        | 394/2620 [00:31<01:40, 22.21it/s]\u001b[A\n",
            " 15%|█▌        | 397/2620 [00:31<01:36, 22.97it/s]\u001b[A\n",
            " 15%|█▌        | 400/2620 [00:31<01:39, 22.30it/s]\u001b[A\n",
            " 15%|█▌        | 404/2620 [00:31<01:29, 24.81it/s]\u001b[A\n",
            " 16%|█▌        | 407/2620 [00:32<01:28, 25.05it/s]\u001b[A\n",
            " 16%|█▌        | 410/2620 [00:32<01:26, 25.54it/s]\u001b[A\n",
            " 16%|█▌        | 413/2620 [00:32<01:43, 21.41it/s]\u001b[A\n",
            " 16%|█▌        | 416/2620 [00:32<01:40, 21.83it/s]\u001b[A\n",
            " 16%|█▌        | 419/2620 [00:32<01:39, 22.13it/s]\u001b[A\n",
            " 16%|█▌        | 422/2620 [00:32<01:49, 20.11it/s]\u001b[A\n",
            " 16%|█▌        | 425/2620 [00:32<01:39, 22.09it/s]\u001b[A\n",
            " 16%|█▋        | 428/2620 [00:33<01:34, 23.24it/s]\u001b[A\n",
            " 16%|█▋        | 431/2620 [00:33<01:50, 19.74it/s]\u001b[A\n",
            " 17%|█▋        | 434/2620 [00:33<01:40, 21.76it/s]\u001b[A\n",
            " 17%|█▋        | 437/2620 [00:33<01:46, 20.41it/s]\u001b[A\n",
            " 17%|█▋        | 440/2620 [00:33<01:58, 18.40it/s]\u001b[A\n",
            " 17%|█▋        | 442/2620 [00:33<02:08, 17.00it/s]\u001b[A\n",
            " 17%|█▋        | 445/2620 [00:33<01:54, 18.98it/s]\u001b[A\n",
            " 17%|█▋        | 448/2620 [00:34<01:47, 20.19it/s]\u001b[A\n",
            " 17%|█▋        | 451/2620 [00:34<01:55, 18.84it/s]\u001b[A\n",
            " 17%|█▋        | 453/2620 [00:34<02:05, 17.30it/s]\u001b[A\n",
            " 17%|█▋        | 456/2620 [00:34<01:52, 19.16it/s]\u001b[A\n",
            " 17%|█▋        | 458/2620 [00:34<01:54, 18.87it/s]\u001b[A\n",
            " 18%|█▊        | 460/2620 [00:34<01:57, 18.36it/s]\u001b[A\n",
            " 18%|█▊        | 462/2620 [00:34<02:11, 16.44it/s]\u001b[A\n",
            " 18%|█▊        | 464/2620 [00:35<02:13, 16.14it/s]\u001b[A\n",
            " 18%|█▊        | 466/2620 [00:35<02:29, 14.42it/s]\u001b[A\n",
            " 18%|█▊        | 468/2620 [00:35<02:32, 14.08it/s]\u001b[A\n",
            " 18%|█▊        | 470/2620 [00:35<02:21, 15.21it/s]\u001b[A\n",
            " 18%|█▊        | 472/2620 [00:35<03:49,  9.36it/s]\u001b[A\n",
            " 18%|█▊        | 474/2620 [00:36<03:16, 10.90it/s]\u001b[A\n",
            " 18%|█▊        | 476/2620 [00:36<02:59, 11.95it/s]\u001b[A\n",
            " 18%|█▊        | 478/2620 [00:36<03:33, 10.02it/s]\u001b[A\n",
            " 18%|█▊        | 481/2620 [00:36<02:57, 12.05it/s]\u001b[A\n",
            " 18%|█▊        | 483/2620 [00:36<02:44, 12.96it/s]\u001b[A\n",
            " 19%|█▊        | 485/2620 [00:36<02:29, 14.32it/s]\u001b[A\n",
            " 19%|█▊        | 488/2620 [00:36<02:05, 16.97it/s]\u001b[A\n",
            " 19%|█▊        | 490/2620 [00:37<02:02, 17.34it/s]\u001b[A\n",
            " 19%|█▉        | 493/2620 [00:37<01:48, 19.69it/s]\u001b[A\n",
            " 19%|█▉        | 496/2620 [00:37<02:04, 17.08it/s]\u001b[A\n",
            " 19%|█▉        | 498/2620 [00:37<02:03, 17.23it/s]\u001b[A\n",
            " 19%|█▉        | 500/2620 [00:37<02:12, 15.96it/s]\u001b[A\n",
            " 19%|█▉        | 503/2620 [00:37<01:55, 18.34it/s]\u001b[A\n",
            " 19%|█▉        | 505/2620 [00:37<02:17, 15.44it/s]\u001b[A\n",
            " 19%|█▉        | 507/2620 [00:38<02:20, 15.07it/s]\u001b[A\n",
            " 19%|█▉        | 509/2620 [00:38<02:22, 14.84it/s]\u001b[A\n",
            " 20%|█▉        | 512/2620 [00:38<02:42, 13.00it/s]\u001b[A\n",
            " 20%|█▉        | 514/2620 [00:38<02:58, 11.80it/s]\u001b[A\n",
            " 20%|█▉        | 516/2620 [00:38<02:56, 11.90it/s]\u001b[A\n",
            " 20%|█▉        | 518/2620 [00:39<03:08, 11.15it/s]\u001b[A\n",
            " 20%|█▉        | 520/2620 [00:39<03:48,  9.20it/s]\u001b[A\n",
            " 20%|█▉        | 522/2620 [00:39<04:46,  7.32it/s]\u001b[A\n",
            " 20%|█▉        | 523/2620 [00:39<04:50,  7.22it/s]\u001b[A\n",
            " 20%|██        | 524/2620 [00:40<04:54,  7.13it/s]\u001b[A\n",
            " 20%|██        | 525/2620 [00:40<04:49,  7.24it/s]\u001b[A\n",
            " 20%|██        | 526/2620 [00:40<04:39,  7.48it/s]\u001b[A\n",
            " 20%|██        | 527/2620 [00:40<04:41,  7.43it/s]\u001b[A\n",
            " 20%|██        | 531/2620 [00:40<02:29, 13.95it/s]\u001b[A\n",
            " 20%|██        | 534/2620 [00:40<01:59, 17.51it/s]\u001b[A\n",
            " 20%|██        | 536/2620 [00:40<01:55, 18.00it/s]\u001b[A\n",
            " 21%|██        | 539/2620 [00:40<01:43, 20.01it/s]\u001b[A\n",
            " 21%|██        | 542/2620 [00:41<02:15, 15.32it/s]\u001b[A\n",
            " 21%|██        | 545/2620 [00:41<02:13, 15.58it/s]\u001b[A\n",
            " 21%|██        | 547/2620 [00:41<02:09, 16.00it/s]\u001b[A\n",
            " 21%|██        | 549/2620 [00:41<02:18, 14.91it/s]\u001b[A\n",
            " 21%|██        | 551/2620 [00:41<02:49, 12.21it/s]\u001b[A\n",
            " 21%|██        | 555/2620 [00:42<02:07, 16.14it/s]\u001b[A\n",
            " 21%|██▏       | 559/2620 [00:42<01:42, 20.08it/s]\u001b[A\n",
            " 21%|██▏       | 562/2620 [00:42<01:45, 19.60it/s]\u001b[A\n",
            " 22%|██▏       | 565/2620 [00:42<01:43, 19.77it/s]\u001b[A\n",
            " 22%|██▏       | 568/2620 [00:42<01:37, 21.08it/s]\u001b[A\n",
            " 22%|██▏       | 571/2620 [00:42<01:55, 17.74it/s]\u001b[A\n",
            " 22%|██▏       | 574/2620 [00:43<02:03, 16.51it/s]\u001b[A\n",
            " 22%|██▏       | 576/2620 [00:43<02:37, 12.99it/s]\u001b[A\n",
            " 22%|██▏       | 578/2620 [00:43<02:59, 11.39it/s]\u001b[A\n",
            " 22%|██▏       | 580/2620 [00:43<02:55, 11.61it/s]\u001b[A\n",
            " 22%|██▏       | 582/2620 [00:43<02:55, 11.59it/s]\u001b[A\n",
            " 22%|██▏       | 584/2620 [00:44<03:14, 10.47it/s]\u001b[A\n",
            " 22%|██▏       | 586/2620 [00:44<03:25,  9.91it/s]\u001b[A\n",
            " 22%|██▏       | 588/2620 [00:44<03:01, 11.21it/s]\u001b[A\n",
            " 23%|██▎       | 590/2620 [00:44<03:55,  8.62it/s]\u001b[A\n",
            " 23%|██▎       | 592/2620 [00:45<04:00,  8.42it/s]\u001b[A\n",
            " 23%|██▎       | 594/2620 [00:45<03:47,  8.90it/s]\u001b[A\n",
            " 23%|██▎       | 595/2620 [00:45<03:52,  8.73it/s]\u001b[A\n",
            " 23%|██▎       | 596/2620 [00:45<03:48,  8.86it/s]\u001b[A\n",
            " 23%|██▎       | 598/2620 [00:45<03:32,  9.50it/s]\u001b[A\n",
            " 23%|██▎       | 599/2620 [00:45<03:57,  8.51it/s]\u001b[A\n",
            " 23%|██▎       | 601/2620 [00:46<04:28,  7.51it/s]\u001b[A\n",
            " 23%|██▎       | 603/2620 [00:46<03:48,  8.83it/s]\u001b[A\n",
            " 23%|██▎       | 604/2620 [00:46<04:04,  8.23it/s]\u001b[A\n",
            " 23%|██▎       | 605/2620 [00:46<04:00,  8.37it/s]\u001b[A\n",
            " 23%|██▎       | 606/2620 [00:46<04:23,  7.64it/s]\u001b[A\n",
            " 23%|██▎       | 608/2620 [00:47<04:40,  7.16it/s]\u001b[A\n",
            " 23%|██▎       | 610/2620 [00:47<03:49,  8.74it/s]\u001b[A\n",
            " 23%|██▎       | 613/2620 [00:47<02:47, 12.00it/s]\u001b[A\n",
            " 23%|██▎       | 615/2620 [00:47<02:44, 12.18it/s]\u001b[A\n",
            " 24%|██▎       | 617/2620 [00:47<02:53, 11.55it/s]\u001b[A\n",
            " 24%|██▎       | 620/2620 [00:47<02:26, 13.63it/s]\u001b[A\n",
            " 24%|██▎       | 622/2620 [00:48<02:23, 13.88it/s]\u001b[A\n",
            " 24%|██▍       | 625/2620 [00:48<02:04, 16.04it/s]\u001b[A\n",
            " 24%|██▍       | 627/2620 [00:48<01:59, 16.63it/s]\u001b[A\n",
            " 24%|██▍       | 629/2620 [00:48<02:01, 16.35it/s]\u001b[A\n",
            " 24%|██▍       | 631/2620 [00:48<02:03, 16.12it/s]\u001b[A\n",
            " 24%|██▍       | 633/2620 [00:48<01:56, 17.00it/s]\u001b[A\n",
            " 24%|██▍       | 636/2620 [00:48<01:46, 18.55it/s]\u001b[A\n",
            " 24%|██▍       | 638/2620 [00:48<01:49, 18.03it/s]\u001b[A\n",
            " 24%|██▍       | 640/2620 [00:49<01:55, 17.15it/s]\u001b[A\n",
            " 25%|██▍       | 642/2620 [00:49<01:53, 17.41it/s]\u001b[A\n",
            " 25%|██▍       | 644/2620 [00:49<02:22, 13.84it/s]\u001b[A\n",
            " 25%|██▍       | 646/2620 [00:49<02:28, 13.33it/s]\u001b[A\n",
            " 25%|██▍       | 650/2620 [00:49<01:47, 18.24it/s]\u001b[A\n",
            " 25%|██▍       | 653/2620 [00:49<01:37, 20.12it/s]\u001b[A\n",
            " 25%|██▌       | 656/2620 [00:49<01:36, 20.40it/s]\u001b[A\n",
            " 25%|██▌       | 659/2620 [00:50<01:52, 17.46it/s]\u001b[A\n",
            " 25%|██▌       | 662/2620 [00:50<01:43, 18.85it/s]\u001b[A\n",
            " 25%|██▌       | 666/2620 [00:50<01:28, 22.18it/s]\u001b[A\n",
            " 26%|██▌       | 670/2620 [00:50<01:19, 24.59it/s]\u001b[A\n",
            " 26%|██▌       | 673/2620 [00:50<01:38, 19.74it/s]\u001b[A\n",
            " 26%|██▌       | 676/2620 [00:50<01:42, 18.90it/s]\u001b[A\n",
            " 26%|██▌       | 679/2620 [00:51<01:45, 18.38it/s]\u001b[A\n",
            " 26%|██▌       | 682/2620 [00:51<01:39, 19.43it/s]\u001b[A\n",
            " 26%|██▌       | 685/2620 [00:51<01:29, 21.63it/s]\u001b[A\n",
            " 26%|██▋       | 688/2620 [00:51<01:25, 22.69it/s]\u001b[A\n",
            " 26%|██▋       | 691/2620 [00:51<01:29, 21.66it/s]\u001b[A\n",
            " 26%|██▋       | 694/2620 [00:51<01:29, 21.46it/s]\u001b[A\n",
            " 27%|██▋       | 697/2620 [00:51<01:32, 20.73it/s]\u001b[A\n",
            " 27%|██▋       | 700/2620 [00:52<01:26, 22.10it/s]\u001b[A\n",
            " 27%|██▋       | 703/2620 [00:52<01:34, 20.33it/s]\u001b[A\n",
            " 27%|██▋       | 706/2620 [00:52<01:42, 18.67it/s]\u001b[A\n",
            " 27%|██▋       | 709/2620 [00:52<01:40, 19.03it/s]\u001b[A\n",
            " 27%|██▋       | 712/2620 [00:52<01:36, 19.74it/s]\u001b[A\n",
            " 27%|██▋       | 715/2620 [00:52<01:27, 21.83it/s]\u001b[A\n",
            " 27%|██▋       | 718/2620 [00:52<01:25, 22.16it/s]\u001b[A\n",
            " 28%|██▊       | 721/2620 [00:53<01:30, 20.90it/s]\u001b[A\n",
            " 28%|██▊       | 724/2620 [00:53<01:35, 19.91it/s]\u001b[A\n",
            " 28%|██▊       | 727/2620 [00:53<01:30, 21.01it/s]\u001b[A\n",
            " 28%|██▊       | 730/2620 [00:53<01:44, 18.13it/s]\u001b[A\n",
            " 28%|██▊       | 732/2620 [00:53<01:45, 17.83it/s]\u001b[A\n",
            " 28%|██▊       | 734/2620 [00:53<01:48, 17.45it/s]\u001b[A\n",
            " 28%|██▊       | 737/2620 [00:53<01:43, 18.18it/s]\u001b[A\n",
            " 28%|██▊       | 739/2620 [00:54<01:48, 17.37it/s]\u001b[A\n",
            " 28%|██▊       | 741/2620 [00:54<01:59, 15.67it/s]\u001b[A\n",
            " 28%|██▊       | 743/2620 [00:54<02:07, 14.75it/s]\u001b[A\n",
            " 28%|██▊       | 745/2620 [00:54<02:05, 14.91it/s]\u001b[A\n",
            " 29%|██▊       | 747/2620 [00:54<02:04, 15.09it/s]\u001b[A\n",
            " 29%|██▊       | 749/2620 [00:54<02:02, 15.23it/s]\u001b[A\n",
            " 29%|██▊       | 751/2620 [00:55<02:32, 12.27it/s]\u001b[A\n",
            " 29%|██▉       | 754/2620 [00:55<02:02, 15.27it/s]\u001b[A\n",
            " 29%|██▉       | 757/2620 [00:55<01:41, 18.43it/s]\u001b[A\n",
            " 29%|██▉       | 760/2620 [00:55<02:05, 14.84it/s]\u001b[A\n",
            " 29%|██▉       | 762/2620 [00:55<02:22, 13.08it/s]\u001b[A\n",
            " 29%|██▉       | 766/2620 [00:55<02:01, 15.21it/s]\u001b[A\n",
            " 29%|██▉       | 768/2620 [00:56<02:06, 14.69it/s]\u001b[A\n",
            " 29%|██▉       | 772/2620 [00:56<01:40, 18.41it/s]\u001b[A\n",
            " 30%|██▉       | 774/2620 [00:56<01:40, 18.29it/s]\u001b[A\n",
            " 30%|██▉       | 776/2620 [00:56<01:38, 18.64it/s]\u001b[A\n",
            " 30%|██▉       | 779/2620 [00:56<01:32, 19.84it/s]\u001b[A\n",
            " 30%|██▉       | 782/2620 [00:56<01:25, 21.44it/s]\u001b[A\n",
            " 30%|██▉       | 785/2620 [00:56<01:31, 19.97it/s]\u001b[A\n",
            " 30%|███       | 788/2620 [00:57<01:53, 16.17it/s]\u001b[A\n",
            " 30%|███       | 792/2620 [00:57<01:48, 16.83it/s]\u001b[A\n",
            " 30%|███       | 794/2620 [00:57<01:57, 15.49it/s]\u001b[A\n",
            " 30%|███       | 796/2620 [00:57<01:56, 15.62it/s]\u001b[A\n",
            " 30%|███       | 799/2620 [00:57<01:40, 18.08it/s]\u001b[A\n",
            " 31%|███       | 803/2620 [00:57<01:27, 20.69it/s]\u001b[A\n",
            " 31%|███       | 806/2620 [00:58<01:38, 18.37it/s]\u001b[A\n",
            " 31%|███       | 808/2620 [00:58<01:38, 18.46it/s]\u001b[A\n",
            " 31%|███       | 811/2620 [00:58<01:31, 19.73it/s]\u001b[A\n",
            " 31%|███       | 815/2620 [00:58<01:17, 23.40it/s]\u001b[A\n",
            " 31%|███       | 818/2620 [00:58<01:18, 23.07it/s]\u001b[A\n",
            " 31%|███▏      | 821/2620 [00:58<01:16, 23.44it/s]\u001b[A\n",
            " 31%|███▏      | 824/2620 [00:58<01:18, 22.79it/s]\u001b[A\n",
            " 32%|███▏      | 827/2620 [00:59<01:20, 22.16it/s]\u001b[A\n",
            " 32%|███▏      | 830/2620 [00:59<01:32, 19.26it/s]\u001b[A\n",
            " 32%|███▏      | 834/2620 [00:59<01:23, 21.40it/s]\u001b[A\n",
            " 32%|███▏      | 837/2620 [00:59<01:27, 20.38it/s]\u001b[A\n",
            " 32%|███▏      | 840/2620 [00:59<01:21, 21.94it/s]\u001b[A\n",
            " 32%|███▏      | 844/2620 [00:59<01:12, 24.47it/s]\u001b[A\n",
            " 32%|███▏      | 847/2620 [01:00<01:26, 20.51it/s]\u001b[A\n",
            " 32%|███▏      | 850/2620 [01:00<01:32, 19.11it/s]\u001b[A\n",
            " 33%|███▎      | 853/2620 [01:00<01:42, 17.28it/s]\u001b[A\n",
            " 33%|███▎      | 855/2620 [01:00<01:45, 16.80it/s]\u001b[A\n",
            " 33%|███▎      | 859/2620 [01:00<01:29, 19.70it/s]\u001b[A\n",
            " 33%|███▎      | 862/2620 [01:00<01:26, 20.21it/s]\u001b[A\n",
            " 33%|███▎      | 865/2620 [01:01<01:35, 18.33it/s]\u001b[A\n",
            " 33%|███▎      | 867/2620 [01:01<01:34, 18.47it/s]\u001b[A\n",
            " 33%|███▎      | 870/2620 [01:01<01:28, 19.72it/s]\u001b[A\n",
            " 33%|███▎      | 873/2620 [01:01<01:48, 16.06it/s]\u001b[A\n",
            " 33%|███▎      | 875/2620 [01:01<02:21, 12.32it/s]\u001b[A\n",
            " 33%|███▎      | 877/2620 [01:02<02:31, 11.48it/s]\u001b[A\n",
            " 34%|███▎      | 879/2620 [01:02<02:45, 10.52it/s]\u001b[A\n",
            " 34%|███▎      | 881/2620 [01:02<02:55,  9.92it/s]\u001b[A\n",
            " 34%|███▎      | 883/2620 [01:02<02:56,  9.86it/s]\u001b[A\n",
            " 34%|███▍      | 885/2620 [01:02<03:18,  8.75it/s]\u001b[A\n",
            " 34%|███▍      | 886/2620 [01:03<03:39,  7.91it/s]\u001b[A\n",
            " 34%|███▍      | 888/2620 [01:03<03:15,  8.86it/s]\u001b[A\n",
            " 34%|███▍      | 890/2620 [01:03<02:54,  9.89it/s]\u001b[A\n",
            " 34%|███▍      | 892/2620 [01:03<02:38, 10.89it/s]\u001b[A\n",
            " 34%|███▍      | 894/2620 [01:03<02:39, 10.83it/s]\u001b[A\n",
            " 34%|███▍      | 896/2620 [01:04<02:50, 10.11it/s]\u001b[A\n",
            " 34%|███▍      | 898/2620 [01:04<02:29, 11.55it/s]\u001b[A\n",
            " 34%|███▍      | 900/2620 [01:04<02:51, 10.01it/s]\u001b[A\n",
            " 34%|███▍      | 902/2620 [01:04<02:37, 10.92it/s]\u001b[A\n",
            " 35%|███▍      | 905/2620 [01:04<02:11, 13.07it/s]\u001b[A\n",
            " 35%|███▍      | 907/2620 [01:04<02:03, 13.85it/s]\u001b[A\n",
            " 35%|███▍      | 909/2620 [01:04<02:02, 13.95it/s]\u001b[A\n",
            " 35%|███▍      | 911/2620 [01:05<02:08, 13.30it/s]\u001b[A\n",
            " 35%|███▍      | 913/2620 [01:05<02:08, 13.26it/s]\u001b[A\n",
            " 35%|███▍      | 915/2620 [01:05<02:11, 12.99it/s]\u001b[A\n",
            " 35%|███▌      | 917/2620 [01:05<02:13, 12.76it/s]\u001b[A\n",
            " 35%|███▌      | 919/2620 [01:05<02:00, 14.09it/s]\u001b[A\n",
            " 35%|███▌      | 921/2620 [01:06<02:49, 10.01it/s]\u001b[A\n",
            " 35%|███▌      | 923/2620 [01:06<02:37, 10.78it/s]\u001b[A\n",
            " 35%|███▌      | 925/2620 [01:06<02:47, 10.15it/s]\u001b[A\n",
            " 35%|███▌      | 927/2620 [01:06<02:49, 10.00it/s]\u001b[A\n",
            " 35%|███▌      | 929/2620 [01:06<02:44, 10.26it/s]\u001b[A\n",
            " 36%|███▌      | 931/2620 [01:07<02:54,  9.68it/s]\u001b[A\n",
            " 36%|███▌      | 933/2620 [01:07<02:37, 10.70it/s]\u001b[A\n",
            " 36%|███▌      | 935/2620 [01:07<02:30, 11.22it/s]\u001b[A\n",
            " 36%|███▌      | 937/2620 [01:07<02:38, 10.65it/s]\u001b[A\n",
            " 36%|███▌      | 939/2620 [01:07<02:28, 11.32it/s]\u001b[A\n",
            " 36%|███▌      | 941/2620 [01:07<02:25, 11.52it/s]\u001b[A\n",
            " 36%|███▌      | 943/2620 [01:08<02:17, 12.22it/s]\u001b[A\n",
            " 36%|███▌      | 945/2620 [01:08<02:21, 11.84it/s]\u001b[A\n",
            " 36%|███▌      | 947/2620 [01:08<02:12, 12.65it/s]\u001b[A\n",
            " 36%|███▌      | 949/2620 [01:08<02:42, 10.28it/s]\u001b[A\n",
            " 36%|███▋      | 951/2620 [01:08<02:34, 10.81it/s]\u001b[A\n",
            " 36%|███▋      | 953/2620 [01:08<02:17, 12.13it/s]\u001b[A\n",
            " 36%|███▋      | 955/2620 [01:09<02:07, 13.11it/s]\u001b[A\n",
            " 37%|███▋      | 957/2620 [01:09<02:13, 12.41it/s]\u001b[A\n",
            " 37%|███▋      | 959/2620 [01:09<02:17, 12.06it/s]\u001b[A\n",
            " 37%|███▋      | 961/2620 [01:09<02:39, 10.39it/s]\u001b[A\n",
            " 37%|███▋      | 963/2620 [01:09<03:04,  8.98it/s]\u001b[A\n",
            " 37%|███▋      | 965/2620 [01:10<02:50,  9.72it/s]\u001b[A\n",
            " 37%|███▋      | 967/2620 [01:10<02:32, 10.86it/s]\u001b[A\n",
            " 37%|███▋      | 969/2620 [01:10<02:23, 11.47it/s]\u001b[A\n",
            " 37%|███▋      | 971/2620 [01:10<02:16, 12.04it/s]\u001b[A\n",
            " 37%|███▋      | 974/2620 [01:10<01:49, 14.97it/s]\u001b[A\n",
            " 37%|███▋      | 976/2620 [01:10<01:42, 15.98it/s]\u001b[A\n",
            " 37%|███▋      | 978/2620 [01:10<01:48, 15.15it/s]\u001b[A\n",
            " 37%|███▋      | 980/2620 [01:11<02:01, 13.48it/s]\u001b[A\n",
            " 37%|███▋      | 982/2620 [01:11<01:53, 14.45it/s]\u001b[A\n",
            " 38%|███▊      | 984/2620 [01:11<01:54, 14.27it/s]\u001b[A\n",
            " 38%|███▊      | 986/2620 [01:11<02:07, 12.77it/s]\u001b[A\n",
            " 38%|███▊      | 988/2620 [01:11<02:03, 13.27it/s]\u001b[A\n",
            " 38%|███▊      | 991/2620 [01:11<01:51, 14.59it/s]\u001b[A\n",
            " 38%|███▊      | 994/2620 [01:12<01:46, 15.28it/s]\u001b[A\n",
            " 38%|███▊      | 996/2620 [01:12<02:00, 13.44it/s]\u001b[A\n",
            " 38%|███▊      | 998/2620 [01:12<02:13, 12.12it/s]\u001b[A\n",
            " 38%|███▊      | 1000/2620 [01:12<02:02, 13.18it/s]\u001b[A\n",
            " 38%|███▊      | 1002/2620 [01:12<01:58, 13.69it/s]\u001b[A\n",
            " 38%|███▊      | 1004/2620 [01:12<01:50, 14.58it/s]\u001b[A\n",
            " 38%|███▊      | 1006/2620 [01:12<01:48, 14.86it/s]\u001b[A\n",
            " 38%|███▊      | 1008/2620 [01:13<02:46,  9.66it/s]\u001b[A\n",
            " 39%|███▊      | 1010/2620 [01:13<02:49,  9.50it/s]\u001b[A\n",
            " 39%|███▊      | 1012/2620 [01:13<02:45,  9.69it/s]\u001b[A\n",
            " 39%|███▊      | 1014/2620 [01:14<03:04,  8.71it/s]\u001b[A\n",
            " 39%|███▉      | 1016/2620 [01:14<02:46,  9.65it/s]\u001b[A\n",
            " 39%|███▉      | 1018/2620 [01:14<03:06,  8.57it/s]\u001b[A\n",
            " 39%|███▉      | 1021/2620 [01:14<02:23, 11.16it/s]\u001b[A\n",
            " 39%|███▉      | 1024/2620 [01:14<02:07, 12.49it/s]\u001b[A\n",
            " 39%|███▉      | 1026/2620 [01:15<02:11, 12.09it/s]\u001b[A\n",
            " 39%|███▉      | 1028/2620 [01:15<02:06, 12.60it/s]\u001b[A\n",
            " 39%|███▉      | 1030/2620 [01:15<02:13, 11.89it/s]\u001b[A\n",
            " 39%|███▉      | 1032/2620 [01:15<02:53,  9.18it/s]\u001b[A\n",
            " 39%|███▉      | 1034/2620 [01:16<03:29,  7.56it/s]\u001b[A\n",
            " 40%|███▉      | 1035/2620 [01:16<03:25,  7.71it/s]\u001b[A\n",
            " 40%|███▉      | 1036/2620 [01:16<03:52,  6.81it/s]\u001b[A\n",
            " 40%|███▉      | 1037/2620 [01:16<03:59,  6.61it/s]\u001b[A\n",
            " 40%|███▉      | 1039/2620 [01:16<02:57,  8.89it/s]\u001b[A\n",
            " 40%|███▉      | 1042/2620 [01:16<02:04, 12.64it/s]\u001b[A\n",
            " 40%|███▉      | 1045/2620 [01:17<02:02, 12.87it/s]\u001b[A\n",
            " 40%|████      | 1048/2620 [01:17<01:57, 13.33it/s]\u001b[A\n",
            " 40%|████      | 1050/2620 [01:17<01:50, 14.25it/s]\u001b[A\n",
            " 40%|████      | 1052/2620 [01:17<01:48, 14.49it/s]\u001b[A\n",
            " 40%|████      | 1055/2620 [01:17<01:28, 17.60it/s]\u001b[A\n",
            " 40%|████      | 1057/2620 [01:17<01:32, 16.88it/s]\u001b[A\n",
            " 40%|████      | 1060/2620 [01:17<01:35, 16.25it/s]\u001b[A\n",
            " 41%|████      | 1062/2620 [01:18<02:37,  9.92it/s]\u001b[A\n",
            " 41%|████      | 1064/2620 [01:18<02:43,  9.53it/s]\u001b[A\n",
            " 41%|████      | 1066/2620 [01:18<02:21, 10.95it/s]\u001b[A\n",
            " 41%|████      | 1068/2620 [01:18<02:20, 11.04it/s]\u001b[A\n",
            " 41%|████      | 1070/2620 [01:19<02:31, 10.20it/s]\u001b[A\n",
            " 41%|████      | 1072/2620 [01:19<02:12, 11.70it/s]\u001b[A\n",
            " 41%|████      | 1074/2620 [01:19<02:41,  9.59it/s]\u001b[A\n",
            " 41%|████      | 1076/2620 [01:19<02:32, 10.13it/s]\u001b[A\n",
            " 41%|████      | 1078/2620 [01:19<02:41,  9.55it/s]\u001b[A\n",
            " 41%|████      | 1080/2620 [01:20<03:06,  8.24it/s]\u001b[A\n",
            " 41%|████▏     | 1081/2620 [01:20<03:06,  8.24it/s]\u001b[A\n",
            " 41%|████▏     | 1082/2620 [01:20<03:17,  7.78it/s]\u001b[A\n",
            " 41%|████▏     | 1083/2620 [01:20<03:07,  8.18it/s]\u001b[A\n",
            " 41%|████▏     | 1084/2620 [01:20<03:04,  8.32it/s]\u001b[A\n",
            " 41%|████▏     | 1086/2620 [01:20<02:41,  9.51it/s]\u001b[A\n",
            " 41%|████▏     | 1087/2620 [01:21<02:44,  9.32it/s]\u001b[A\n",
            " 42%|████▏     | 1089/2620 [01:21<02:27, 10.36it/s]\u001b[A\n",
            " 42%|████▏     | 1091/2620 [01:21<02:45,  9.26it/s]\u001b[A\n",
            " 42%|████▏     | 1093/2620 [01:21<03:02,  8.38it/s]\u001b[A\n",
            " 42%|████▏     | 1095/2620 [01:21<02:34,  9.90it/s]\u001b[A\n",
            " 42%|████▏     | 1097/2620 [01:22<02:39,  9.55it/s]\u001b[A\n",
            " 42%|████▏     | 1099/2620 [01:22<02:36,  9.69it/s]\u001b[A\n",
            " 42%|████▏     | 1101/2620 [01:22<02:13, 11.37it/s]\u001b[A\n",
            " 42%|████▏     | 1103/2620 [01:22<02:36,  9.70it/s]\u001b[A\n",
            " 42%|████▏     | 1105/2620 [01:22<02:58,  8.49it/s]\u001b[A\n",
            " 42%|████▏     | 1107/2620 [01:23<02:44,  9.18it/s]\u001b[A\n",
            " 42%|████▏     | 1109/2620 [01:23<03:08,  8.00it/s]\u001b[A\n",
            " 42%|████▏     | 1110/2620 [01:23<03:07,  8.07it/s]\u001b[A\n",
            " 42%|████▏     | 1111/2620 [01:23<03:00,  8.34it/s]\u001b[A\n",
            " 42%|████▏     | 1112/2620 [01:23<03:32,  7.10it/s]\u001b[A\n",
            " 42%|████▏     | 1113/2620 [01:23<03:21,  7.48it/s]\u001b[A\n",
            " 43%|████▎     | 1116/2620 [01:24<02:10, 11.55it/s]\u001b[A\n",
            " 43%|████▎     | 1118/2620 [01:24<02:03, 12.16it/s]\u001b[A\n",
            " 43%|████▎     | 1122/2620 [01:24<01:29, 16.83it/s]\u001b[A\n",
            " 43%|████▎     | 1124/2620 [01:24<01:27, 17.18it/s]\u001b[A\n",
            " 43%|████▎     | 1127/2620 [01:24<01:18, 18.96it/s]\u001b[A\n",
            " 43%|████▎     | 1130/2620 [01:24<01:20, 18.45it/s]\u001b[A\n",
            " 43%|████▎     | 1132/2620 [01:24<01:31, 16.29it/s]\u001b[A\n",
            " 43%|████▎     | 1134/2620 [01:25<01:28, 16.75it/s]\u001b[A\n",
            " 43%|████▎     | 1138/2620 [01:25<01:09, 21.28it/s]\u001b[A\n",
            " 44%|████▎     | 1141/2620 [01:25<01:13, 20.24it/s]\u001b[A\n",
            " 44%|████▎     | 1144/2620 [01:25<01:16, 19.34it/s]\u001b[A\n",
            " 44%|████▍     | 1147/2620 [01:25<01:13, 20.06it/s]\u001b[A\n",
            " 44%|████▍     | 1150/2620 [01:25<01:18, 18.71it/s]\u001b[A\n",
            " 44%|████▍     | 1154/2620 [01:25<01:06, 21.90it/s]\u001b[A\n",
            " 44%|████▍     | 1157/2620 [01:26<01:09, 21.06it/s]\u001b[A\n",
            " 44%|████▍     | 1160/2620 [01:26<01:05, 22.44it/s]\u001b[A\n",
            " 44%|████▍     | 1163/2620 [01:26<01:08, 21.23it/s]\u001b[A\n",
            " 45%|████▍     | 1166/2620 [01:26<01:03, 22.79it/s]\u001b[A\n",
            " 45%|████▍     | 1169/2620 [01:26<01:02, 23.30it/s]\u001b[A\n",
            " 45%|████▍     | 1172/2620 [01:26<01:02, 23.19it/s]\u001b[A\n",
            " 45%|████▍     | 1175/2620 [01:26<01:02, 23.17it/s]\u001b[A\n",
            " 45%|████▍     | 1178/2620 [01:27<01:08, 20.94it/s]\u001b[A\n",
            " 45%|████▌     | 1182/2620 [01:27<01:00, 23.60it/s]\u001b[A\n",
            " 45%|████▌     | 1185/2620 [01:27<01:03, 22.50it/s]\u001b[A\n",
            " 45%|████▌     | 1188/2620 [01:27<01:01, 23.17it/s]\u001b[A\n",
            " 45%|████▌     | 1192/2620 [01:27<01:00, 23.69it/s]\u001b[A\n",
            " 46%|████▌     | 1195/2620 [01:27<00:57, 24.70it/s]\u001b[A\n",
            " 46%|████▌     | 1198/2620 [01:27<00:56, 25.03it/s]\u001b[A\n",
            " 46%|████▌     | 1202/2620 [01:28<00:54, 26.05it/s]\u001b[A\n",
            " 46%|████▌     | 1205/2620 [01:28<00:56, 24.90it/s]\u001b[A\n",
            " 46%|████▌     | 1208/2620 [01:28<00:54, 25.98it/s]\u001b[A\n",
            " 46%|████▌     | 1211/2620 [01:28<00:58, 24.25it/s]\u001b[A\n",
            " 46%|████▋     | 1214/2620 [01:28<00:54, 25.63it/s]\u001b[A\n",
            " 46%|████▋     | 1217/2620 [01:28<00:57, 24.42it/s]\u001b[A\n",
            " 47%|████▋     | 1220/2620 [01:28<00:59, 23.41it/s]\u001b[A\n",
            " 47%|████▋     | 1223/2620 [01:28<01:03, 21.86it/s]\u001b[A\n",
            " 47%|████▋     | 1226/2620 [01:29<00:59, 23.38it/s]\u001b[A\n",
            " 47%|████▋     | 1230/2620 [01:29<00:54, 25.70it/s]\u001b[A\n",
            " 47%|████▋     | 1233/2620 [01:29<01:17, 18.00it/s]\u001b[A\n",
            " 47%|████▋     | 1236/2620 [01:29<01:19, 17.50it/s]\u001b[A\n",
            " 47%|████▋     | 1238/2620 [01:29<01:30, 15.26it/s]\u001b[A\n",
            " 47%|████▋     | 1240/2620 [01:29<01:30, 15.22it/s]\u001b[A\n",
            " 47%|████▋     | 1242/2620 [01:30<01:47, 12.84it/s]\u001b[A\n",
            " 47%|████▋     | 1244/2620 [01:30<01:53, 12.16it/s]\u001b[A\n",
            " 48%|████▊     | 1246/2620 [01:30<01:51, 12.32it/s]\u001b[A\n",
            " 48%|████▊     | 1248/2620 [01:30<02:30,  9.11it/s]\u001b[A\n",
            " 48%|████▊     | 1252/2620 [01:31<01:57, 11.62it/s]\u001b[A\n",
            " 48%|████▊     | 1254/2620 [01:31<02:29,  9.11it/s]\u001b[A\n",
            " 48%|████▊     | 1256/2620 [01:31<02:18,  9.84it/s]\u001b[A\n",
            " 48%|████▊     | 1258/2620 [01:31<02:03, 11.02it/s]\u001b[A\n",
            " 48%|████▊     | 1261/2620 [01:32<01:54, 11.82it/s]\u001b[A\n",
            " 48%|████▊     | 1264/2620 [01:32<01:40, 13.51it/s]\u001b[A\n",
            " 48%|████▊     | 1267/2620 [01:32<01:23, 16.19it/s]\u001b[A\n",
            " 48%|████▊     | 1269/2620 [01:32<02:00, 11.19it/s]\u001b[A\n",
            " 49%|████▊     | 1271/2620 [01:32<01:48, 12.48it/s]\u001b[A\n",
            " 49%|████▊     | 1274/2620 [01:32<01:34, 14.30it/s]\u001b[A\n",
            " 49%|████▊     | 1276/2620 [01:33<01:30, 14.87it/s]\u001b[A\n",
            " 49%|████▉     | 1278/2620 [01:33<01:28, 15.14it/s]\u001b[A\n",
            " 49%|████▉     | 1281/2620 [01:33<01:19, 16.94it/s]\u001b[A\n",
            " 49%|████▉     | 1283/2620 [01:33<01:22, 16.15it/s]\u001b[A\n",
            " 49%|████▉     | 1285/2620 [01:33<01:43, 12.95it/s]\u001b[A\n",
            " 49%|████▉     | 1287/2620 [01:33<01:58, 11.24it/s]\u001b[A\n",
            " 49%|████▉     | 1289/2620 [01:34<02:26,  9.08it/s]\u001b[A\n",
            " 49%|████▉     | 1291/2620 [01:34<02:07, 10.39it/s]\u001b[A\n",
            " 49%|████▉     | 1293/2620 [01:34<02:15,  9.78it/s]\u001b[A\n",
            " 49%|████▉     | 1295/2620 [01:34<02:06, 10.51it/s]\u001b[A\n",
            " 50%|████▉     | 1298/2620 [01:34<01:35, 13.90it/s]\u001b[A\n",
            " 50%|████▉     | 1300/2620 [01:34<01:30, 14.61it/s]\u001b[A\n",
            " 50%|████▉     | 1302/2620 [01:35<01:27, 15.07it/s]\u001b[A\n",
            " 50%|████▉     | 1305/2620 [01:35<01:20, 16.36it/s]\u001b[A\n",
            " 50%|████▉     | 1307/2620 [01:35<01:25, 15.28it/s]\u001b[A\n",
            " 50%|████▉     | 1309/2620 [01:35<01:27, 14.92it/s]\u001b[A\n",
            " 50%|█████     | 1311/2620 [01:35<01:27, 14.95it/s]\u001b[A\n",
            " 50%|█████     | 1313/2620 [01:35<01:35, 13.63it/s]\u001b[A\n",
            " 50%|█████     | 1316/2620 [01:35<01:18, 16.66it/s]\u001b[A\n",
            " 50%|█████     | 1319/2620 [01:36<01:06, 19.63it/s]\u001b[A\n",
            " 50%|█████     | 1322/2620 [01:36<01:02, 20.91it/s]\u001b[A\n",
            " 51%|█████     | 1325/2620 [01:36<01:07, 19.05it/s]\u001b[A\n",
            " 51%|█████     | 1328/2620 [01:36<01:31, 14.11it/s]\u001b[A\n",
            " 51%|█████     | 1331/2620 [01:36<01:21, 15.90it/s]\u001b[A\n",
            " 51%|█████     | 1333/2620 [01:37<01:30, 14.27it/s]\u001b[A\n",
            " 51%|█████     | 1335/2620 [01:37<01:35, 13.50it/s]\u001b[A\n",
            " 51%|█████     | 1337/2620 [01:37<01:40, 12.82it/s]\u001b[A\n",
            " 51%|█████     | 1339/2620 [01:37<01:42, 12.46it/s]\u001b[A\n",
            " 51%|█████     | 1341/2620 [01:37<01:32, 13.79it/s]\u001b[A\n",
            " 51%|█████▏    | 1343/2620 [01:37<01:28, 14.48it/s]\u001b[A\n",
            " 51%|█████▏    | 1345/2620 [01:37<01:29, 14.25it/s]\u001b[A\n",
            " 51%|█████▏    | 1348/2620 [01:38<01:12, 17.53it/s]\u001b[A\n",
            " 52%|█████▏    | 1350/2620 [01:38<01:22, 15.40it/s]\u001b[A\n",
            " 52%|█████▏    | 1353/2620 [01:38<01:11, 17.73it/s]\u001b[A\n",
            " 52%|█████▏    | 1355/2620 [01:38<01:14, 17.05it/s]\u001b[A\n",
            " 52%|█████▏    | 1357/2620 [01:38<01:35, 13.18it/s]\u001b[A\n",
            " 52%|█████▏    | 1359/2620 [01:38<01:28, 14.22it/s]\u001b[A\n",
            " 52%|█████▏    | 1362/2620 [01:39<01:28, 14.14it/s]\u001b[A\n",
            " 52%|█████▏    | 1364/2620 [01:39<01:26, 14.46it/s]\u001b[A\n",
            " 52%|█████▏    | 1366/2620 [01:39<01:28, 14.13it/s]\u001b[A\n",
            " 52%|█████▏    | 1368/2620 [01:39<01:58, 10.57it/s]\u001b[A\n",
            " 52%|█████▏    | 1371/2620 [01:39<01:35, 13.13it/s]\u001b[A\n",
            " 52%|█████▏    | 1373/2620 [01:39<01:28, 14.16it/s]\u001b[A\n",
            " 52%|█████▏    | 1375/2620 [01:40<01:38, 12.69it/s]\u001b[A\n",
            " 53%|█████▎    | 1377/2620 [01:40<01:31, 13.65it/s]\u001b[A\n",
            " 53%|█████▎    | 1379/2620 [01:40<01:26, 14.32it/s]\u001b[A\n",
            " 53%|█████▎    | 1381/2620 [01:40<01:28, 13.95it/s]\u001b[A\n",
            " 53%|█████▎    | 1383/2620 [01:40<01:23, 14.77it/s]\u001b[A\n",
            " 53%|█████▎    | 1385/2620 [01:40<01:23, 14.83it/s]\u001b[A\n",
            " 53%|█████▎    | 1387/2620 [01:40<01:25, 14.34it/s]\u001b[A\n",
            " 53%|█████▎    | 1389/2620 [01:41<01:35, 12.86it/s]\u001b[A\n",
            " 53%|█████▎    | 1391/2620 [01:41<01:50, 11.11it/s]\u001b[A\n",
            " 53%|█████▎    | 1393/2620 [01:41<01:40, 12.18it/s]\u001b[A\n",
            " 53%|█████▎    | 1395/2620 [01:41<01:34, 12.90it/s]\u001b[A\n",
            " 53%|█████▎    | 1397/2620 [01:41<01:45, 11.62it/s]\u001b[A\n",
            " 53%|█████▎    | 1400/2620 [01:41<01:26, 14.13it/s]\u001b[A\n",
            " 54%|█████▎    | 1402/2620 [01:42<01:23, 14.59it/s]\u001b[A\n",
            " 54%|█████▎    | 1404/2620 [01:42<01:42, 11.86it/s]\u001b[A\n",
            " 54%|█████▎    | 1406/2620 [01:42<01:56, 10.45it/s]\u001b[A\n",
            " 54%|█████▍    | 1409/2620 [01:42<01:37, 12.43it/s]\u001b[A\n",
            " 54%|█████▍    | 1411/2620 [01:42<01:42, 11.75it/s]\u001b[A\n",
            " 54%|█████▍    | 1413/2620 [01:43<01:43, 11.62it/s]\u001b[A\n",
            " 54%|█████▍    | 1415/2620 [01:43<02:13,  9.04it/s]\u001b[A\n",
            " 54%|█████▍    | 1417/2620 [01:43<02:19,  8.63it/s]\u001b[A\n",
            " 54%|█████▍    | 1419/2620 [01:43<02:13,  8.99it/s]\u001b[A\n",
            " 54%|█████▍    | 1422/2620 [01:44<01:45, 11.33it/s]\u001b[A\n",
            " 54%|█████▍    | 1424/2620 [01:44<01:48, 11.01it/s]\u001b[A\n",
            " 54%|█████▍    | 1426/2620 [01:44<01:44, 11.45it/s]\u001b[A\n",
            " 55%|█████▍    | 1428/2620 [01:44<01:38, 12.16it/s]\u001b[A\n",
            " 55%|█████▍    | 1430/2620 [01:44<01:27, 13.55it/s]\u001b[A\n",
            " 55%|█████▍    | 1433/2620 [01:44<01:16, 15.61it/s]\u001b[A\n",
            " 55%|█████▍    | 1435/2620 [01:44<01:11, 16.50it/s]\u001b[A\n",
            " 55%|█████▍    | 1438/2620 [01:44<01:02, 18.91it/s]\u001b[A\n",
            " 55%|█████▍    | 1440/2620 [01:45<01:11, 16.58it/s]\u001b[A\n",
            " 55%|█████▌    | 1442/2620 [01:45<01:29, 13.09it/s]\u001b[A\n",
            " 55%|█████▌    | 1444/2620 [01:45<01:47, 10.96it/s]\u001b[A\n",
            " 55%|█████▌    | 1446/2620 [01:45<01:41, 11.62it/s]\u001b[A\n",
            " 55%|█████▌    | 1448/2620 [01:46<01:51, 10.49it/s]\u001b[A\n",
            " 55%|█████▌    | 1451/2620 [01:46<01:35, 12.29it/s]\u001b[A\n",
            " 55%|█████▌    | 1454/2620 [01:46<01:30, 12.87it/s]\u001b[A\n",
            " 56%|█████▌    | 1456/2620 [01:46<01:23, 13.88it/s]\u001b[A\n",
            " 56%|█████▌    | 1458/2620 [01:46<01:47, 10.80it/s]\u001b[A\n",
            " 56%|█████▌    | 1460/2620 [01:47<01:46, 10.91it/s]\u001b[A\n",
            " 56%|█████▌    | 1462/2620 [01:47<01:44, 11.13it/s]\u001b[A\n",
            " 56%|█████▌    | 1465/2620 [01:47<01:20, 14.33it/s]\u001b[A\n",
            " 56%|█████▌    | 1469/2620 [01:47<01:03, 18.15it/s]\u001b[A\n",
            " 56%|█████▌    | 1472/2620 [01:47<00:56, 20.37it/s]\u001b[A\n",
            " 56%|█████▋    | 1475/2620 [01:47<00:52, 21.76it/s]\u001b[A\n",
            " 56%|█████▋    | 1479/2620 [01:47<00:45, 24.94it/s]\u001b[A\n",
            " 57%|█████▋    | 1482/2620 [01:47<00:44, 25.36it/s]\u001b[A\n",
            " 57%|█████▋    | 1485/2620 [01:48<00:44, 25.53it/s]\u001b[A\n",
            " 57%|█████▋    | 1488/2620 [01:48<00:44, 25.19it/s]\u001b[A\n",
            " 57%|█████▋    | 1492/2620 [01:48<00:40, 27.93it/s]\u001b[A\n",
            " 57%|█████▋    | 1495/2620 [01:48<00:48, 23.16it/s]\u001b[A\n",
            " 57%|█████▋    | 1498/2620 [01:48<00:51, 21.95it/s]\u001b[A\n",
            " 57%|█████▋    | 1501/2620 [01:48<00:49, 22.66it/s]\u001b[A\n",
            " 57%|█████▋    | 1505/2620 [01:48<00:45, 24.40it/s]\u001b[A\n",
            " 58%|█████▊    | 1508/2620 [01:48<00:44, 25.02it/s]\u001b[A\n",
            " 58%|█████▊    | 1511/2620 [01:49<00:43, 25.60it/s]\u001b[A\n",
            " 58%|█████▊    | 1514/2620 [01:49<00:42, 25.77it/s]\u001b[A\n",
            " 58%|█████▊    | 1517/2620 [01:49<00:46, 23.79it/s]\u001b[A\n",
            " 58%|█████▊    | 1521/2620 [01:49<00:41, 26.17it/s]\u001b[A\n",
            " 58%|█████▊    | 1524/2620 [01:49<00:41, 26.64it/s]\u001b[A\n",
            " 58%|█████▊    | 1528/2620 [01:49<00:39, 27.33it/s]\u001b[A\n",
            " 58%|█████▊    | 1531/2620 [01:49<00:40, 26.69it/s]\u001b[A\n",
            " 59%|█████▊    | 1534/2620 [01:49<00:43, 24.98it/s]\u001b[A\n",
            " 59%|█████▊    | 1537/2620 [01:50<00:47, 22.91it/s]\u001b[A\n",
            " 59%|█████▉    | 1540/2620 [01:50<00:58, 18.35it/s]\u001b[A\n",
            " 59%|█████▉    | 1543/2620 [01:50<00:53, 20.04it/s]\u001b[A\n",
            " 59%|█████▉    | 1546/2620 [01:50<01:11, 15.03it/s]\u001b[A\n",
            " 59%|█████▉    | 1549/2620 [01:50<01:03, 16.97it/s]\u001b[A\n",
            " 59%|█████▉    | 1552/2620 [01:51<00:56, 18.75it/s]\u001b[A\n",
            " 59%|█████▉    | 1555/2620 [01:51<01:14, 14.27it/s]\u001b[A\n",
            " 59%|█████▉    | 1557/2620 [01:51<01:17, 13.67it/s]\u001b[A\n",
            " 60%|█████▉    | 1561/2620 [01:51<01:01, 17.26it/s]\u001b[A\n",
            " 60%|█████▉    | 1564/2620 [01:51<01:08, 15.33it/s]\u001b[A\n",
            " 60%|█████▉    | 1566/2620 [01:52<01:23, 12.62it/s]\u001b[A\n",
            " 60%|█████▉    | 1568/2620 [01:52<01:44, 10.09it/s]\u001b[A\n",
            " 60%|█████▉    | 1570/2620 [01:52<01:40, 10.46it/s]\u001b[A\n",
            " 60%|██████    | 1572/2620 [01:52<01:31, 11.44it/s]\u001b[A\n",
            " 60%|██████    | 1576/2620 [01:52<01:05, 16.00it/s]\u001b[A\n",
            " 60%|██████    | 1578/2620 [01:53<01:07, 15.43it/s]\u001b[A\n",
            " 60%|██████    | 1580/2620 [01:53<01:13, 14.23it/s]\u001b[A\n",
            " 60%|██████    | 1582/2620 [01:53<01:13, 14.13it/s]\u001b[A\n",
            " 60%|██████    | 1584/2620 [01:53<01:28, 11.66it/s]\u001b[A\n",
            " 61%|██████    | 1586/2620 [01:53<01:31, 11.35it/s]\u001b[A\n",
            " 61%|██████    | 1588/2620 [01:54<01:41, 10.22it/s]\u001b[A\n",
            " 61%|██████    | 1590/2620 [01:54<01:35, 10.74it/s]\u001b[A\n",
            " 61%|██████    | 1592/2620 [01:54<01:58,  8.69it/s]\u001b[A\n",
            " 61%|██████    | 1593/2620 [01:54<01:58,  8.69it/s]\u001b[A\n",
            " 61%|██████    | 1595/2620 [01:54<02:04,  8.23it/s]\u001b[A\n",
            " 61%|██████    | 1596/2620 [01:55<02:32,  6.72it/s]\u001b[A\n",
            " 61%|██████    | 1597/2620 [01:55<02:34,  6.60it/s]\u001b[A\n",
            " 61%|██████    | 1599/2620 [01:55<02:15,  7.55it/s]\u001b[A\n",
            " 61%|██████    | 1600/2620 [01:55<02:21,  7.22it/s]\u001b[A\n",
            " 61%|██████    | 1601/2620 [01:55<02:14,  7.58it/s]\u001b[A\n",
            " 61%|██████    | 1602/2620 [01:55<02:07,  7.96it/s]\u001b[A\n",
            " 61%|██████    | 1603/2620 [01:56<02:08,  7.92it/s]\u001b[A\n",
            " 61%|██████    | 1604/2620 [01:56<02:10,  7.78it/s]\u001b[A\n",
            " 61%|██████▏   | 1605/2620 [01:56<02:07,  7.93it/s]\u001b[A\n",
            " 61%|██████▏   | 1606/2620 [01:56<02:17,  7.39it/s]\u001b[A\n",
            " 61%|██████▏   | 1610/2620 [01:56<01:10, 14.30it/s]\u001b[A\n",
            " 62%|██████▏   | 1612/2620 [01:56<01:13, 13.66it/s]\u001b[A\n",
            " 62%|██████▏   | 1615/2620 [01:56<01:04, 15.63it/s]\u001b[A\n",
            " 62%|██████▏   | 1617/2620 [01:57<01:07, 14.95it/s]\u001b[A\n",
            " 62%|██████▏   | 1619/2620 [01:57<01:24, 11.87it/s]\u001b[A\n",
            " 62%|██████▏   | 1622/2620 [01:57<01:09, 14.30it/s]\u001b[A\n",
            " 62%|██████▏   | 1625/2620 [01:57<00:58, 16.95it/s]\u001b[A\n",
            " 62%|██████▏   | 1628/2620 [01:57<00:51, 19.09it/s]\u001b[A\n",
            " 62%|██████▏   | 1631/2620 [01:57<00:53, 18.51it/s]\u001b[A\n",
            " 62%|██████▏   | 1634/2620 [01:58<00:57, 17.22it/s]\u001b[A\n",
            " 62%|██████▏   | 1636/2620 [01:58<01:07, 14.48it/s]\u001b[A\n",
            " 63%|██████▎   | 1638/2620 [01:58<01:11, 13.73it/s]\u001b[A\n",
            " 63%|██████▎   | 1641/2620 [01:58<01:00, 16.25it/s]\u001b[A\n",
            " 63%|██████▎   | 1643/2620 [01:58<00:57, 16.95it/s]\u001b[A\n",
            " 63%|██████▎   | 1645/2620 [01:58<01:11, 13.61it/s]\u001b[A\n",
            " 63%|██████▎   | 1647/2620 [01:59<01:06, 14.58it/s]\u001b[A\n",
            " 63%|██████▎   | 1649/2620 [01:59<01:04, 15.11it/s]\u001b[A\n",
            " 63%|██████▎   | 1652/2620 [01:59<00:53, 18.13it/s]\u001b[A\n",
            " 63%|██████▎   | 1655/2620 [01:59<00:49, 19.61it/s]\u001b[A\n",
            " 63%|██████▎   | 1658/2620 [01:59<00:57, 16.85it/s]\u001b[A\n",
            " 63%|██████▎   | 1660/2620 [01:59<01:04, 14.78it/s]\u001b[A\n",
            " 63%|██████▎   | 1662/2620 [01:59<01:09, 13.75it/s]\u001b[A\n",
            " 64%|██████▎   | 1665/2620 [02:00<01:00, 15.68it/s]\u001b[A\n",
            " 64%|██████▎   | 1667/2620 [02:00<01:04, 14.69it/s]\u001b[A\n",
            " 64%|██████▎   | 1669/2620 [02:00<01:06, 14.37it/s]\u001b[A\n",
            " 64%|██████▍   | 1671/2620 [02:00<01:01, 15.42it/s]\u001b[A\n",
            " 64%|██████▍   | 1673/2620 [02:00<01:14, 12.73it/s]\u001b[A\n",
            " 64%|██████▍   | 1675/2620 [02:00<01:10, 13.32it/s]\u001b[A\n",
            " 64%|██████▍   | 1677/2620 [02:01<01:04, 14.55it/s]\u001b[A\n",
            " 64%|██████▍   | 1680/2620 [02:01<00:53, 17.54it/s]\u001b[A\n",
            " 64%|██████▍   | 1683/2620 [02:01<00:46, 20.02it/s]\u001b[A\n",
            " 64%|██████▍   | 1686/2620 [02:01<00:43, 21.68it/s]\u001b[A\n",
            " 64%|██████▍   | 1689/2620 [02:01<00:40, 22.73it/s]\u001b[A\n",
            " 65%|██████▍   | 1692/2620 [02:01<00:43, 21.38it/s]\u001b[A\n",
            " 65%|██████▍   | 1695/2620 [02:01<00:43, 21.47it/s]\u001b[A\n",
            " 65%|██████▍   | 1698/2620 [02:01<00:45, 20.40it/s]\u001b[A\n",
            " 65%|██████▍   | 1701/2620 [02:02<00:43, 21.16it/s]\u001b[A\n",
            " 65%|██████▌   | 1704/2620 [02:02<00:41, 21.83it/s]\u001b[A\n",
            " 65%|██████▌   | 1707/2620 [02:02<00:43, 21.21it/s]\u001b[A\n",
            " 65%|██████▌   | 1710/2620 [02:02<00:44, 20.23it/s]\u001b[A\n",
            " 65%|██████▌   | 1713/2620 [02:02<00:42, 21.36it/s]\u001b[A\n",
            " 65%|██████▌   | 1716/2620 [02:02<00:42, 21.40it/s]\u001b[A\n",
            " 66%|██████▌   | 1719/2620 [02:02<00:42, 20.96it/s]\u001b[A\n",
            " 66%|██████▌   | 1722/2620 [02:03<00:39, 22.87it/s]\u001b[A\n",
            " 66%|██████▌   | 1725/2620 [02:03<00:41, 21.41it/s]\u001b[A\n",
            " 66%|██████▌   | 1728/2620 [02:03<00:38, 23.25it/s]\u001b[A\n",
            " 66%|██████▌   | 1731/2620 [02:03<00:40, 21.71it/s]\u001b[A\n",
            " 66%|██████▌   | 1734/2620 [02:03<00:39, 22.64it/s]\u001b[A\n",
            " 66%|██████▋   | 1737/2620 [02:03<00:41, 21.37it/s]\u001b[A\n",
            " 66%|██████▋   | 1740/2620 [02:03<00:38, 22.86it/s]\u001b[A\n",
            " 67%|██████▋   | 1743/2620 [02:03<00:37, 23.64it/s]\u001b[A\n",
            " 67%|██████▋   | 1746/2620 [02:04<00:38, 22.43it/s]\u001b[A\n",
            " 67%|██████▋   | 1749/2620 [02:04<00:42, 20.43it/s]\u001b[A\n",
            " 67%|██████▋   | 1753/2620 [02:04<00:36, 23.82it/s]\u001b[A\n",
            " 67%|██████▋   | 1756/2620 [02:04<00:36, 23.84it/s]\u001b[A\n",
            " 67%|██████▋   | 1759/2620 [02:04<00:35, 24.49it/s]\u001b[A\n",
            " 67%|██████▋   | 1762/2620 [02:04<00:37, 22.87it/s]\u001b[A\n",
            " 67%|██████▋   | 1765/2620 [02:04<00:37, 22.78it/s]\u001b[A\n",
            " 67%|██████▋   | 1768/2620 [02:05<00:37, 22.75it/s]\u001b[A\n",
            " 68%|██████▊   | 1771/2620 [02:05<00:42, 19.93it/s]\u001b[A\n",
            " 68%|██████▊   | 1774/2620 [02:05<00:43, 19.55it/s]\u001b[A\n",
            " 68%|██████▊   | 1777/2620 [02:05<00:40, 20.88it/s]\u001b[A\n",
            " 68%|██████▊   | 1780/2620 [02:05<00:40, 20.63it/s]\u001b[A\n",
            " 68%|██████▊   | 1783/2620 [02:05<00:47, 17.49it/s]\u001b[A\n",
            " 68%|██████▊   | 1785/2620 [02:06<00:47, 17.58it/s]\u001b[A\n",
            " 68%|██████▊   | 1787/2620 [02:06<00:54, 15.41it/s]\u001b[A\n",
            " 68%|██████▊   | 1789/2620 [02:06<00:59, 14.00it/s]\u001b[A\n",
            " 68%|██████▊   | 1791/2620 [02:06<00:57, 14.31it/s]\u001b[A\n",
            " 68%|██████▊   | 1793/2620 [02:06<01:02, 13.15it/s]\u001b[A\n",
            " 69%|██████▊   | 1795/2620 [02:07<01:21, 10.15it/s]\u001b[A\n",
            " 69%|██████▊   | 1798/2620 [02:07<01:04, 12.73it/s]\u001b[A\n",
            " 69%|██████▊   | 1800/2620 [02:07<00:59, 13.84it/s]\u001b[A\n",
            " 69%|██████▉   | 1802/2620 [02:07<00:57, 14.24it/s]\u001b[A\n",
            " 69%|██████▉   | 1805/2620 [02:07<00:49, 16.42it/s]\u001b[A\n",
            " 69%|██████▉   | 1807/2620 [02:07<00:49, 16.31it/s]\u001b[A\n",
            " 69%|██████▉   | 1809/2620 [02:07<01:04, 12.56it/s]\u001b[A\n",
            " 69%|██████▉   | 1811/2620 [02:08<01:02, 12.99it/s]\u001b[A\n",
            " 69%|██████▉   | 1813/2620 [02:08<01:05, 12.37it/s]\u001b[A\n",
            " 69%|██████▉   | 1816/2620 [02:08<00:51, 15.74it/s]\u001b[A\n",
            " 69%|██████▉   | 1820/2620 [02:08<00:41, 19.16it/s]\u001b[A\n",
            " 70%|██████▉   | 1823/2620 [02:08<00:37, 21.24it/s]\u001b[A\n",
            " 70%|██████▉   | 1826/2620 [02:08<00:39, 19.90it/s]\u001b[A\n",
            " 70%|██████▉   | 1829/2620 [02:08<00:44, 17.67it/s]\u001b[A\n",
            " 70%|██████▉   | 1831/2620 [02:09<00:49, 16.00it/s]\u001b[A\n",
            " 70%|███████   | 1834/2620 [02:09<00:45, 17.41it/s]\u001b[A\n",
            " 70%|███████   | 1837/2620 [02:09<00:40, 19.42it/s]\u001b[A\n",
            " 70%|███████   | 1840/2620 [02:09<00:37, 21.01it/s]\u001b[A\n",
            " 70%|███████   | 1843/2620 [02:09<00:40, 19.03it/s]\u001b[A\n",
            " 70%|███████   | 1846/2620 [02:09<00:37, 20.69it/s]\u001b[A\n",
            " 71%|███████   | 1849/2620 [02:09<00:37, 20.68it/s]\u001b[A\n",
            " 71%|███████   | 1852/2620 [02:10<00:42, 17.90it/s]\u001b[A\n",
            " 71%|███████   | 1854/2620 [02:10<00:46, 16.30it/s]\u001b[A\n",
            " 71%|███████   | 1856/2620 [02:10<00:45, 16.72it/s]\u001b[A\n",
            " 71%|███████   | 1858/2620 [02:10<00:49, 15.28it/s]\u001b[A\n",
            " 71%|███████   | 1860/2620 [02:10<00:52, 14.47it/s]\u001b[A\n",
            " 71%|███████   | 1862/2620 [02:10<00:58, 12.87it/s]\u001b[A\n",
            " 71%|███████   | 1865/2620 [02:11<00:50, 14.97it/s]\u001b[A\n",
            " 71%|███████▏  | 1867/2620 [02:11<00:50, 14.93it/s]\u001b[A\n",
            " 71%|███████▏  | 1870/2620 [02:11<00:43, 17.07it/s]\u001b[A\n",
            " 71%|███████▏  | 1873/2620 [02:11<00:39, 18.86it/s]\u001b[A\n",
            " 72%|███████▏  | 1876/2620 [02:11<00:37, 19.92it/s]\u001b[A\n",
            " 72%|███████▏  | 1879/2620 [02:11<00:37, 19.92it/s]\u001b[A\n",
            " 72%|███████▏  | 1882/2620 [02:11<00:38, 19.16it/s]\u001b[A\n",
            " 72%|███████▏  | 1884/2620 [02:12<00:38, 19.11it/s]\u001b[A\n",
            " 72%|███████▏  | 1886/2620 [02:12<00:38, 18.92it/s]\u001b[A\n",
            " 72%|███████▏  | 1889/2620 [02:12<00:34, 20.97it/s]\u001b[A\n",
            " 72%|███████▏  | 1892/2620 [02:12<00:33, 21.73it/s]\u001b[A\n",
            " 72%|███████▏  | 1895/2620 [02:12<00:34, 20.78it/s]\u001b[A\n",
            " 72%|███████▏  | 1898/2620 [02:12<00:32, 22.32it/s]\u001b[A\n",
            " 73%|███████▎  | 1901/2620 [02:12<00:31, 23.12it/s]\u001b[A\n",
            " 73%|███████▎  | 1905/2620 [02:12<00:27, 26.29it/s]\u001b[A\n",
            " 73%|███████▎  | 1909/2620 [02:13<00:26, 26.97it/s]\u001b[A\n",
            " 73%|███████▎  | 1912/2620 [02:13<00:28, 25.15it/s]\u001b[A\n",
            " 73%|███████▎  | 1915/2620 [02:13<00:35, 19.93it/s]\u001b[A\n",
            " 73%|███████▎  | 1918/2620 [02:13<00:40, 17.24it/s]\u001b[A\n",
            " 73%|███████▎  | 1920/2620 [02:13<00:42, 16.38it/s]\u001b[A\n",
            " 73%|███████▎  | 1922/2620 [02:14<00:50, 13.86it/s]\u001b[A\n",
            " 73%|███████▎  | 1924/2620 [02:14<00:54, 12.69it/s]\u001b[A\n",
            " 74%|███████▎  | 1926/2620 [02:14<00:53, 12.89it/s]\u001b[A\n",
            " 74%|███████▎  | 1928/2620 [02:14<00:57, 12.11it/s]\u001b[A\n",
            " 74%|███████▎  | 1931/2620 [02:14<00:49, 13.90it/s]\u001b[A\n",
            " 74%|███████▍  | 1933/2620 [02:14<00:46, 14.78it/s]\u001b[A\n",
            " 74%|███████▍  | 1935/2620 [02:14<00:45, 15.17it/s]\u001b[A\n",
            " 74%|███████▍  | 1938/2620 [02:15<00:38, 17.66it/s]\u001b[A\n",
            " 74%|███████▍  | 1940/2620 [02:15<00:40, 16.73it/s]\u001b[A\n",
            " 74%|███████▍  | 1943/2620 [02:15<00:35, 19.16it/s]\u001b[A\n",
            " 74%|███████▍  | 1945/2620 [02:15<00:38, 17.33it/s]\u001b[A\n",
            " 74%|███████▍  | 1948/2620 [02:15<00:36, 18.23it/s]\u001b[A\n",
            " 74%|███████▍  | 1951/2620 [02:15<00:33, 19.74it/s]\u001b[A\n",
            " 75%|███████▍  | 1954/2620 [02:15<00:38, 17.28it/s]\u001b[A\n",
            " 75%|███████▍  | 1956/2620 [02:16<00:54, 12.15it/s]\u001b[A\n",
            " 75%|███████▍  | 1958/2620 [02:16<00:55, 11.83it/s]\u001b[A\n",
            " 75%|███████▍  | 1961/2620 [02:16<00:48, 13.68it/s]\u001b[A\n",
            " 75%|███████▌  | 1965/2620 [02:16<00:36, 17.92it/s]\u001b[A\n",
            " 75%|███████▌  | 1968/2620 [02:17<00:45, 14.35it/s]\u001b[A\n",
            " 75%|███████▌  | 1970/2620 [02:17<00:46, 13.83it/s]\u001b[A\n",
            " 75%|███████▌  | 1972/2620 [02:17<00:54, 11.99it/s]\u001b[A\n",
            " 75%|███████▌  | 1975/2620 [02:17<00:44, 14.55it/s]\u001b[A\n",
            " 75%|███████▌  | 1978/2620 [02:17<00:39, 16.10it/s]\u001b[A\n",
            " 76%|███████▌  | 1981/2620 [02:17<00:35, 17.93it/s]\u001b[A\n",
            " 76%|███████▌  | 1984/2620 [02:18<00:34, 18.46it/s]\u001b[A\n",
            " 76%|███████▌  | 1987/2620 [02:18<00:32, 19.37it/s]\u001b[A\n",
            " 76%|███████▌  | 1990/2620 [02:18<00:38, 16.24it/s]\u001b[A\n",
            " 76%|███████▌  | 1993/2620 [02:18<00:34, 18.18it/s]\u001b[A\n",
            " 76%|███████▌  | 1996/2620 [02:18<00:32, 18.97it/s]\u001b[A\n",
            " 76%|███████▋  | 1999/2620 [02:18<00:30, 20.49it/s]\u001b[A\n",
            " 76%|███████▋  | 2002/2620 [02:19<00:38, 15.89it/s]\u001b[A\n",
            " 76%|███████▋  | 2004/2620 [02:19<00:43, 14.17it/s]\u001b[A\n",
            " 77%|███████▋  | 2006/2620 [02:19<00:42, 14.61it/s]\u001b[A\n",
            " 77%|███████▋  | 2009/2620 [02:19<00:36, 16.79it/s]\u001b[A\n",
            " 77%|███████▋  | 2011/2620 [02:19<00:40, 15.17it/s]\u001b[A\n",
            " 77%|███████▋  | 2014/2620 [02:19<00:34, 17.32it/s]\u001b[A\n",
            " 77%|███████▋  | 2016/2620 [02:19<00:36, 16.35it/s]\u001b[A\n",
            " 77%|███████▋  | 2020/2620 [02:20<00:28, 20.86it/s]\u001b[A\n",
            " 77%|███████▋  | 2023/2620 [02:20<00:26, 22.24it/s]\u001b[A\n",
            " 77%|███████▋  | 2026/2620 [02:20<00:26, 22.26it/s]\u001b[A\n",
            " 77%|███████▋  | 2030/2620 [02:20<00:23, 25.17it/s]\u001b[A\n",
            " 78%|███████▊  | 2033/2620 [02:20<00:42, 13.84it/s]\u001b[A\n",
            " 78%|███████▊  | 2036/2620 [02:21<00:49, 11.69it/s]\u001b[A\n",
            " 78%|███████▊  | 2038/2620 [02:21<00:53, 10.88it/s]\u001b[A\n",
            " 78%|███████▊  | 2040/2620 [02:21<00:55, 10.41it/s]\u001b[A\n",
            " 78%|███████▊  | 2042/2620 [02:21<00:51, 11.18it/s]\u001b[A\n",
            " 78%|███████▊  | 2044/2620 [02:21<00:46, 12.46it/s]\u001b[A\n",
            " 78%|███████▊  | 2047/2620 [02:22<00:39, 14.62it/s]\u001b[A\n",
            " 78%|███████▊  | 2049/2620 [02:22<00:43, 13.05it/s]\u001b[A\n",
            " 78%|███████▊  | 2051/2620 [02:22<00:47, 12.04it/s]\u001b[A\n",
            " 78%|███████▊  | 2053/2620 [02:22<00:42, 13.41it/s]\u001b[A\n",
            " 78%|███████▊  | 2055/2620 [02:22<00:44, 12.71it/s]\u001b[A\n",
            " 79%|███████▊  | 2058/2620 [02:22<00:36, 15.48it/s]\u001b[A\n",
            " 79%|███████▊  | 2060/2620 [02:23<00:51, 10.83it/s]\u001b[A\n",
            " 79%|███████▊  | 2062/2620 [02:23<00:47, 11.69it/s]\u001b[A\n",
            " 79%|███████▉  | 2064/2620 [02:23<00:49, 11.25it/s]\u001b[A\n",
            " 79%|███████▉  | 2067/2620 [02:23<00:38, 14.24it/s]\u001b[A\n",
            " 79%|███████▉  | 2069/2620 [02:23<00:44, 12.52it/s]\u001b[A\n",
            " 79%|███████▉  | 2071/2620 [02:24<00:43, 12.71it/s]\u001b[A\n",
            " 79%|███████▉  | 2073/2620 [02:24<00:41, 13.27it/s]\u001b[A\n",
            " 79%|███████▉  | 2075/2620 [02:24<00:40, 13.58it/s]\u001b[A\n",
            " 79%|███████▉  | 2077/2620 [02:24<00:39, 13.69it/s]\u001b[A\n",
            " 79%|███████▉  | 2080/2620 [02:24<00:33, 16.18it/s]\u001b[A\n",
            " 79%|███████▉  | 2082/2620 [02:24<00:32, 16.54it/s]\u001b[A\n",
            " 80%|███████▉  | 2084/2620 [02:24<00:33, 15.89it/s]\u001b[A\n",
            " 80%|███████▉  | 2088/2620 [02:25<00:27, 19.02it/s]\u001b[A\n",
            " 80%|███████▉  | 2090/2620 [02:25<00:34, 15.52it/s]\u001b[A\n",
            " 80%|███████▉  | 2092/2620 [02:25<00:36, 14.42it/s]\u001b[A\n",
            " 80%|███████▉  | 2095/2620 [02:25<00:32, 16.03it/s]\u001b[A\n",
            " 80%|████████  | 2098/2620 [02:25<00:29, 17.67it/s]\u001b[A\n",
            " 80%|████████  | 2101/2620 [02:25<00:29, 17.84it/s]\u001b[A\n",
            " 80%|████████  | 2103/2620 [02:26<00:32, 15.77it/s]\u001b[A\n",
            " 80%|████████  | 2105/2620 [02:26<00:35, 14.47it/s]\u001b[A\n",
            " 80%|████████  | 2107/2620 [02:26<00:39, 13.02it/s]\u001b[A\n",
            " 81%|████████  | 2110/2620 [02:26<00:32, 15.62it/s]\u001b[A\n",
            " 81%|████████  | 2112/2620 [02:26<00:33, 15.19it/s]\u001b[A\n",
            " 81%|████████  | 2115/2620 [02:26<00:37, 13.45it/s]\u001b[A\n",
            " 81%|████████  | 2117/2620 [02:27<00:42, 11.78it/s]\u001b[A\n",
            " 81%|████████  | 2119/2620 [02:27<00:48, 10.43it/s]\u001b[A\n",
            " 81%|████████  | 2122/2620 [02:27<00:39, 12.61it/s]\u001b[A\n",
            " 81%|████████  | 2126/2620 [02:27<00:29, 16.67it/s]\u001b[A\n",
            " 81%|████████  | 2128/2620 [02:27<00:36, 13.48it/s]\u001b[A\n",
            " 81%|████████▏ | 2130/2620 [02:28<00:34, 14.12it/s]\u001b[A\n",
            " 81%|████████▏ | 2133/2620 [02:28<00:31, 15.32it/s]\u001b[A\n",
            " 81%|████████▏ | 2135/2620 [02:28<00:31, 15.29it/s]\u001b[A\n",
            " 82%|████████▏ | 2138/2620 [02:28<00:26, 17.97it/s]\u001b[A\n",
            " 82%|████████▏ | 2141/2620 [02:28<00:26, 17.75it/s]\u001b[A\n",
            " 82%|████████▏ | 2143/2620 [02:28<00:27, 17.48it/s]\u001b[A\n",
            " 82%|████████▏ | 2145/2620 [02:28<00:27, 17.30it/s]\u001b[A\n",
            " 82%|████████▏ | 2147/2620 [02:29<00:29, 16.25it/s]\u001b[A\n",
            " 82%|████████▏ | 2150/2620 [02:29<00:24, 19.15it/s]\u001b[A\n",
            " 82%|████████▏ | 2152/2620 [02:29<00:25, 18.17it/s]\u001b[A\n",
            " 82%|████████▏ | 2155/2620 [02:29<00:22, 20.92it/s]\u001b[A\n",
            " 82%|████████▏ | 2158/2620 [02:29<00:28, 16.14it/s]\u001b[A\n",
            " 82%|████████▏ | 2160/2620 [02:29<00:31, 14.38it/s]\u001b[A\n",
            " 83%|████████▎ | 2162/2620 [02:30<00:36, 12.59it/s]\u001b[A\n",
            " 83%|████████▎ | 2164/2620 [02:30<00:35, 12.95it/s]\u001b[A\n",
            " 83%|████████▎ | 2167/2620 [02:30<00:34, 13.21it/s]\u001b[A\n",
            " 83%|████████▎ | 2170/2620 [02:30<00:29, 15.25it/s]\u001b[A\n",
            " 83%|████████▎ | 2172/2620 [02:30<00:30, 14.84it/s]\u001b[A\n",
            " 83%|████████▎ | 2174/2620 [02:30<00:32, 13.55it/s]\u001b[A\n",
            " 83%|████████▎ | 2177/2620 [02:30<00:26, 16.66it/s]\u001b[A\n",
            " 83%|████████▎ | 2179/2620 [02:31<00:28, 15.49it/s]\u001b[A\n",
            " 83%|████████▎ | 2181/2620 [02:31<00:30, 14.34it/s]\u001b[A\n",
            " 83%|████████▎ | 2183/2620 [02:31<00:33, 13.24it/s]\u001b[A\n",
            " 83%|████████▎ | 2185/2620 [02:31<00:32, 13.35it/s]\u001b[A\n",
            " 84%|████████▎ | 2188/2620 [02:31<00:31, 13.71it/s]\u001b[A\n",
            " 84%|████████▎ | 2190/2620 [02:31<00:29, 14.63it/s]\u001b[A\n",
            " 84%|████████▎ | 2193/2620 [02:32<00:24, 17.23it/s]\u001b[A\n",
            " 84%|████████▍ | 2195/2620 [02:32<00:24, 17.49it/s]\u001b[A\n",
            " 84%|████████▍ | 2198/2620 [02:32<00:21, 20.04it/s]\u001b[A\n",
            " 84%|████████▍ | 2201/2620 [02:32<00:23, 17.59it/s]\u001b[A\n",
            " 84%|████████▍ | 2203/2620 [02:32<00:28, 14.60it/s]\u001b[A\n",
            " 84%|████████▍ | 2205/2620 [02:32<00:28, 14.42it/s]\u001b[A\n",
            " 84%|████████▍ | 2207/2620 [02:32<00:27, 14.94it/s]\u001b[A\n",
            " 84%|████████▍ | 2210/2620 [02:33<00:25, 15.78it/s]\u001b[A\n",
            " 84%|████████▍ | 2212/2620 [02:33<00:24, 16.60it/s]\u001b[A\n",
            " 85%|████████▍ | 2214/2620 [02:33<00:28, 14.32it/s]\u001b[A\n",
            " 85%|████████▍ | 2217/2620 [02:33<00:22, 17.68it/s]\u001b[A\n",
            " 85%|████████▍ | 2219/2620 [02:33<00:22, 17.47it/s]\u001b[A\n",
            " 85%|████████▍ | 2222/2620 [02:33<00:19, 20.03it/s]\u001b[A\n",
            " 85%|████████▍ | 2225/2620 [02:33<00:18, 21.00it/s]\u001b[A\n",
            " 85%|████████▌ | 2228/2620 [02:34<00:21, 17.97it/s]\u001b[A\n",
            " 85%|████████▌ | 2230/2620 [02:34<00:22, 17.64it/s]\u001b[A\n",
            " 85%|████████▌ | 2232/2620 [02:34<00:25, 15.43it/s]\u001b[A\n",
            " 85%|████████▌ | 2235/2620 [02:34<00:21, 17.74it/s]\u001b[A\n",
            " 85%|████████▌ | 2237/2620 [02:34<00:28, 13.32it/s]\u001b[A\n",
            " 85%|████████▌ | 2239/2620 [02:35<00:32, 11.63it/s]\u001b[A\n",
            " 86%|████████▌ | 2241/2620 [02:35<00:37, 10.17it/s]\u001b[A\n",
            " 86%|████████▌ | 2243/2620 [02:35<00:44,  8.44it/s]\u001b[A\n",
            " 86%|████████▌ | 2245/2620 [02:35<00:45,  8.20it/s]\u001b[A\n",
            " 86%|████████▌ | 2248/2620 [02:36<00:33, 10.96it/s]\u001b[A\n",
            " 86%|████████▌ | 2250/2620 [02:36<00:36, 10.09it/s]\u001b[A\n",
            " 86%|████████▌ | 2252/2620 [02:36<00:38,  9.53it/s]\u001b[A\n",
            " 86%|████████▌ | 2254/2620 [02:36<00:36,  9.91it/s]\u001b[A\n",
            " 86%|████████▌ | 2256/2620 [02:36<00:35, 10.12it/s]\u001b[A\n",
            " 86%|████████▌ | 2258/2620 [02:36<00:32, 10.98it/s]\u001b[A\n",
            " 86%|████████▋ | 2260/2620 [02:37<00:33, 10.85it/s]\u001b[A\n",
            " 86%|████████▋ | 2262/2620 [02:37<00:30, 11.61it/s]\u001b[A\n",
            " 86%|████████▋ | 2264/2620 [02:37<00:28, 12.37it/s]\u001b[A\n",
            " 86%|████████▋ | 2266/2620 [02:37<00:32, 10.80it/s]\u001b[A\n",
            " 87%|████████▋ | 2269/2620 [02:37<00:26, 13.27it/s]\u001b[A\n",
            " 87%|████████▋ | 2271/2620 [02:38<00:27, 12.65it/s]\u001b[A\n",
            " 87%|████████▋ | 2273/2620 [02:38<00:26, 13.07it/s]\u001b[A\n",
            " 87%|████████▋ | 2275/2620 [02:38<00:29, 11.62it/s]\u001b[A\n",
            " 87%|████████▋ | 2277/2620 [02:38<00:30, 11.43it/s]\u001b[A\n",
            " 87%|████████▋ | 2279/2620 [02:38<00:32, 10.53it/s]\u001b[A\n",
            " 87%|████████▋ | 2281/2620 [02:39<00:39,  8.64it/s]\u001b[A\n",
            " 87%|████████▋ | 2282/2620 [02:39<00:44,  7.65it/s]\u001b[A\n",
            " 87%|████████▋ | 2283/2620 [02:39<00:50,  6.72it/s]\u001b[A\n",
            " 87%|████████▋ | 2284/2620 [02:39<00:47,  7.02it/s]\u001b[A\n",
            " 87%|████████▋ | 2285/2620 [02:39<00:51,  6.46it/s]\u001b[A\n",
            " 87%|████████▋ | 2286/2620 [02:40<00:58,  5.67it/s]\u001b[A\n",
            " 87%|████████▋ | 2287/2620 [02:40<00:56,  5.88it/s]\u001b[A\n",
            " 87%|████████▋ | 2289/2620 [02:40<00:51,  6.40it/s]\u001b[A\n",
            " 87%|████████▋ | 2290/2620 [02:40<00:59,  5.54it/s]\u001b[A\n",
            " 87%|████████▋ | 2291/2620 [02:40<00:55,  5.98it/s]\u001b[A\n",
            " 88%|████████▊ | 2293/2620 [02:41<00:48,  6.77it/s]\u001b[A\n",
            " 88%|████████▊ | 2294/2620 [02:41<00:46,  7.04it/s]\u001b[A\n",
            " 88%|████████▊ | 2295/2620 [02:41<00:47,  6.82it/s]\u001b[A\n",
            " 88%|████████▊ | 2296/2620 [02:41<00:45,  7.16it/s]\u001b[A\n",
            " 88%|████████▊ | 2297/2620 [02:41<00:55,  5.82it/s]\u001b[A\n",
            " 88%|████████▊ | 2298/2620 [02:41<00:54,  5.93it/s]\u001b[A\n",
            " 88%|████████▊ | 2300/2620 [02:42<00:47,  6.69it/s]\u001b[A\n",
            " 88%|████████▊ | 2302/2620 [02:42<00:37,  8.51it/s]\u001b[A\n",
            " 88%|████████▊ | 2303/2620 [02:42<00:41,  7.63it/s]\u001b[A\n",
            " 88%|████████▊ | 2305/2620 [02:42<00:34,  9.23it/s]\u001b[A\n",
            " 88%|████████▊ | 2307/2620 [02:42<00:31,  9.89it/s]\u001b[A\n",
            " 88%|████████▊ | 2309/2620 [02:43<00:30, 10.30it/s]\u001b[A\n",
            " 88%|████████▊ | 2311/2620 [02:43<00:38,  7.99it/s]\u001b[A\n",
            " 88%|████████▊ | 2313/2620 [02:43<00:32,  9.35it/s]\u001b[A\n",
            " 88%|████████▊ | 2315/2620 [02:43<00:36,  8.30it/s]\u001b[A\n",
            " 88%|████████▊ | 2317/2620 [02:44<00:32,  9.23it/s]\u001b[A\n",
            " 89%|████████▊ | 2319/2620 [02:44<00:29, 10.23it/s]\u001b[A\n",
            " 89%|████████▊ | 2321/2620 [02:44<00:28, 10.49it/s]\u001b[A\n",
            " 89%|████████▊ | 2323/2620 [02:44<00:32,  9.12it/s]\u001b[A\n",
            " 89%|████████▉ | 2326/2620 [02:44<00:28, 10.40it/s]\u001b[A\n",
            " 89%|████████▉ | 2328/2620 [02:45<00:26, 11.08it/s]\u001b[A\n",
            " 89%|████████▉ | 2330/2620 [02:45<00:25, 11.17it/s]\u001b[A\n",
            " 89%|████████▉ | 2332/2620 [02:45<00:29,  9.80it/s]\u001b[A\n",
            " 89%|████████▉ | 2334/2620 [02:45<00:26, 10.85it/s]\u001b[A\n",
            " 89%|████████▉ | 2336/2620 [02:45<00:23, 12.11it/s]\u001b[A\n",
            " 89%|████████▉ | 2338/2620 [02:45<00:26, 10.50it/s]\u001b[A\n",
            " 89%|████████▉ | 2340/2620 [02:46<00:27, 10.29it/s]\u001b[A\n",
            " 89%|████████▉ | 2342/2620 [02:46<00:27, 10.12it/s]\u001b[A\n",
            " 89%|████████▉ | 2344/2620 [02:46<00:26, 10.31it/s]\u001b[A\n",
            " 90%|████████▉ | 2346/2620 [02:46<00:24, 11.16it/s]\u001b[A\n",
            " 90%|████████▉ | 2348/2620 [02:46<00:25, 10.77it/s]\u001b[A\n",
            " 90%|████████▉ | 2350/2620 [02:47<00:26, 10.22it/s]\u001b[A\n",
            " 90%|████████▉ | 2352/2620 [02:47<00:28,  9.45it/s]\u001b[A\n",
            " 90%|████████▉ | 2353/2620 [02:47<00:30,  8.77it/s]\u001b[A\n",
            " 90%|████████▉ | 2354/2620 [02:47<00:32,  8.30it/s]\u001b[A\n",
            " 90%|████████▉ | 2355/2620 [02:47<00:35,  7.51it/s]\u001b[A\n",
            " 90%|████████▉ | 2356/2620 [02:48<00:40,  6.46it/s]\u001b[A\n",
            " 90%|█████████ | 2358/2620 [02:48<00:37,  7.06it/s]\u001b[A\n",
            " 90%|█████████ | 2360/2620 [02:48<00:30,  8.42it/s]\u001b[A\n",
            " 90%|█████████ | 2362/2620 [02:48<00:25,  9.93it/s]\u001b[A\n",
            " 90%|█████████ | 2365/2620 [02:48<00:21, 12.07it/s]\u001b[A\n",
            " 90%|█████████ | 2368/2620 [02:48<00:17, 14.51it/s]\u001b[A\n",
            " 90%|█████████ | 2370/2620 [02:49<00:16, 15.43it/s]\u001b[A\n",
            " 91%|█████████ | 2372/2620 [02:49<00:15, 15.67it/s]\u001b[A\n",
            " 91%|█████████ | 2374/2620 [02:49<00:18, 13.48it/s]\u001b[A\n",
            " 91%|█████████ | 2376/2620 [02:49<00:17, 13.81it/s]\u001b[A\n",
            " 91%|█████████ | 2379/2620 [02:49<00:17, 14.04it/s]\u001b[A\n",
            " 91%|█████████ | 2381/2620 [02:49<00:16, 14.73it/s]\u001b[A\n",
            " 91%|█████████ | 2384/2620 [02:49<00:15, 15.31it/s]\u001b[A\n",
            " 91%|█████████ | 2386/2620 [02:50<00:14, 16.07it/s]\u001b[A\n",
            " 91%|█████████ | 2388/2620 [02:50<00:16, 13.94it/s]\u001b[A\n",
            " 91%|█████████▏| 2391/2620 [02:50<00:15, 15.22it/s]\u001b[A\n",
            " 91%|█████████▏| 2393/2620 [02:50<00:16, 13.70it/s]\u001b[A\n",
            " 91%|█████████▏| 2395/2620 [02:50<00:16, 13.62it/s]\u001b[A\n",
            " 91%|█████████▏| 2397/2620 [02:50<00:15, 14.47it/s]\u001b[A\n",
            " 92%|█████████▏| 2399/2620 [02:51<00:14, 15.25it/s]\u001b[A\n",
            " 92%|█████████▏| 2401/2620 [02:51<00:13, 16.37it/s]\u001b[A\n",
            " 92%|█████████▏| 2403/2620 [02:51<00:15, 13.95it/s]\u001b[A\n",
            " 92%|█████████▏| 2406/2620 [02:51<00:12, 17.13it/s]\u001b[A\n",
            " 92%|█████████▏| 2408/2620 [02:51<00:13, 15.28it/s]\u001b[A\n",
            " 92%|█████████▏| 2410/2620 [02:51<00:13, 15.65it/s]\u001b[A\n",
            " 92%|█████████▏| 2412/2620 [02:51<00:13, 15.95it/s]\u001b[A\n",
            " 92%|█████████▏| 2414/2620 [02:51<00:13, 14.86it/s]\u001b[A\n",
            " 92%|█████████▏| 2416/2620 [02:52<00:13, 15.25it/s]\u001b[A\n",
            " 92%|█████████▏| 2418/2620 [02:52<00:15, 12.89it/s]\u001b[A\n",
            " 92%|█████████▏| 2422/2620 [02:52<00:11, 17.45it/s]\u001b[A\n",
            " 93%|█████████▎| 2426/2620 [02:52<00:10, 19.26it/s]\u001b[A\n",
            " 93%|█████████▎| 2428/2620 [02:52<00:10, 19.17it/s]\u001b[A\n",
            " 93%|█████████▎| 2430/2620 [02:52<00:11, 17.21it/s]\u001b[A\n",
            " 93%|█████████▎| 2432/2620 [02:53<00:11, 16.66it/s]\u001b[A\n",
            " 93%|█████████▎| 2434/2620 [02:53<00:13, 13.51it/s]\u001b[A\n",
            " 93%|█████████▎| 2436/2620 [02:53<00:15, 11.65it/s]\u001b[A\n",
            " 93%|█████████▎| 2439/2620 [02:53<00:12, 13.99it/s]\u001b[A\n",
            " 93%|█████████▎| 2441/2620 [02:53<00:12, 14.69it/s]\u001b[A\n",
            " 93%|█████████▎| 2444/2620 [02:53<00:10, 16.76it/s]\u001b[A\n",
            " 93%|█████████▎| 2446/2620 [02:54<00:11, 14.69it/s]\u001b[A\n",
            " 93%|█████████▎| 2448/2620 [02:54<00:11, 15.47it/s]\u001b[A\n",
            " 94%|█████████▎| 2450/2620 [02:54<00:14, 11.55it/s]\u001b[A\n",
            " 94%|█████████▎| 2452/2620 [02:54<00:15, 10.67it/s]\u001b[A\n",
            " 94%|█████████▎| 2455/2620 [02:54<00:12, 13.30it/s]\u001b[A\n",
            " 94%|█████████▍| 2457/2620 [02:54<00:12, 12.55it/s]\u001b[A\n",
            " 94%|█████████▍| 2460/2620 [02:55<00:10, 15.74it/s]\u001b[A\n",
            " 94%|█████████▍| 2463/2620 [02:55<00:08, 18.67it/s]\u001b[A\n",
            " 94%|█████████▍| 2466/2620 [02:55<00:09, 16.29it/s]\u001b[A\n",
            " 94%|█████████▍| 2469/2620 [02:55<00:08, 18.14it/s]\u001b[A\n",
            " 94%|█████████▍| 2472/2620 [02:55<00:08, 17.16it/s]\u001b[A\n",
            " 94%|█████████▍| 2474/2620 [02:55<00:08, 16.95it/s]\u001b[A\n",
            " 95%|█████████▍| 2476/2620 [02:56<00:08, 16.91it/s]\u001b[A\n",
            " 95%|█████████▍| 2478/2620 [02:56<00:08, 17.40it/s]\u001b[A\n",
            " 95%|█████████▍| 2480/2620 [02:56<00:08, 17.03it/s]\u001b[A\n",
            " 95%|█████████▍| 2483/2620 [02:56<00:07, 18.53it/s]\u001b[A\n",
            " 95%|█████████▍| 2486/2620 [02:56<00:06, 21.11it/s]\u001b[A\n",
            " 95%|█████████▌| 2490/2620 [02:56<00:05, 23.89it/s]\u001b[A\n",
            " 95%|█████████▌| 2493/2620 [02:56<00:05, 22.15it/s]\u001b[A\n",
            " 95%|█████████▌| 2496/2620 [02:56<00:06, 19.99it/s]\u001b[A\n",
            " 95%|█████████▌| 2499/2620 [02:57<00:07, 16.88it/s]\u001b[A\n",
            " 95%|█████████▌| 2501/2620 [02:57<00:07, 15.17it/s]\u001b[A\n",
            " 96%|█████████▌| 2503/2620 [02:57<00:08, 14.07it/s]\u001b[A\n",
            " 96%|█████████▌| 2505/2620 [02:57<00:08, 14.23it/s]\u001b[A\n",
            " 96%|█████████▌| 2507/2620 [02:57<00:07, 14.55it/s]\u001b[A\n",
            " 96%|█████████▌| 2509/2620 [02:58<00:08, 12.95it/s]\u001b[A\n",
            " 96%|█████████▌| 2511/2620 [02:58<00:07, 13.93it/s]\u001b[A\n",
            " 96%|█████████▌| 2513/2620 [02:58<00:07, 13.83it/s]\u001b[A\n",
            " 96%|█████████▌| 2515/2620 [02:58<00:07, 14.67it/s]\u001b[A\n",
            " 96%|█████████▌| 2517/2620 [02:58<00:08, 11.56it/s]\u001b[A\n",
            " 96%|█████████▌| 2519/2620 [02:58<00:08, 12.31it/s]\u001b[A\n",
            " 96%|█████████▌| 2521/2620 [02:58<00:07, 12.54it/s]\u001b[A\n",
            " 96%|█████████▋| 2523/2620 [02:59<00:07, 12.40it/s]\u001b[A\n",
            " 96%|█████████▋| 2525/2620 [02:59<00:06, 13.83it/s]\u001b[A\n",
            " 96%|█████████▋| 2527/2620 [02:59<00:06, 13.50it/s]\u001b[A\n",
            " 97%|█████████▋| 2529/2620 [02:59<00:07, 12.60it/s]\u001b[A\n",
            " 97%|█████████▋| 2531/2620 [02:59<00:07, 11.42it/s]\u001b[A\n",
            " 97%|█████████▋| 2534/2620 [02:59<00:06, 13.64it/s]\u001b[A\n",
            " 97%|█████████▋| 2537/2620 [03:00<00:05, 16.24it/s]\u001b[A\n",
            " 97%|█████████▋| 2539/2620 [03:00<00:06, 12.95it/s]\u001b[A\n",
            " 97%|█████████▋| 2542/2620 [03:00<00:05, 15.32it/s]\u001b[A\n",
            " 97%|█████████▋| 2544/2620 [03:00<00:05, 14.02it/s]\u001b[A\n",
            " 97%|█████████▋| 2546/2620 [03:00<00:05, 13.97it/s]\u001b[A\n",
            " 97%|█████████▋| 2548/2620 [03:00<00:05, 12.86it/s]\u001b[A\n",
            " 97%|█████████▋| 2550/2620 [03:01<00:06, 11.66it/s]\u001b[A\n",
            " 97%|█████████▋| 2552/2620 [03:01<00:05, 11.74it/s]\u001b[A\n",
            " 97%|█████████▋| 2554/2620 [03:01<00:05, 11.49it/s]\u001b[A\n",
            " 98%|█████████▊| 2556/2620 [03:01<00:05, 12.14it/s]\u001b[A\n",
            " 98%|█████████▊| 2558/2620 [03:01<00:05, 11.13it/s]\u001b[A\n",
            " 98%|█████████▊| 2561/2620 [03:01<00:04, 14.02it/s]\u001b[A\n",
            " 98%|█████████▊| 2564/2620 [03:02<00:03, 15.16it/s]\u001b[A\n",
            " 98%|█████████▊| 2568/2620 [03:02<00:03, 16.42it/s]\u001b[A\n",
            " 98%|█████████▊| 2570/2620 [03:02<00:03, 15.77it/s]\u001b[A\n",
            " 98%|█████████▊| 2572/2620 [03:02<00:05,  9.58it/s]\u001b[A\n",
            " 98%|█████████▊| 2574/2620 [03:03<00:04, 10.85it/s]\u001b[A\n",
            " 98%|█████████▊| 2576/2620 [03:03<00:04, 10.83it/s]\u001b[A\n",
            " 98%|█████████▊| 2579/2620 [03:03<00:03, 11.75it/s]\u001b[A\n",
            " 99%|█████████▊| 2582/2620 [03:03<00:02, 13.94it/s]\u001b[A\n",
            " 99%|█████████▊| 2584/2620 [03:03<00:03, 11.31it/s]\u001b[A\n",
            " 99%|█████████▊| 2586/2620 [03:04<00:02, 12.04it/s]\u001b[A\n",
            " 99%|█████████▉| 2588/2620 [03:04<00:02, 12.91it/s]\u001b[A\n",
            " 99%|█████████▉| 2590/2620 [03:04<00:02, 12.78it/s]\u001b[A\n",
            " 99%|█████████▉| 2592/2620 [03:04<00:01, 14.20it/s]\u001b[A\n",
            " 99%|█████████▉| 2595/2620 [03:04<00:01, 17.36it/s]\u001b[A\n",
            " 99%|█████████▉| 2597/2620 [03:04<00:01, 16.03it/s]\u001b[A\n",
            " 99%|█████████▉| 2599/2620 [03:04<00:01, 13.86it/s]\u001b[A\n",
            " 99%|█████████▉| 2601/2620 [03:04<00:01, 15.09it/s]\u001b[A\n",
            " 99%|█████████▉| 2603/2620 [03:05<00:01, 14.64it/s]\u001b[A\n",
            " 99%|█████████▉| 2605/2620 [03:05<00:00, 15.80it/s]\u001b[A\n",
            "100%|█████████▉| 2608/2620 [03:05<00:00, 17.17it/s]\u001b[A\n",
            "100%|█████████▉| 2610/2620 [03:05<00:00, 12.93it/s]\u001b[A\n",
            "100%|█████████▉| 2612/2620 [03:05<00:00, 13.46it/s]\u001b[A\n",
            "100%|█████████▉| 2614/2620 [03:05<00:00, 14.12it/s]\u001b[A\n",
            "100%|█████████▉| 2616/2620 [03:06<00:00, 13.77it/s]\u001b[A\n",
            "100%|█████████▉| 2618/2620 [03:06<00:00, 12.42it/s]\u001b[A\n",
            "100%|██████████| 2620/2620 [03:07<00:00, 14.01it/s]\n"
          ]
        }
      ],
      "source": [
        "#TODO: Make predictions\n",
        "\n",
        "# Follow the steps below:\n",
        "# 1. Create a new object for CTCBeamDecoder with larger (why?) number of beams\n",
        "# 2. Get prediction string by decoding the results of the beam decoder\n",
        "\n",
        "TEST_BEAM_WIDTH = 40\n",
        "\n",
        "test_decoder    =  CTCBeamDecoder(LABELS, beam_width = TEST_BEAM_WIDTH, log_probs_input = True)\n",
        "results = []\n",
        "\n",
        "model.eval()\n",
        "print(\"Testing\")\n",
        "for data in tqdm(test_loader):\n",
        "\n",
        "    x, lx   = data\n",
        "    x       = x.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        h, lh = model(x, lx)\n",
        "\n",
        "    prediction_string= decode_prediction(h, lh, test_decoder) # TODO call decode_prediction \n",
        "    #TODO save the output in results array.\n",
        "    results.extend(prediction_string)\n",
        "    del x, lx, h, lh\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d70dvu_lsMlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d6460a5-dfdb-4de2-d24c-261504d803a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.13 / client 1.5.8)\n",
            "100% 210k/210k [00:02<00:00, 89.1kB/s]\n",
            "Successfully submitted to Automatic Speech Recognition (ASR)"
          ]
        }
      ],
      "source": [
        "data_dir = \"/content/11-785-s23-hw3p2/test-clean/random_submission.csv\"\n",
        "df = pd.read_csv(data_dir)\n",
        "df.label = results\n",
        "df.to_csv('submission.csv', index = False)\n",
        "\n",
        "!kaggle competitions submit -c 11-785-s23-hw3p2 -f submission.csv -m \"I made it!\"\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10 (main, Feb 16 2023, 02:49:39) [Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}